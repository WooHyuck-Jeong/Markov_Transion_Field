{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "WARNING:tensorflow:From C:\\Users\\jwhyu\\AppData\\Local\\Temp/ipykernel_17292/3517717524.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "print(tf.__version__) \n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15864860127578588323\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6300696576\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11863087174265375822\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:0a:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from math import ceil\n",
    "from numba import njit, prange\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array\n",
    "from pyts.preprocessing import MinMaxScaler\n",
    "from pyts.approximation import PiecewiseAggregateApproximation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import np_utils\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Welding_data = np.load('E:/Result/ver.3.22/MTF/MTF.npz')\n",
    "\n",
    "X_data = Welding_data['X_data']\n",
    "y_data = Welding_data['y_data']\n",
    "i_data = Welding_data['i_data']\n",
    "\n",
    "Welding_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 2 ... 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, i_train, i_test = train_test_split(X_data,y_data,i_data, test_size = 0.2, shuffle = True, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 300, 300, 2)\n",
      "(943,)\n",
      "(236, 300, 300, 2)\n",
      "(236,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate((X_train,X_test))\n",
    "targets = np.concatenate((y_train,y_test))\n",
    "index = np.concatenate((i_train,i_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "targets = np_utils.to_categorical(targets)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import Dense, Input, Activation, Flatten, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization.batch_normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 300, 300, 2  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " zero_padding2d (ZeroPadding2D)  (None, 306, 306, 2)  0          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1/conv (Conv2D)            (None, 150, 150, 64  6272        ['zero_padding2d[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1/bn (BatchNormalization)  (None, 150, 150, 64  256         ['conv1/conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1/relu (Activation)        (None, 150, 150, 64  0           ['conv1/bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " zero_padding2d_1 (ZeroPadding2  (None, 152, 152, 64  0          ['conv1/relu[0][0]']             \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " pool1 (MaxPooling2D)           (None, 75, 75, 64)   0           ['zero_padding2d_1[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 75, 75, 64)  256         ['pool1[0][0]']                  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_0_relu (Activatio  (None, 75, 75, 64)  0           ['conv2_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 75, 75, 128)  8192        ['conv2_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_concat (Concatena  (None, 75, 75, 96)  0           ['pool1[0][0]',                  \n",
      " te)                                                              'conv2_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_0_bn (BatchNormal  (None, 75, 75, 96)  384         ['conv2_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_0_relu (Activatio  (None, 75, 75, 96)  0           ['conv2_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 75, 75, 128)  12288       ['conv2_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_concat (Concatena  (None, 75, 75, 128)  0          ['conv2_block1_concat[0][0]',    \n",
      " te)                                                              'conv2_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_0_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_0_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 75, 75, 128)  16384       ['conv2_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_concat (Concatena  (None, 75, 75, 160)  0          ['conv2_block2_concat[0][0]',    \n",
      " te)                                                              'conv2_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_0_bn (BatchNormal  (None, 75, 75, 160)  640        ['conv2_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_0_relu (Activatio  (None, 75, 75, 160)  0          ['conv2_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_1_conv (Conv2D)   (None, 75, 75, 128)  20480       ['conv2_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_concat (Concatena  (None, 75, 75, 192)  0          ['conv2_block3_concat[0][0]',    \n",
      " te)                                                              'conv2_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_0_bn (BatchNormal  (None, 75, 75, 192)  768        ['conv2_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_0_relu (Activatio  (None, 75, 75, 192)  0          ['conv2_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_1_conv (Conv2D)   (None, 75, 75, 128)  24576       ['conv2_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_concat (Concatena  (None, 75, 75, 224)  0          ['conv2_block4_concat[0][0]',    \n",
      " te)                                                              'conv2_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_0_bn (BatchNormal  (None, 75, 75, 224)  896        ['conv2_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_0_relu (Activatio  (None, 75, 75, 224)  0          ['conv2_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_1_conv (Conv2D)   (None, 75, 75, 128)  28672       ['conv2_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_concat (Concatena  (None, 75, 75, 256)  0          ['conv2_block5_concat[0][0]',    \n",
      " te)                                                              'conv2_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_bn (BatchNormalization)  (None, 75, 75, 256)  1024        ['conv2_block6_concat[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_relu (Activation)        (None, 75, 75, 256)  0           ['pool2_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool2_conv (Conv2D)            (None, 75, 75, 128)  32768       ['pool2_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool2_pool (AveragePooling2D)  (None, 37, 37, 128)  0           ['pool2_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 37, 37, 128)  512        ['pool2_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_0_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 37, 37, 128)  16384       ['conv3_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_concat (Concatena  (None, 37, 37, 160)  0          ['pool2_pool[0][0]',             \n",
      " te)                                                              'conv3_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_0_bn (BatchNormal  (None, 37, 37, 160)  640        ['conv3_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_0_relu (Activatio  (None, 37, 37, 160)  0          ['conv3_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 37, 37, 128)  20480       ['conv3_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_concat (Concatena  (None, 37, 37, 192)  0          ['conv3_block1_concat[0][0]',    \n",
      " te)                                                              'conv3_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_0_bn (BatchNormal  (None, 37, 37, 192)  768        ['conv3_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_0_relu (Activatio  (None, 37, 37, 192)  0          ['conv3_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 37, 37, 128)  24576       ['conv3_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_concat (Concatena  (None, 37, 37, 224)  0          ['conv3_block2_concat[0][0]',    \n",
      " te)                                                              'conv3_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_0_bn (BatchNormal  (None, 37, 37, 224)  896        ['conv3_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_0_relu (Activatio  (None, 37, 37, 224)  0          ['conv3_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 37, 37, 128)  28672       ['conv3_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_concat (Concatena  (None, 37, 37, 256)  0          ['conv3_block3_concat[0][0]',    \n",
      " te)                                                              'conv3_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_0_bn (BatchNormal  (None, 37, 37, 256)  1024       ['conv3_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_0_relu (Activatio  (None, 37, 37, 256)  0          ['conv3_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_1_conv (Conv2D)   (None, 37, 37, 128)  32768       ['conv3_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_concat (Concatena  (None, 37, 37, 288)  0          ['conv3_block4_concat[0][0]',    \n",
      " te)                                                              'conv3_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_0_bn (BatchNormal  (None, 37, 37, 288)  1152       ['conv3_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_0_relu (Activatio  (None, 37, 37, 288)  0          ['conv3_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_1_conv (Conv2D)   (None, 37, 37, 128)  36864       ['conv3_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_concat (Concatena  (None, 37, 37, 320)  0          ['conv3_block5_concat[0][0]',    \n",
      " te)                                                              'conv3_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_0_bn (BatchNormal  (None, 37, 37, 320)  1280       ['conv3_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_0_relu (Activatio  (None, 37, 37, 320)  0          ['conv3_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_1_conv (Conv2D)   (None, 37, 37, 128)  40960       ['conv3_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_concat (Concatena  (None, 37, 37, 352)  0          ['conv3_block6_concat[0][0]',    \n",
      " te)                                                              'conv3_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_0_bn (BatchNormal  (None, 37, 37, 352)  1408       ['conv3_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_0_relu (Activatio  (None, 37, 37, 352)  0          ['conv3_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_1_conv (Conv2D)   (None, 37, 37, 128)  45056       ['conv3_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_concat (Concatena  (None, 37, 37, 384)  0          ['conv3_block7_concat[0][0]',    \n",
      " te)                                                              'conv3_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_0_bn (BatchNormal  (None, 37, 37, 384)  1536       ['conv3_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_0_relu (Activatio  (None, 37, 37, 384)  0          ['conv3_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_1_conv (Conv2D)   (None, 37, 37, 128)  49152       ['conv3_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_concat (Concatena  (None, 37, 37, 416)  0          ['conv3_block8_concat[0][0]',    \n",
      " te)                                                              'conv3_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block10_0_bn (BatchNorma  (None, 37, 37, 416)  1664       ['conv3_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_0_relu (Activati  (None, 37, 37, 416)  0          ['conv3_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block10_1_conv (Conv2D)  (None, 37, 37, 128)  53248       ['conv3_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_1_bn (BatchNorma  (None, 37, 37, 128)  512        ['conv3_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_1_relu (Activati  (None, 37, 37, 128)  0          ['conv3_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block10_2_conv (Conv2D)  (None, 37, 37, 32)   36864       ['conv3_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_concat (Concaten  (None, 37, 37, 448)  0          ['conv3_block9_concat[0][0]',    \n",
      " ate)                                                             'conv3_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_0_bn (BatchNorma  (None, 37, 37, 448)  1792       ['conv3_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_0_relu (Activati  (None, 37, 37, 448)  0          ['conv3_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_1_conv (Conv2D)  (None, 37, 37, 128)  57344       ['conv3_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_1_bn (BatchNorma  (None, 37, 37, 128)  512        ['conv3_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_1_relu (Activati  (None, 37, 37, 128)  0          ['conv3_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_2_conv (Conv2D)  (None, 37, 37, 32)   36864       ['conv3_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_concat (Concaten  (None, 37, 37, 480)  0          ['conv3_block10_concat[0][0]',   \n",
      " ate)                                                             'conv3_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_0_bn (BatchNorma  (None, 37, 37, 480)  1920       ['conv3_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_0_relu (Activati  (None, 37, 37, 480)  0          ['conv3_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_1_conv (Conv2D)  (None, 37, 37, 128)  61440       ['conv3_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_1_bn (BatchNorma  (None, 37, 37, 128)  512        ['conv3_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_1_relu (Activati  (None, 37, 37, 128)  0          ['conv3_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_2_conv (Conv2D)  (None, 37, 37, 32)   36864       ['conv3_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_concat (Concaten  (None, 37, 37, 512)  0          ['conv3_block11_concat[0][0]',   \n",
      " ate)                                                             'conv3_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_bn (BatchNormalization)  (None, 37, 37, 512)  2048        ['conv3_block12_concat[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_relu (Activation)        (None, 37, 37, 512)  0           ['pool3_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool3_conv (Conv2D)            (None, 37, 37, 256)  131072      ['pool3_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool3_pool (AveragePooling2D)  (None, 18, 18, 256)  0           ['pool3_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 18, 18, 256)  1024       ['pool3_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_0_relu (Activatio  (None, 18, 18, 256)  0          ['conv4_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 18, 18, 128)  32768       ['conv4_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_concat (Concatena  (None, 18, 18, 288)  0          ['pool3_pool[0][0]',             \n",
      " te)                                                              'conv4_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_0_bn (BatchNormal  (None, 18, 18, 288)  1152       ['conv4_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_0_relu (Activatio  (None, 18, 18, 288)  0          ['conv4_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 18, 18, 128)  36864       ['conv4_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_concat (Concatena  (None, 18, 18, 320)  0          ['conv4_block1_concat[0][0]',    \n",
      " te)                                                              'conv4_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_0_bn (BatchNormal  (None, 18, 18, 320)  1280       ['conv4_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_0_relu (Activatio  (None, 18, 18, 320)  0          ['conv4_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 18, 18, 128)  40960       ['conv4_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_concat (Concatena  (None, 18, 18, 352)  0          ['conv4_block2_concat[0][0]',    \n",
      " te)                                                              'conv4_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_0_bn (BatchNormal  (None, 18, 18, 352)  1408       ['conv4_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_0_relu (Activatio  (None, 18, 18, 352)  0          ['conv4_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 18, 18, 128)  45056       ['conv4_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_concat (Concatena  (None, 18, 18, 384)  0          ['conv4_block3_concat[0][0]',    \n",
      " te)                                                              'conv4_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_0_bn (BatchNormal  (None, 18, 18, 384)  1536       ['conv4_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_0_relu (Activatio  (None, 18, 18, 384)  0          ['conv4_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 18, 18, 128)  49152       ['conv4_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_concat (Concatena  (None, 18, 18, 416)  0          ['conv4_block4_concat[0][0]',    \n",
      " te)                                                              'conv4_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_0_bn (BatchNormal  (None, 18, 18, 416)  1664       ['conv4_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_0_relu (Activatio  (None, 18, 18, 416)  0          ['conv4_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 18, 18, 128)  53248       ['conv4_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_concat (Concatena  (None, 18, 18, 448)  0          ['conv4_block5_concat[0][0]',    \n",
      " te)                                                              'conv4_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_0_bn (BatchNormal  (None, 18, 18, 448)  1792       ['conv4_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_0_relu (Activatio  (None, 18, 18, 448)  0          ['conv4_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_1_conv (Conv2D)   (None, 18, 18, 128)  57344       ['conv4_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_concat (Concatena  (None, 18, 18, 480)  0          ['conv4_block6_concat[0][0]',    \n",
      " te)                                                              'conv4_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_0_bn (BatchNormal  (None, 18, 18, 480)  1920       ['conv4_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_0_relu (Activatio  (None, 18, 18, 480)  0          ['conv4_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_1_conv (Conv2D)   (None, 18, 18, 128)  61440       ['conv4_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_concat (Concatena  (None, 18, 18, 512)  0          ['conv4_block7_concat[0][0]',    \n",
      " te)                                                              'conv4_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_0_bn (BatchNormal  (None, 18, 18, 512)  2048       ['conv4_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_0_relu (Activatio  (None, 18, 18, 512)  0          ['conv4_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_1_conv (Conv2D)   (None, 18, 18, 128)  65536       ['conv4_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_concat (Concatena  (None, 18, 18, 544)  0          ['conv4_block8_concat[0][0]',    \n",
      " te)                                                              'conv4_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block10_0_bn (BatchNorma  (None, 18, 18, 544)  2176       ['conv4_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_0_relu (Activati  (None, 18, 18, 544)  0          ['conv4_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_1_conv (Conv2D)  (None, 18, 18, 128)  69632       ['conv4_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_concat (Concaten  (None, 18, 18, 576)  0          ['conv4_block9_concat[0][0]',    \n",
      " ate)                                                             'conv4_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_0_bn (BatchNorma  (None, 18, 18, 576)  2304       ['conv4_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_0_relu (Activati  (None, 18, 18, 576)  0          ['conv4_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_1_conv (Conv2D)  (None, 18, 18, 128)  73728       ['conv4_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_concat (Concaten  (None, 18, 18, 608)  0          ['conv4_block10_concat[0][0]',   \n",
      " ate)                                                             'conv4_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_0_bn (BatchNorma  (None, 18, 18, 608)  2432       ['conv4_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_0_relu (Activati  (None, 18, 18, 608)  0          ['conv4_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_1_conv (Conv2D)  (None, 18, 18, 128)  77824       ['conv4_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_concat (Concaten  (None, 18, 18, 640)  0          ['conv4_block11_concat[0][0]',   \n",
      " ate)                                                             'conv4_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_0_bn (BatchNorma  (None, 18, 18, 640)  2560       ['conv4_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_0_relu (Activati  (None, 18, 18, 640)  0          ['conv4_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_1_conv (Conv2D)  (None, 18, 18, 128)  81920       ['conv4_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_concat (Concaten  (None, 18, 18, 672)  0          ['conv4_block12_concat[0][0]',   \n",
      " ate)                                                             'conv4_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_0_bn (BatchNorma  (None, 18, 18, 672)  2688       ['conv4_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_0_relu (Activati  (None, 18, 18, 672)  0          ['conv4_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_1_conv (Conv2D)  (None, 18, 18, 128)  86016       ['conv4_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_concat (Concaten  (None, 18, 18, 704)  0          ['conv4_block13_concat[0][0]',   \n",
      " ate)                                                             'conv4_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_0_bn (BatchNorma  (None, 18, 18, 704)  2816       ['conv4_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_0_relu (Activati  (None, 18, 18, 704)  0          ['conv4_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_1_conv (Conv2D)  (None, 18, 18, 128)  90112       ['conv4_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block15_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_concat (Concaten  (None, 18, 18, 736)  0          ['conv4_block14_concat[0][0]',   \n",
      " ate)                                                             'conv4_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_0_bn (BatchNorma  (None, 18, 18, 736)  2944       ['conv4_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_0_relu (Activati  (None, 18, 18, 736)  0          ['conv4_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_1_conv (Conv2D)  (None, 18, 18, 128)  94208       ['conv4_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_concat (Concaten  (None, 18, 18, 768)  0          ['conv4_block15_concat[0][0]',   \n",
      " ate)                                                             'conv4_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_0_bn (BatchNorma  (None, 18, 18, 768)  3072       ['conv4_block16_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_0_relu (Activati  (None, 18, 18, 768)  0          ['conv4_block17_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_1_conv (Conv2D)  (None, 18, 18, 128)  98304       ['conv4_block17_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block17_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block17_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block17_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_concat (Concaten  (None, 18, 18, 800)  0          ['conv4_block16_concat[0][0]',   \n",
      " ate)                                                             'conv4_block17_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_0_bn (BatchNorma  (None, 18, 18, 800)  3200       ['conv4_block17_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_0_relu (Activati  (None, 18, 18, 800)  0          ['conv4_block18_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_1_conv (Conv2D)  (None, 18, 18, 128)  102400      ['conv4_block18_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block18_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block18_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block18_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_concat (Concaten  (None, 18, 18, 832)  0          ['conv4_block17_concat[0][0]',   \n",
      " ate)                                                             'conv4_block18_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_0_bn (BatchNorma  (None, 18, 18, 832)  3328       ['conv4_block18_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_0_relu (Activati  (None, 18, 18, 832)  0          ['conv4_block19_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_1_conv (Conv2D)  (None, 18, 18, 128)  106496      ['conv4_block19_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block19_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block19_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block19_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_concat (Concaten  (None, 18, 18, 864)  0          ['conv4_block18_concat[0][0]',   \n",
      " ate)                                                             'conv4_block19_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_0_bn (BatchNorma  (None, 18, 18, 864)  3456       ['conv4_block19_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block20_0_relu (Activati  (None, 18, 18, 864)  0          ['conv4_block20_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_1_conv (Conv2D)  (None, 18, 18, 128)  110592      ['conv4_block20_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block20_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block20_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block20_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block20_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_concat (Concaten  (None, 18, 18, 896)  0          ['conv4_block19_concat[0][0]',   \n",
      " ate)                                                             'conv4_block20_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_0_bn (BatchNorma  (None, 18, 18, 896)  3584       ['conv4_block20_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_0_relu (Activati  (None, 18, 18, 896)  0          ['conv4_block21_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_1_conv (Conv2D)  (None, 18, 18, 128)  114688      ['conv4_block21_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block21_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block21_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block21_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_concat (Concaten  (None, 18, 18, 928)  0          ['conv4_block20_concat[0][0]',   \n",
      " ate)                                                             'conv4_block21_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_0_bn (BatchNorma  (None, 18, 18, 928)  3712       ['conv4_block21_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_0_relu (Activati  (None, 18, 18, 928)  0          ['conv4_block22_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_1_conv (Conv2D)  (None, 18, 18, 128)  118784      ['conv4_block22_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block22_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block22_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block22_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_concat (Concaten  (None, 18, 18, 960)  0          ['conv4_block21_concat[0][0]',   \n",
      " ate)                                                             'conv4_block22_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_0_bn (BatchNorma  (None, 18, 18, 960)  3840       ['conv4_block22_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_0_relu (Activati  (None, 18, 18, 960)  0          ['conv4_block23_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_1_conv (Conv2D)  (None, 18, 18, 128)  122880      ['conv4_block23_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block23_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block23_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block23_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_concat (Concaten  (None, 18, 18, 992)  0          ['conv4_block22_concat[0][0]',   \n",
      " ate)                                                             'conv4_block23_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_0_bn (BatchNorma  (None, 18, 18, 992)  3968       ['conv4_block23_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block24_0_relu (Activati  (None, 18, 18, 992)  0          ['conv4_block24_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_1_conv (Conv2D)  (None, 18, 18, 128)  126976      ['conv4_block24_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block24_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block24_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block24_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block24_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_concat (Concaten  (None, 18, 18, 1024  0          ['conv4_block23_concat[0][0]',   \n",
      " ate)                           )                                 'conv4_block24_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool4_bn (BatchNormalization)  (None, 18, 18, 1024  4096        ['conv4_block24_concat[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool4_relu (Activation)        (None, 18, 18, 1024  0           ['pool4_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool4_conv (Conv2D)            (None, 18, 18, 512)  524288      ['pool4_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool4_pool (AveragePooling2D)  (None, 9, 9, 512)    0           ['pool4_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 9, 9, 512)   2048        ['pool4_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_0_relu (Activatio  (None, 9, 9, 512)   0           ['conv5_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 9, 9, 128)    65536       ['conv5_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_concat (Concatena  (None, 9, 9, 544)   0           ['pool4_pool[0][0]',             \n",
      " te)                                                              'conv5_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_0_bn (BatchNormal  (None, 9, 9, 544)   2176        ['conv5_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_0_relu (Activatio  (None, 9, 9, 544)   0           ['conv5_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 9, 9, 128)    69632       ['conv5_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_concat (Concatena  (None, 9, 9, 576)   0           ['conv5_block1_concat[0][0]',    \n",
      " te)                                                              'conv5_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_0_bn (BatchNormal  (None, 9, 9, 576)   2304        ['conv5_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_0_relu (Activatio  (None, 9, 9, 576)   0           ['conv5_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 9, 9, 128)    73728       ['conv5_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_concat (Concatena  (None, 9, 9, 608)   0           ['conv5_block2_concat[0][0]',    \n",
      " te)                                                              'conv5_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_0_bn (BatchNormal  (None, 9, 9, 608)   2432        ['conv5_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_0_relu (Activatio  (None, 9, 9, 608)   0           ['conv5_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_1_conv (Conv2D)   (None, 9, 9, 128)    77824       ['conv5_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_concat (Concatena  (None, 9, 9, 640)   0           ['conv5_block3_concat[0][0]',    \n",
      " te)                                                              'conv5_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_0_bn (BatchNormal  (None, 9, 9, 640)   2560        ['conv5_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_0_relu (Activatio  (None, 9, 9, 640)   0           ['conv5_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_1_conv (Conv2D)   (None, 9, 9, 128)    81920       ['conv5_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_concat (Concatena  (None, 9, 9, 672)   0           ['conv5_block4_concat[0][0]',    \n",
      " te)                                                              'conv5_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_0_bn (BatchNormal  (None, 9, 9, 672)   2688        ['conv5_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_0_relu (Activatio  (None, 9, 9, 672)   0           ['conv5_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_1_conv (Conv2D)   (None, 9, 9, 128)    86016       ['conv5_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_concat (Concatena  (None, 9, 9, 704)   0           ['conv5_block5_concat[0][0]',    \n",
      " te)                                                              'conv5_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_0_bn (BatchNormal  (None, 9, 9, 704)   2816        ['conv5_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_0_relu (Activatio  (None, 9, 9, 704)   0           ['conv5_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_1_conv (Conv2D)   (None, 9, 9, 128)    90112       ['conv5_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_concat (Concatena  (None, 9, 9, 736)   0           ['conv5_block6_concat[0][0]',    \n",
      " te)                                                              'conv5_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_0_bn (BatchNormal  (None, 9, 9, 736)   2944        ['conv5_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_0_relu (Activatio  (None, 9, 9, 736)   0           ['conv5_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block8_1_conv (Conv2D)   (None, 9, 9, 128)    94208       ['conv5_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block8_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_concat (Concatena  (None, 9, 9, 768)   0           ['conv5_block7_concat[0][0]',    \n",
      " te)                                                              'conv5_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_0_bn (BatchNormal  (None, 9, 9, 768)   3072        ['conv5_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_0_relu (Activatio  (None, 9, 9, 768)   0           ['conv5_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_1_conv (Conv2D)   (None, 9, 9, 128)    98304       ['conv5_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_concat (Concatena  (None, 9, 9, 800)   0           ['conv5_block8_concat[0][0]',    \n",
      " te)                                                              'conv5_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block10_0_bn (BatchNorma  (None, 9, 9, 800)   3200        ['conv5_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_0_relu (Activati  (None, 9, 9, 800)   0           ['conv5_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_1_conv (Conv2D)  (None, 9, 9, 128)    102400      ['conv5_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_concat (Concaten  (None, 9, 9, 832)   0           ['conv5_block9_concat[0][0]',    \n",
      " ate)                                                             'conv5_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_0_bn (BatchNorma  (None, 9, 9, 832)   3328        ['conv5_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_0_relu (Activati  (None, 9, 9, 832)   0           ['conv5_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_1_conv (Conv2D)  (None, 9, 9, 128)    106496      ['conv5_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_concat (Concaten  (None, 9, 9, 864)   0           ['conv5_block10_concat[0][0]',   \n",
      " ate)                                                             'conv5_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_0_bn (BatchNorma  (None, 9, 9, 864)   3456        ['conv5_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_0_relu (Activati  (None, 9, 9, 864)   0           ['conv5_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_1_conv (Conv2D)  (None, 9, 9, 128)    110592      ['conv5_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_concat (Concaten  (None, 9, 9, 896)   0           ['conv5_block11_concat[0][0]',   \n",
      " ate)                                                             'conv5_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_0_bn (BatchNorma  (None, 9, 9, 896)   3584        ['conv5_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block13_0_relu (Activati  (None, 9, 9, 896)   0           ['conv5_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_1_conv (Conv2D)  (None, 9, 9, 128)    114688      ['conv5_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block13_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_concat (Concaten  (None, 9, 9, 928)   0           ['conv5_block12_concat[0][0]',   \n",
      " ate)                                                             'conv5_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_0_bn (BatchNorma  (None, 9, 9, 928)   3712        ['conv5_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_0_relu (Activati  (None, 9, 9, 928)   0           ['conv5_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_1_conv (Conv2D)  (None, 9, 9, 128)    118784      ['conv5_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_concat (Concaten  (None, 9, 9, 960)   0           ['conv5_block13_concat[0][0]',   \n",
      " ate)                                                             'conv5_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_0_bn (BatchNorma  (None, 9, 9, 960)   3840        ['conv5_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_0_relu (Activati  (None, 9, 9, 960)   0           ['conv5_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_1_conv (Conv2D)  (None, 9, 9, 128)    122880      ['conv5_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block15_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_concat (Concaten  (None, 9, 9, 992)   0           ['conv5_block14_concat[0][0]',   \n",
      " ate)                                                             'conv5_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_0_bn (BatchNorma  (None, 9, 9, 992)   3968        ['conv5_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_0_relu (Activati  (None, 9, 9, 992)   0           ['conv5_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_1_conv (Conv2D)  (None, 9, 9, 128)    126976      ['conv5_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_concat (Concaten  (None, 9, 9, 1024)  0           ['conv5_block15_concat[0][0]',   \n",
      " ate)                                                             'conv5_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " bn (BatchNormalization)        (None, 9, 9, 1024)   4096        ['conv5_block16_concat[0][0]']   \n",
      "                                                                                                  \n",
      " relu (Activation)              (None, 9, 9, 1024)   0           ['bn[0][0]']                     \n",
      "                                                                                                  \n",
      " avg_pool (GlobalAveragePooling  (None, 1024)        0           ['relu[0][0]']                   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['avg_pool[0][0]']               \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 3)            3075        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,037,443\n",
      "Trainable params: 6,953,795\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(300, 300, 2))\n",
    "model = DenseNet121(input_tensor=input, include_top=False, weights=None, pooling='avg')\n",
    " \n",
    "x = model.output\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(3, activation='softmax', name='softmax')(x)\n",
    "\n",
    "model = Model(model.input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class LearningRateSchedule(Callback):\n",
    "    def __init__(self, selected_epochs=[]):\n",
    "        self.selected_epochs = selected_epochs\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch+1) in self.selected_epochs:\n",
    "            lr = K.get_value(self.model.optimizer.lr)\n",
    "            K.set_value(self.model.optimizer.lr, lr*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  log   ()\n",
    "import datetime\n",
    "\n",
    "log_dir = \"logs/my_board/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "#    \n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= log_dir, histogram_freq= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "117\n",
      "1062\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "test = []\n",
    "train= []\n",
    "test_ = []\n",
    "train_ = []\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    print(len(test))\n",
    "    print(len(train))\n",
    "    for i in zip(test):\n",
    "        test_.append(i)\n",
    "    for i in zip(train):\n",
    "        train_.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_[0:1061]\n",
    "train = np.reshape(train, 1061)\n",
    "test = test_[0:117]\n",
    "test = np.reshape(test, 117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   9   17   34   40   46   53   54   83   97  101  109  113  128  140\n",
      "  141  146  168  171  179  186  204  228  253  263  264  280  306  316\n",
      "  358  372  374  377  379  389  412  422  431  432  446  449  459  463\n",
      "  467  473  484  492  508  509  510  513  523  551  564  571  597  602\n",
      "  620  631  632  642  663  665  666  685  698  716  724  729  732  734\n",
      "  756  761  771  793  797  804  815  819  831  836  840  841  879  883\n",
      "  884  890  907  912  915  920  936  941  946  955  963  975  982  983\n",
      "  985 1003 1013 1025 1047 1050 1053 1067 1070 1077 1083 1102 1105 1111\n",
      " 1120 1154 1166 1172 1177]\n",
      "[   0    1    2 ... 1174 1175 1176]\n",
      "['P_282' 'N_252' 'P_327' 'H_260' 'N_303' 'H_232' 'P_44' 'N_110' 'P_0'\n",
      " 'H_369' 'P_191' 'N_244' 'N_178' 'H_137' 'H_156' 'P_220' 'H_277' 'P_382'\n",
      " 'H_108' 'N_79' 'H_45' 'H_12' 'N_85' 'P_153' 'N_185' 'P_82' 'N_46' 'H_375'\n",
      " 'P_80' 'N_120' 'H_192' 'P_328' 'H_377' 'N_247' 'H_251' 'H_217' 'N_52'\n",
      " 'P_193' 'P_173' 'P_62' 'N_336' 'N_275' 'P_242' 'N_352' 'N_194' 'P_371'\n",
      " 'H_297' 'P_6' 'N_91' 'P_362' 'H_336' 'N_216' 'P_84' 'P_387' 'H_314'\n",
      " 'P_11' 'P_158' 'H_326' 'P_295' 'N_73' 'N_358' 'H_24' 'N_47' 'P_115'\n",
      " 'N_219' 'P_223' 'N_323' 'P_9' 'H_330' 'N_217' 'P_225' 'H_339' 'P_22'\n",
      " 'N_325' 'H_386' 'H_100' 'H_252' 'P_68' 'P_139' 'P_163' 'N_239' 'P_329'\n",
      " 'N_224' 'H_173' 'P_336' 'H_79' 'H_38' 'P_231' 'H_107' 'N_302' 'H_328'\n",
      " 'H_48' 'H_281' 'N_49' 'H_381' 'N_281' 'N_14' 'N_199' 'N_215' 'H_273'\n",
      " 'H_106' 'N_367' 'N_322' 'P_350' 'H_123' 'H_244' 'H_93' 'H_103' 'H_254'\n",
      " 'N_364' 'H_0' 'P_278' 'P_151' 'P_159' 'P_283' 'P_290' 'N_381']\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(train)\n",
    "print(index[test])\n",
    "print(targets[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.mkdir('E:/Result/ver.3.28/MTF/'+ 'weight_')\n",
    "os.mkdir('E:/Result/ver.3.28/MTF/'+ 'train_')\n",
    "os.mkdir('E:/Result/ver.3.28/MTF/'+ 'test_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(848,) (95,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jwhyu\\anaconda3\\envs\\venv\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/100\n",
      "212/212 [==============================] - 36s 118ms/step - loss: 0.8290 - accuracy: 0.6545 - val_loss: 4.7757 - val_accuracy: 0.3263\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.5583 - accuracy: 0.7665 - val_loss: 0.6934 - val_accuracy: 0.6421\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.4956 - accuracy: 0.7936 - val_loss: 0.4727 - val_accuracy: 0.7474\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.4105 - accuracy: 0.8290 - val_loss: 0.3235 - val_accuracy: 0.8632\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.4034 - accuracy: 0.8514 - val_loss: 11.1846 - val_accuracy: 0.4316\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.3564 - accuracy: 0.8491 - val_loss: 0.2914 - val_accuracy: 0.8526\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.3707 - accuracy: 0.8479 - val_loss: 0.9071 - val_accuracy: 0.6737\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.3297 - accuracy: 0.8774 - val_loss: 0.1590 - val_accuracy: 0.9579\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.2289 - accuracy: 0.9092 - val_loss: 0.5188 - val_accuracy: 0.8316\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.2573 - accuracy: 0.9009 - val_loss: 0.4430 - val_accuracy: 0.7789\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.2185 - accuracy: 0.9186 - val_loss: 0.3623 - val_accuracy: 0.8526\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.2388 - accuracy: 0.9186 - val_loss: 1.5557 - val_accuracy: 0.6842\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.2175 - accuracy: 0.9269 - val_loss: 0.4421 - val_accuracy: 0.8632\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.1633 - accuracy: 0.9399 - val_loss: 0.5635 - val_accuracy: 0.8421\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.1751 - accuracy: 0.9316 - val_loss: 0.3271 - val_accuracy: 0.8842\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.1730 - accuracy: 0.9375 - val_loss: 0.2245 - val_accuracy: 0.9263\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.1514 - accuracy: 0.9410 - val_loss: 0.2329 - val_accuracy: 0.9368\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.1488 - accuracy: 0.9517 - val_loss: 0.2087 - val_accuracy: 0.9158\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.1320 - accuracy: 0.9611 - val_loss: 0.5105 - val_accuracy: 0.7789\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.1276 - accuracy: 0.9552 - val_loss: 0.3983 - val_accuracy: 0.9158\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.1299 - accuracy: 0.9587 - val_loss: 0.4697 - val_accuracy: 0.8526\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.1091 - accuracy: 0.9658 - val_loss: 0.6574 - val_accuracy: 0.8211\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0892 - accuracy: 0.9705 - val_loss: 0.7385 - val_accuracy: 0.7789\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0747 - accuracy: 0.9705 - val_loss: 0.3311 - val_accuracy: 0.9158\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.1446 - accuracy: 0.9564 - val_loss: 0.2242 - val_accuracy: 0.9158\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0846 - accuracy: 0.9752 - val_loss: 0.2053 - val_accuracy: 0.9158\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0638 - accuracy: 0.9776 - val_loss: 0.8898 - val_accuracy: 0.6947\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0364 - accuracy: 0.9870 - val_loss: 0.3134 - val_accuracy: 0.8947\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0406 - accuracy: 0.9917 - val_loss: 0.2183 - val_accuracy: 0.9368\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0660 - accuracy: 0.9811 - val_loss: 0.2469 - val_accuracy: 0.8947\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.1056 - accuracy: 0.9564 - val_loss: 0.4727 - val_accuracy: 0.8947\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0595 - accuracy: 0.9847 - val_loss: 0.2005 - val_accuracy: 0.9158\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0763 - accuracy: 0.9764 - val_loss: 0.1773 - val_accuracy: 0.9579\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0512 - accuracy: 0.9847 - val_loss: 0.2252 - val_accuracy: 0.9263\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0630 - accuracy: 0.9811 - val_loss: 0.3015 - val_accuracy: 0.9263\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0974 - accuracy: 0.9670 - val_loss: 0.2777 - val_accuracy: 0.8947\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0525 - accuracy: 0.9835 - val_loss: 0.2733 - val_accuracy: 0.9053\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0726 - accuracy: 0.9752 - val_loss: 0.2930 - val_accuracy: 0.9158\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0945 - accuracy: 0.9634 - val_loss: 0.1689 - val_accuracy: 0.9263\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0519 - accuracy: 0.9811 - val_loss: 0.2065 - val_accuracy: 0.9263\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0546 - accuracy: 0.9811 - val_loss: 0.4897 - val_accuracy: 0.8737\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0289 - accuracy: 0.9917 - val_loss: 0.3465 - val_accuracy: 0.9158\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0302 - accuracy: 0.9941 - val_loss: 0.2145 - val_accuracy: 0.9158\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0511 - accuracy: 0.9823 - val_loss: 0.1930 - val_accuracy: 0.9263\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0579 - accuracy: 0.9800 - val_loss: 0.1664 - val_accuracy: 0.9368\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0530 - accuracy: 0.9847 - val_loss: 0.2834 - val_accuracy: 0.9158\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0534 - accuracy: 0.9811 - val_loss: 0.6735 - val_accuracy: 0.8737\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0441 - accuracy: 0.9870 - val_loss: 0.2080 - val_accuracy: 0.9474\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0429 - accuracy: 0.9847 - val_loss: 0.4954 - val_accuracy: 0.8737\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0396 - accuracy: 0.9894 - val_loss: 0.2588 - val_accuracy: 0.9368\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0597 - accuracy: 0.9811 - val_loss: 0.3839 - val_accuracy: 0.9158\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0229 - accuracy: 0.9941 - val_loss: 0.1624 - val_accuracy: 0.9368\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0486 - accuracy: 0.9800 - val_loss: 0.2227 - val_accuracy: 0.9263\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0398 - accuracy: 0.9835 - val_loss: 0.1902 - val_accuracy: 0.9474\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0306 - accuracy: 0.9882 - val_loss: 0.1799 - val_accuracy: 0.9263\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0308 - accuracy: 0.9917 - val_loss: 0.1703 - val_accuracy: 0.9368\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0407 - accuracy: 0.9858 - val_loss: 0.2389 - val_accuracy: 0.9158\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0347 - accuracy: 0.9870 - val_loss: 0.1878 - val_accuracy: 0.9474\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0633 - accuracy: 0.9800 - val_loss: 0.1698 - val_accuracy: 0.9368\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0318 - accuracy: 0.9929 - val_loss: 0.1575 - val_accuracy: 0.9368\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0282 - accuracy: 0.9906 - val_loss: 0.1763 - val_accuracy: 0.9474\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0377 - accuracy: 0.9882 - val_loss: 0.1524 - val_accuracy: 0.9474\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0278 - accuracy: 0.9894 - val_loss: 0.2061 - val_accuracy: 0.9368\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0309 - accuracy: 0.9906 - val_loss: 0.1511 - val_accuracy: 0.9474\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0319 - accuracy: 0.9906 - val_loss: 0.1647 - val_accuracy: 0.9368\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0529 - accuracy: 0.9800 - val_loss: 0.3066 - val_accuracy: 0.8842\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0381 - accuracy: 0.9823 - val_loss: 0.1567 - val_accuracy: 0.9368\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0234 - accuracy: 0.9941 - val_loss: 0.2260 - val_accuracy: 0.9158\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0449 - accuracy: 0.9847 - val_loss: 0.1433 - val_accuracy: 0.9368\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0286 - accuracy: 0.9906 - val_loss: 0.1335 - val_accuracy: 0.9368\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0172 - accuracy: 0.9953 - val_loss: 0.1542 - val_accuracy: 0.9474\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0253 - accuracy: 0.9929 - val_loss: 0.2038 - val_accuracy: 0.9263\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0140 - accuracy: 0.9965 - val_loss: 0.1552 - val_accuracy: 0.9579\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0420 - accuracy: 0.9835 - val_loss: 0.3478 - val_accuracy: 0.8947\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0209 - accuracy: 0.9941 - val_loss: 0.1186 - val_accuracy: 0.9474\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0315 - accuracy: 0.9882 - val_loss: 0.1955 - val_accuracy: 0.9368\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0224 - accuracy: 0.9941 - val_loss: 0.1749 - val_accuracy: 0.9579\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0300 - accuracy: 0.9894 - val_loss: 0.1782 - val_accuracy: 0.9368\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0295 - accuracy: 0.9882 - val_loss: 0.1559 - val_accuracy: 0.9263\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0254 - accuracy: 0.9906 - val_loss: 0.2503 - val_accuracy: 0.9263\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0124 - accuracy: 0.9976 - val_loss: 0.1508 - val_accuracy: 0.9579\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0188 - accuracy: 0.9941 - val_loss: 0.1771 - val_accuracy: 0.9474\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0234 - accuracy: 0.9941 - val_loss: 0.1890 - val_accuracy: 0.9158\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0286 - accuracy: 0.9882 - val_loss: 0.1413 - val_accuracy: 0.9474\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0292 - accuracy: 0.9882 - val_loss: 0.1511 - val_accuracy: 0.9263\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0213 - accuracy: 0.9929 - val_loss: 0.1450 - val_accuracy: 0.9474\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0187 - accuracy: 0.9941 - val_loss: 0.1561 - val_accuracy: 0.9368\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0262 - accuracy: 0.9894 - val_loss: 0.1629 - val_accuracy: 0.9474\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0155 - accuracy: 0.9953 - val_loss: 0.1279 - val_accuracy: 0.9474\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0180 - accuracy: 0.9953 - val_loss: 0.1846 - val_accuracy: 0.9158\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0262 - accuracy: 0.9906 - val_loss: 0.1970 - val_accuracy: 0.9158\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0193 - accuracy: 0.9941 - val_loss: 0.1553 - val_accuracy: 0.9368\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0351 - accuracy: 0.9847 - val_loss: 0.1841 - val_accuracy: 0.9368\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0147 - accuracy: 0.9965 - val_loss: 0.2148 - val_accuracy: 0.9474\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0318 - accuracy: 0.9882 - val_loss: 0.1694 - val_accuracy: 0.9368\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0298 - accuracy: 0.9882 - val_loss: 0.1772 - val_accuracy: 0.9474\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0194 - accuracy: 0.9917 - val_loss: 0.1711 - val_accuracy: 0.9368\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0113 - accuracy: 0.9965 - val_loss: 0.1490 - val_accuracy: 0.9368\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0090 - accuracy: 0.9988 - val_loss: 0.1914 - val_accuracy: 0.9474\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0222 - accuracy: 0.9917 - val_loss: 0.1634 - val_accuracy: 0.9474\n",
      "Score for fold 1: loss of 0.16335202753543854; accuracy of 94.73684430122375%\n",
      "(848,) (95,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "212/212 [==============================] - 29s 111ms/step - loss: 0.8943 - accuracy: 0.6203 - val_loss: 4.6540 - val_accuracy: 0.3263\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.5489 - accuracy: 0.7724 - val_loss: 1.1604 - val_accuracy: 0.6421\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.4437 - accuracy: 0.8066 - val_loss: 0.5405 - val_accuracy: 0.8000\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.4014 - accuracy: 0.8231 - val_loss: 0.9555 - val_accuracy: 0.6737\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.3730 - accuracy: 0.8620 - val_loss: 0.3849 - val_accuracy: 0.8316\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.3550 - accuracy: 0.8455 - val_loss: 1.0862 - val_accuracy: 0.7684\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.3176 - accuracy: 0.8785 - val_loss: 0.6661 - val_accuracy: 0.8105\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.2555 - accuracy: 0.8856 - val_loss: 0.6721 - val_accuracy: 0.8526\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.2185 - accuracy: 0.9210 - val_loss: 1.2187 - val_accuracy: 0.6737\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.2426 - accuracy: 0.9080 - val_loss: 0.3847 - val_accuracy: 0.8316\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.2069 - accuracy: 0.9257 - val_loss: 1.0535 - val_accuracy: 0.7158\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.1529 - accuracy: 0.9505 - val_loss: 0.4179 - val_accuracy: 0.8526\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.2518 - accuracy: 0.9116 - val_loss: 1.2775 - val_accuracy: 0.6737\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0987 - accuracy: 0.9646 - val_loss: 0.5185 - val_accuracy: 0.8211\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.1882 - accuracy: 0.9340 - val_loss: 0.4855 - val_accuracy: 0.8211\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.1319 - accuracy: 0.9540 - val_loss: 0.4272 - val_accuracy: 0.8526\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.1180 - accuracy: 0.9587 - val_loss: 0.4455 - val_accuracy: 0.7895\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.1222 - accuracy: 0.9611 - val_loss: 0.4059 - val_accuracy: 0.8526\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0888 - accuracy: 0.9741 - val_loss: 0.6150 - val_accuracy: 0.8632\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0899 - accuracy: 0.9682 - val_loss: 0.4310 - val_accuracy: 0.8842\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0745 - accuracy: 0.9729 - val_loss: 0.2881 - val_accuracy: 0.8737\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.1089 - accuracy: 0.9564 - val_loss: 0.7156 - val_accuracy: 0.8421\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0699 - accuracy: 0.9811 - val_loss: 1.3837 - val_accuracy: 0.7053\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0856 - accuracy: 0.9764 - val_loss: 0.4290 - val_accuracy: 0.8421\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0587 - accuracy: 0.9847 - val_loss: 0.9481 - val_accuracy: 0.8000\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0565 - accuracy: 0.9823 - val_loss: 0.3940 - val_accuracy: 0.8842\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0886 - accuracy: 0.9717 - val_loss: 0.6174 - val_accuracy: 0.7684\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0805 - accuracy: 0.9705 - val_loss: 0.6271 - val_accuracy: 0.8526\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0431 - accuracy: 0.9858 - val_loss: 0.4856 - val_accuracy: 0.8211\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0417 - accuracy: 0.9870 - val_loss: 0.3619 - val_accuracy: 0.8947\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0678 - accuracy: 0.9835 - val_loss: 0.9299 - val_accuracy: 0.7789\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0306 - accuracy: 0.9917 - val_loss: 0.3614 - val_accuracy: 0.8737\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0576 - accuracy: 0.9847 - val_loss: 0.5627 - val_accuracy: 0.8105\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0379 - accuracy: 0.9894 - val_loss: 0.3003 - val_accuracy: 0.8632\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0616 - accuracy: 0.9788 - val_loss: 0.5252 - val_accuracy: 0.8105\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0493 - accuracy: 0.9800 - val_loss: 0.4487 - val_accuracy: 0.8842\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0307 - accuracy: 0.9906 - val_loss: 0.6658 - val_accuracy: 0.8211\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0332 - accuracy: 0.9917 - val_loss: 0.4903 - val_accuracy: 0.8526\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0399 - accuracy: 0.9870 - val_loss: 0.6454 - val_accuracy: 0.7789\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0335 - accuracy: 0.9870 - val_loss: 0.4818 - val_accuracy: 0.8526\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0337 - accuracy: 0.9870 - val_loss: 0.3646 - val_accuracy: 0.8842\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0459 - accuracy: 0.9858 - val_loss: 0.3899 - val_accuracy: 0.8737\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0455 - accuracy: 0.9800 - val_loss: 0.4221 - val_accuracy: 0.8316\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0451 - accuracy: 0.9858 - val_loss: 0.9153 - val_accuracy: 0.8000\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0336 - accuracy: 0.9894 - val_loss: 0.4732 - val_accuracy: 0.8421\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0482 - accuracy: 0.9835 - val_loss: 0.5982 - val_accuracy: 0.8526\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0293 - accuracy: 0.9929 - val_loss: 0.4602 - val_accuracy: 0.8737\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0438 - accuracy: 0.9811 - val_loss: 0.3841 - val_accuracy: 0.8526\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0208 - accuracy: 0.9906 - val_loss: 0.3376 - val_accuracy: 0.8632\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0178 - accuracy: 0.9941 - val_loss: 0.3496 - val_accuracy: 0.8737\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0536 - accuracy: 0.9788 - val_loss: 0.3825 - val_accuracy: 0.8632\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0296 - accuracy: 0.9870 - val_loss: 0.3449 - val_accuracy: 0.8842\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0262 - accuracy: 0.9929 - val_loss: 0.3114 - val_accuracy: 0.8842\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0462 - accuracy: 0.9882 - val_loss: 0.3279 - val_accuracy: 0.8842\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0285 - accuracy: 0.9882 - val_loss: 0.6607 - val_accuracy: 0.8316\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0467 - accuracy: 0.9811 - val_loss: 0.2846 - val_accuracy: 0.8947\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0109 - accuracy: 0.9976 - val_loss: 0.3106 - val_accuracy: 0.9053\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0182 - accuracy: 0.9917 - val_loss: 0.3039 - val_accuracy: 0.9053\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0169 - accuracy: 0.9929 - val_loss: 0.4082 - val_accuracy: 0.8632\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0273 - accuracy: 0.9882 - val_loss: 0.5133 - val_accuracy: 0.8105\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0361 - accuracy: 0.9870 - val_loss: 0.3808 - val_accuracy: 0.8737\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0322 - accuracy: 0.9882 - val_loss: 0.3006 - val_accuracy: 0.8842\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0148 - accuracy: 0.9976 - val_loss: 0.3154 - val_accuracy: 0.8947\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0258 - accuracy: 0.9906 - val_loss: 0.4319 - val_accuracy: 0.8000\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0096 - accuracy: 0.9965 - val_loss: 0.3271 - val_accuracy: 0.8947\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0109 - accuracy: 0.9965 - val_loss: 0.3180 - val_accuracy: 0.8947\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0321 - accuracy: 0.9870 - val_loss: 0.2812 - val_accuracy: 0.9053\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0327 - accuracy: 0.9870 - val_loss: 0.2509 - val_accuracy: 0.9158\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0323 - accuracy: 0.9870 - val_loss: 0.2863 - val_accuracy: 0.8947\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0168 - accuracy: 0.9941 - val_loss: 0.3340 - val_accuracy: 0.8842\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0307 - accuracy: 0.9882 - val_loss: 0.3748 - val_accuracy: 0.8316\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0212 - accuracy: 0.9953 - val_loss: 0.2857 - val_accuracy: 0.8947\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0175 - accuracy: 0.9929 - val_loss: 0.4131 - val_accuracy: 0.8842\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0209 - accuracy: 0.9941 - val_loss: 0.3479 - val_accuracy: 0.8632\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0261 - accuracy: 0.9906 - val_loss: 0.3171 - val_accuracy: 0.8842\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0319 - accuracy: 0.9894 - val_loss: 0.3651 - val_accuracy: 0.8632\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0261 - accuracy: 0.9894 - val_loss: 0.2967 - val_accuracy: 0.8947\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0285 - accuracy: 0.9894 - val_loss: 0.5033 - val_accuracy: 0.8737\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0252 - accuracy: 0.9894 - val_loss: 0.3325 - val_accuracy: 0.8632\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0131 - accuracy: 0.9965 - val_loss: 0.3167 - val_accuracy: 0.8632\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0297 - accuracy: 0.9882 - val_loss: 0.2937 - val_accuracy: 0.8737\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0066 - accuracy: 0.9988 - val_loss: 0.3291 - val_accuracy: 0.8842\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0253 - accuracy: 0.9917 - val_loss: 0.3377 - val_accuracy: 0.8842\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0209 - accuracy: 0.9917 - val_loss: 0.2533 - val_accuracy: 0.8947\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0252 - accuracy: 0.9894 - val_loss: 0.3092 - val_accuracy: 0.8632\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0134 - accuracy: 0.9965 - val_loss: 0.3462 - val_accuracy: 0.8842\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0083 - accuracy: 0.9988 - val_loss: 0.3468 - val_accuracy: 0.8842\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0350 - accuracy: 0.9823 - val_loss: 0.3336 - val_accuracy: 0.8947\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0153 - accuracy: 0.9941 - val_loss: 0.3688 - val_accuracy: 0.8526\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0284 - accuracy: 0.9858 - val_loss: 0.3611 - val_accuracy: 0.9053\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0109 - accuracy: 0.9965 - val_loss: 0.3974 - val_accuracy: 0.8737\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0092 - accuracy: 0.9965 - val_loss: 0.3512 - val_accuracy: 0.8632\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0114 - accuracy: 0.9965 - val_loss: 0.4050 - val_accuracy: 0.8632\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0197 - accuracy: 0.9917 - val_loss: 0.3312 - val_accuracy: 0.8632\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0166 - accuracy: 0.9929 - val_loss: 0.3903 - val_accuracy: 0.8526\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0219 - accuracy: 0.9906 - val_loss: 0.3312 - val_accuracy: 0.8947\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0191 - accuracy: 0.9929 - val_loss: 0.2488 - val_accuracy: 0.9158\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0234 - accuracy: 0.9917 - val_loss: 0.3758 - val_accuracy: 0.8526\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0102 - accuracy: 0.9976 - val_loss: 0.3883 - val_accuracy: 0.8842\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.4035 - val_accuracy: 0.8737\n",
      "Score for fold 2: loss of 0.40351244807243347; accuracy of 87.36842274665833%\n",
      "(848,) (95,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "212/212 [==============================] - 29s 112ms/step - loss: 0.9042 - accuracy: 0.6368 - val_loss: 5.2390 - val_accuracy: 0.3263\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.6065 - accuracy: 0.7547 - val_loss: 1.1896 - val_accuracy: 0.6211\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.4931 - accuracy: 0.7913 - val_loss: 1.9748 - val_accuracy: 0.6105\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.4114 - accuracy: 0.8314 - val_loss: 0.2656 - val_accuracy: 0.8947\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.3957 - accuracy: 0.8373 - val_loss: 1.3481 - val_accuracy: 0.6737\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.3631 - accuracy: 0.8550 - val_loss: 0.2930 - val_accuracy: 0.9263\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.3299 - accuracy: 0.8797 - val_loss: 0.4622 - val_accuracy: 0.7684\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.3046 - accuracy: 0.8868 - val_loss: 0.2473 - val_accuracy: 0.8947\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.2650 - accuracy: 0.8927 - val_loss: 2.8137 - val_accuracy: 0.5158\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.3392 - accuracy: 0.8774 - val_loss: 0.9608 - val_accuracy: 0.5789\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.2644 - accuracy: 0.8880 - val_loss: 0.2833 - val_accuracy: 0.8842\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.1789 - accuracy: 0.9399 - val_loss: 0.2439 - val_accuracy: 0.8842\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.2315 - accuracy: 0.9057 - val_loss: 0.2157 - val_accuracy: 0.9158\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.1922 - accuracy: 0.9269 - val_loss: 1.3801 - val_accuracy: 0.7158\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.1926 - accuracy: 0.9410 - val_loss: 0.3624 - val_accuracy: 0.8421\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.1330 - accuracy: 0.9587 - val_loss: 0.4311 - val_accuracy: 0.8316\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.1423 - accuracy: 0.9481 - val_loss: 0.5318 - val_accuracy: 0.8526\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.1136 - accuracy: 0.9587 - val_loss: 2.7425 - val_accuracy: 0.6947\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.1266 - accuracy: 0.9528 - val_loss: 0.2847 - val_accuracy: 0.9053\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.1529 - accuracy: 0.9422 - val_loss: 0.8616 - val_accuracy: 0.7579\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.1002 - accuracy: 0.9623 - val_loss: 0.2583 - val_accuracy: 0.8947\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.1165 - accuracy: 0.9552 - val_loss: 0.3403 - val_accuracy: 0.9053\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.1043 - accuracy: 0.9658 - val_loss: 0.3375 - val_accuracy: 0.8737\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.1329 - accuracy: 0.9658 - val_loss: 0.2782 - val_accuracy: 0.8737\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0893 - accuracy: 0.9729 - val_loss: 0.2959 - val_accuracy: 0.8632\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0799 - accuracy: 0.9788 - val_loss: 0.3119 - val_accuracy: 0.9053\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0708 - accuracy: 0.9717 - val_loss: 2.1978 - val_accuracy: 0.6316\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0985 - accuracy: 0.9670 - val_loss: 0.3627 - val_accuracy: 0.8632\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0730 - accuracy: 0.9752 - val_loss: 0.3746 - val_accuracy: 0.8842\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0464 - accuracy: 0.9858 - val_loss: 0.4363 - val_accuracy: 0.8842\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0603 - accuracy: 0.9788 - val_loss: 0.7644 - val_accuracy: 0.8316\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0513 - accuracy: 0.9811 - val_loss: 0.2967 - val_accuracy: 0.9368\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0901 - accuracy: 0.9705 - val_loss: 0.3414 - val_accuracy: 0.8842\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0253 - accuracy: 0.9941 - val_loss: 0.3239 - val_accuracy: 0.8947\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0725 - accuracy: 0.9764 - val_loss: 0.5599 - val_accuracy: 0.8421\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0526 - accuracy: 0.9870 - val_loss: 0.4208 - val_accuracy: 0.8947\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0495 - accuracy: 0.9870 - val_loss: 0.4299 - val_accuracy: 0.9053\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0559 - accuracy: 0.9776 - val_loss: 0.3794 - val_accuracy: 0.9158\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0507 - accuracy: 0.9811 - val_loss: 0.3875 - val_accuracy: 0.8842\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0222 - accuracy: 0.9965 - val_loss: 0.5929 - val_accuracy: 0.8421\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0729 - accuracy: 0.9776 - val_loss: 0.4229 - val_accuracy: 0.8632\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0336 - accuracy: 0.9906 - val_loss: 0.3265 - val_accuracy: 0.9158\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0292 - accuracy: 0.9917 - val_loss: 0.3128 - val_accuracy: 0.9053\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0294 - accuracy: 0.9894 - val_loss: 0.6409 - val_accuracy: 0.8737\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0557 - accuracy: 0.9752 - val_loss: 0.4175 - val_accuracy: 0.9158\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0558 - accuracy: 0.9776 - val_loss: 0.3370 - val_accuracy: 0.9053\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0348 - accuracy: 0.9882 - val_loss: 0.3235 - val_accuracy: 0.8947\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0680 - accuracy: 0.9741 - val_loss: 0.3633 - val_accuracy: 0.8947\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0478 - accuracy: 0.9847 - val_loss: 0.4719 - val_accuracy: 0.8526\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0374 - accuracy: 0.9870 - val_loss: 0.3795 - val_accuracy: 0.9053\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0191 - accuracy: 0.9953 - val_loss: 0.3010 - val_accuracy: 0.9053\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0227 - accuracy: 0.9917 - val_loss: 0.3357 - val_accuracy: 0.9158\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0322 - accuracy: 0.9870 - val_loss: 0.3818 - val_accuracy: 0.9053\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0206 - accuracy: 0.9953 - val_loss: 0.3554 - val_accuracy: 0.9053\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0393 - accuracy: 0.9870 - val_loss: 0.2897 - val_accuracy: 0.9053\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0525 - accuracy: 0.9776 - val_loss: 0.3439 - val_accuracy: 0.9053\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0113 - accuracy: 0.9988 - val_loss: 0.3873 - val_accuracy: 0.9053\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0289 - accuracy: 0.9917 - val_loss: 0.3282 - val_accuracy: 0.9158\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0266 - accuracy: 0.9894 - val_loss: 0.2924 - val_accuracy: 0.9053\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0269 - accuracy: 0.9929 - val_loss: 0.2840 - val_accuracy: 0.9158\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0151 - accuracy: 0.9953 - val_loss: 0.3406 - val_accuracy: 0.9053\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0276 - accuracy: 0.9906 - val_loss: 0.6766 - val_accuracy: 0.8421\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0469 - accuracy: 0.9847 - val_loss: 0.3128 - val_accuracy: 0.8947\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0164 - accuracy: 0.9965 - val_loss: 0.3955 - val_accuracy: 0.9158\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0354 - accuracy: 0.9906 - val_loss: 0.2924 - val_accuracy: 0.9158\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0221 - accuracy: 0.9953 - val_loss: 0.3317 - val_accuracy: 0.9263\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0352 - accuracy: 0.9870 - val_loss: 0.2858 - val_accuracy: 0.9263\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0256 - accuracy: 0.9917 - val_loss: 0.3458 - val_accuracy: 0.8947\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0174 - accuracy: 0.9941 - val_loss: 0.4045 - val_accuracy: 0.9263\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 23s 108ms/step - loss: 0.0177 - accuracy: 0.9976 - val_loss: 0.4408 - val_accuracy: 0.9263\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0098 - accuracy: 0.9965 - val_loss: 0.4129 - val_accuracy: 0.9158\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0312 - accuracy: 0.9882 - val_loss: 0.3191 - val_accuracy: 0.8947\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0211 - accuracy: 0.9929 - val_loss: 0.4756 - val_accuracy: 0.9053\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0099 - accuracy: 0.9988 - val_loss: 0.4064 - val_accuracy: 0.9158\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0133 - accuracy: 0.9953 - val_loss: 0.3450 - val_accuracy: 0.9053\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0154 - accuracy: 0.9953 - val_loss: 0.3473 - val_accuracy: 0.9263\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 23s 107ms/step - loss: 0.0167 - accuracy: 0.9953 - val_loss: 0.3577 - val_accuracy: 0.9263\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0250 - accuracy: 0.9929 - val_loss: 0.3342 - val_accuracy: 0.9263\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0255 - accuracy: 0.9941 - val_loss: 0.4117 - val_accuracy: 0.8947\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0142 - accuracy: 0.9953 - val_loss: 0.3070 - val_accuracy: 0.9263\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0267 - accuracy: 0.9917 - val_loss: 0.3550 - val_accuracy: 0.9158\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0170 - accuracy: 0.9953 - val_loss: 0.3748 - val_accuracy: 0.9158\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0198 - accuracy: 0.9941 - val_loss: 0.3569 - val_accuracy: 0.9158\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0258 - accuracy: 0.9906 - val_loss: 0.6678 - val_accuracy: 0.8632\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0184 - accuracy: 0.9941 - val_loss: 0.3690 - val_accuracy: 0.9158\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0177 - accuracy: 0.9953 - val_loss: 0.4007 - val_accuracy: 0.9158\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0253 - accuracy: 0.9941 - val_loss: 0.4510 - val_accuracy: 0.9053\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0119 - accuracy: 0.9965 - val_loss: 0.3928 - val_accuracy: 0.9053\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0101 - accuracy: 0.9988 - val_loss: 0.3731 - val_accuracy: 0.9053\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0106 - accuracy: 0.9965 - val_loss: 0.4608 - val_accuracy: 0.9053\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0146 - accuracy: 0.9965 - val_loss: 0.3391 - val_accuracy: 0.9158\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0114 - accuracy: 0.9953 - val_loss: 0.3932 - val_accuracy: 0.9053\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0084 - accuracy: 0.9976 - val_loss: 0.4067 - val_accuracy: 0.9158\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0214 - accuracy: 0.9917 - val_loss: 0.3812 - val_accuracy: 0.9053\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 22s 105ms/step - loss: 0.0192 - accuracy: 0.9929 - val_loss: 0.4009 - val_accuracy: 0.9053\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0189 - accuracy: 0.9929 - val_loss: 0.4140 - val_accuracy: 0.9053\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0165 - accuracy: 0.9941 - val_loss: 0.3732 - val_accuracy: 0.9053\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0130 - accuracy: 0.9953 - val_loss: 0.3505 - val_accuracy: 0.9158\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 22s 106ms/step - loss: 0.0188 - accuracy: 0.9941 - val_loss: 0.3527 - val_accuracy: 0.9053\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 23s 106ms/step - loss: 0.0175 - accuracy: 0.9941 - val_loss: 0.3259 - val_accuracy: 0.9158\n",
      "Score for fold 3: loss of 0.3259102702140808; accuracy of 91.57894849777222%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 34s 133ms/step - loss: 0.8701 - accuracy: 0.6207 - val_loss: 1.3727 - val_accuracy: 0.5319\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.5725 - accuracy: 0.7550 - val_loss: 0.3939 - val_accuracy: 0.8085\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.4702 - accuracy: 0.8057 - val_loss: 0.5962 - val_accuracy: 0.8085\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.4094 - accuracy: 0.8469 - val_loss: 0.5337 - val_accuracy: 0.7979\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3373 - accuracy: 0.8563 - val_loss: 0.9335 - val_accuracy: 0.6596\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.3592 - accuracy: 0.8587 - val_loss: 0.5648 - val_accuracy: 0.7766\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.3226 - accuracy: 0.8716 - val_loss: 0.5263 - val_accuracy: 0.7553\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.2953 - accuracy: 0.8928 - val_loss: 1.2998 - val_accuracy: 0.6383\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.2825 - accuracy: 0.8846 - val_loss: 1.5482 - val_accuracy: 0.8085\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.2941 - accuracy: 0.8846 - val_loss: 0.5214 - val_accuracy: 0.8298\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.2171 - accuracy: 0.9164 - val_loss: 0.8884 - val_accuracy: 0.7553\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1908 - accuracy: 0.9223 - val_loss: 0.4345 - val_accuracy: 0.8298\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1878 - accuracy: 0.9258 - val_loss: 0.4225 - val_accuracy: 0.8617\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1650 - accuracy: 0.9458 - val_loss: 0.3098 - val_accuracy: 0.8298\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1240 - accuracy: 0.9541 - val_loss: 0.4083 - val_accuracy: 0.8617\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1444 - accuracy: 0.9458 - val_loss: 0.8307 - val_accuracy: 0.7766\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1483 - accuracy: 0.9446 - val_loss: 1.4877 - val_accuracy: 0.6596\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1242 - accuracy: 0.9541 - val_loss: 0.5047 - val_accuracy: 0.8511\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1328 - accuracy: 0.9494 - val_loss: 0.5912 - val_accuracy: 0.8298\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0853 - accuracy: 0.9741 - val_loss: 0.7472 - val_accuracy: 0.8404\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0869 - accuracy: 0.9706 - val_loss: 0.3649 - val_accuracy: 0.8830\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 23s 109ms/step - loss: 0.0879 - accuracy: 0.9717 - val_loss: 0.3794 - val_accuracy: 0.8617\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1013 - accuracy: 0.9682 - val_loss: 0.4079 - val_accuracy: 0.8936\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0867 - accuracy: 0.9694 - val_loss: 0.4130 - val_accuracy: 0.8298\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0583 - accuracy: 0.9764 - val_loss: 0.3984 - val_accuracy: 0.8830\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0921 - accuracy: 0.9729 - val_loss: 0.4040 - val_accuracy: 0.8723\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0682 - accuracy: 0.9753 - val_loss: 0.4883 - val_accuracy: 0.8298\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0619 - accuracy: 0.9823 - val_loss: 0.5770 - val_accuracy: 0.8404\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0734 - accuracy: 0.9694 - val_loss: 0.5870 - val_accuracy: 0.8723\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0784 - accuracy: 0.9694 - val_loss: 0.4100 - val_accuracy: 0.8723\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0483 - accuracy: 0.9847 - val_loss: 0.3712 - val_accuracy: 0.9043\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0601 - accuracy: 0.9835 - val_loss: 0.5683 - val_accuracy: 0.8511\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0473 - accuracy: 0.9929 - val_loss: 0.3603 - val_accuracy: 0.8617\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0400 - accuracy: 0.9894 - val_loss: 0.4477 - val_accuracy: 0.8511\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0179 - accuracy: 0.9953 - val_loss: 0.4572 - val_accuracy: 0.9043\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0617 - accuracy: 0.9741 - val_loss: 0.4868 - val_accuracy: 0.8191\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0387 - accuracy: 0.9870 - val_loss: 0.6182 - val_accuracy: 0.8617\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0473 - accuracy: 0.9882 - val_loss: 0.3947 - val_accuracy: 0.8617\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0513 - accuracy: 0.9823 - val_loss: 0.3572 - val_accuracy: 0.8830\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0542 - accuracy: 0.9788 - val_loss: 0.3896 - val_accuracy: 0.8617\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0261 - accuracy: 0.9906 - val_loss: 0.3862 - val_accuracy: 0.8830\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0408 - accuracy: 0.9859 - val_loss: 0.4142 - val_accuracy: 0.8936\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0400 - accuracy: 0.9847 - val_loss: 0.6120 - val_accuracy: 0.8617\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0544 - accuracy: 0.9847 - val_loss: 0.2940 - val_accuracy: 0.8936\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0319 - accuracy: 0.9906 - val_loss: 0.2805 - val_accuracy: 0.9043\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0394 - accuracy: 0.9859 - val_loss: 0.2861 - val_accuracy: 0.8936\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0523 - accuracy: 0.9823 - val_loss: 0.2821 - val_accuracy: 0.8936\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0227 - accuracy: 0.9965 - val_loss: 0.3206 - val_accuracy: 0.8723\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0177 - accuracy: 0.9941 - val_loss: 0.3479 - val_accuracy: 0.8830\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0183 - accuracy: 0.9941 - val_loss: 0.5467 - val_accuracy: 0.8723\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0520 - accuracy: 0.9788 - val_loss: 0.3055 - val_accuracy: 0.8830\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0180 - accuracy: 0.9929 - val_loss: 0.3030 - val_accuracy: 0.9043\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0157 - accuracy: 0.9953 - val_loss: 0.3326 - val_accuracy: 0.8936\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0458 - accuracy: 0.9800 - val_loss: 0.3164 - val_accuracy: 0.9149\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0370 - accuracy: 0.9894 - val_loss: 0.3937 - val_accuracy: 0.8830\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0223 - accuracy: 0.9894 - val_loss: 0.3819 - val_accuracy: 0.8723\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0407 - accuracy: 0.9859 - val_loss: 0.2702 - val_accuracy: 0.9255\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0393 - accuracy: 0.9835 - val_loss: 0.3408 - val_accuracy: 0.9149\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0371 - accuracy: 0.9870 - val_loss: 0.2729 - val_accuracy: 0.8936\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0306 - accuracy: 0.9906 - val_loss: 0.3422 - val_accuracy: 0.8830\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0258 - accuracy: 0.9929 - val_loss: 0.3066 - val_accuracy: 0.8830\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0309 - accuracy: 0.9882 - val_loss: 0.3016 - val_accuracy: 0.8936\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0245 - accuracy: 0.9929 - val_loss: 0.3092 - val_accuracy: 0.8936\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0258 - accuracy: 0.9906 - val_loss: 0.3083 - val_accuracy: 0.8936\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0268 - accuracy: 0.9918 - val_loss: 0.3474 - val_accuracy: 0.9043\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0489 - accuracy: 0.9800 - val_loss: 0.5288 - val_accuracy: 0.8404\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0316 - accuracy: 0.9870 - val_loss: 0.3746 - val_accuracy: 0.8723\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0102 - accuracy: 0.9953 - val_loss: 0.4719 - val_accuracy: 0.8830\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0267 - accuracy: 0.9918 - val_loss: 0.5334 - val_accuracy: 0.8617\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0296 - accuracy: 0.9882 - val_loss: 0.3990 - val_accuracy: 0.8617\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0219 - accuracy: 0.9929 - val_loss: 0.3575 - val_accuracy: 0.8830\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0286 - accuracy: 0.9918 - val_loss: 0.2637 - val_accuracy: 0.9255\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0186 - accuracy: 0.9918 - val_loss: 0.4017 - val_accuracy: 0.8617\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0225 - accuracy: 0.9953 - val_loss: 0.2771 - val_accuracy: 0.9043\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0077 - accuracy: 0.9988 - val_loss: 0.3053 - val_accuracy: 0.8936\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0185 - accuracy: 0.9941 - val_loss: 0.2957 - val_accuracy: 0.8830\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0322 - accuracy: 0.9847 - val_loss: 0.3015 - val_accuracy: 0.8723\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0237 - accuracy: 0.9918 - val_loss: 0.2642 - val_accuracy: 0.9043\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0270 - accuracy: 0.9870 - val_loss: 0.6024 - val_accuracy: 0.8511\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0181 - accuracy: 0.9953 - val_loss: 0.2911 - val_accuracy: 0.9149\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0140 - accuracy: 0.9965 - val_loss: 0.3188 - val_accuracy: 0.8936\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0231 - accuracy: 0.9929 - val_loss: 0.3153 - val_accuracy: 0.9043\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0082 - accuracy: 0.9988 - val_loss: 0.2830 - val_accuracy: 0.8936\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0069 - accuracy: 0.9988 - val_loss: 0.2982 - val_accuracy: 0.8723\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0087 - accuracy: 0.9988 - val_loss: 0.3857 - val_accuracy: 0.8830\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0336 - accuracy: 0.9882 - val_loss: 0.3136 - val_accuracy: 0.8830\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0199 - accuracy: 0.9941 - val_loss: 0.2456 - val_accuracy: 0.9149\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0097 - accuracy: 0.9976 - val_loss: 0.3298 - val_accuracy: 0.8830\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0122 - accuracy: 0.9965 - val_loss: 0.3634 - val_accuracy: 0.8830\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0304 - accuracy: 0.9894 - val_loss: 0.3280 - val_accuracy: 0.9043\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0221 - accuracy: 0.9894 - val_loss: 0.4816 - val_accuracy: 0.8723\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0159 - accuracy: 0.9953 - val_loss: 0.2877 - val_accuracy: 0.8936\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0086 - accuracy: 0.9988 - val_loss: 0.3245 - val_accuracy: 0.8936\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0095 - accuracy: 0.9976 - val_loss: 0.3170 - val_accuracy: 0.8936\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0221 - accuracy: 0.9941 - val_loss: 0.3079 - val_accuracy: 0.9043\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0083 - accuracy: 0.9988 - val_loss: 0.2714 - val_accuracy: 0.9043\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0263 - accuracy: 0.9882 - val_loss: 0.3104 - val_accuracy: 0.9043\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0190 - accuracy: 0.9918 - val_loss: 0.2492 - val_accuracy: 0.9149\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0110 - accuracy: 0.9976 - val_loss: 0.2997 - val_accuracy: 0.9043\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0135 - accuracy: 0.9965 - val_loss: 0.2952 - val_accuracy: 0.9149\n",
      "Score for fold 4: loss of 0.295211523771286; accuracy of 91.4893627166748%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 29s 111ms/step - loss: 0.8646 - accuracy: 0.6408 - val_loss: 6.6550 - val_accuracy: 0.3191\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.6044 - accuracy: 0.7574 - val_loss: 1.4362 - val_accuracy: 0.5213\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.5198 - accuracy: 0.7998 - val_loss: 0.3330 - val_accuracy: 0.8617\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.3909 - accuracy: 0.8339 - val_loss: 0.9664 - val_accuracy: 0.7766\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.4045 - accuracy: 0.8375 - val_loss: 1.0681 - val_accuracy: 0.8191\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3471 - accuracy: 0.8681 - val_loss: 2.5475 - val_accuracy: 0.6596\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.3123 - accuracy: 0.8740 - val_loss: 0.2921 - val_accuracy: 0.8936\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3162 - accuracy: 0.8669 - val_loss: 0.2595 - val_accuracy: 0.9149\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.2980 - accuracy: 0.8787 - val_loss: 1.0003 - val_accuracy: 0.7234\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2779 - accuracy: 0.8905 - val_loss: 0.8601 - val_accuracy: 0.7340\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2470 - accuracy: 0.9093 - val_loss: 0.9146 - val_accuracy: 0.8404\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1741 - accuracy: 0.9364 - val_loss: 1.0282 - val_accuracy: 0.7234\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1909 - accuracy: 0.9329 - val_loss: 0.3863 - val_accuracy: 0.8723\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2094 - accuracy: 0.9199 - val_loss: 0.4652 - val_accuracy: 0.8404\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1636 - accuracy: 0.9305 - val_loss: 0.4152 - val_accuracy: 0.8298\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1627 - accuracy: 0.9423 - val_loss: 0.6355 - val_accuracy: 0.8298\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1558 - accuracy: 0.9505 - val_loss: 0.4429 - val_accuracy: 0.8830\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1258 - accuracy: 0.9564 - val_loss: 0.3090 - val_accuracy: 0.8723\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1170 - accuracy: 0.9623 - val_loss: 1.2833 - val_accuracy: 0.7234\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1360 - accuracy: 0.9505 - val_loss: 0.4039 - val_accuracy: 0.8830\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0682 - accuracy: 0.9800 - val_loss: 0.5422 - val_accuracy: 0.8723\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0720 - accuracy: 0.9753 - val_loss: 0.5245 - val_accuracy: 0.8723\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0711 - accuracy: 0.9776 - val_loss: 0.4781 - val_accuracy: 0.8617\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1015 - accuracy: 0.9682 - val_loss: 0.4358 - val_accuracy: 0.8617\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1174 - accuracy: 0.9635 - val_loss: 0.8098 - val_accuracy: 0.8511\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0722 - accuracy: 0.9812 - val_loss: 0.6012 - val_accuracy: 0.8723\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0999 - accuracy: 0.9647 - val_loss: 0.3801 - val_accuracy: 0.8617\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0593 - accuracy: 0.9859 - val_loss: 0.5645 - val_accuracy: 0.8404\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1109 - accuracy: 0.9611 - val_loss: 0.5177 - val_accuracy: 0.8085\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0850 - accuracy: 0.9741 - val_loss: 1.7668 - val_accuracy: 0.7447\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0670 - accuracy: 0.9776 - val_loss: 1.7672 - val_accuracy: 0.7447\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0726 - accuracy: 0.9741 - val_loss: 1.2670 - val_accuracy: 0.7872\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0528 - accuracy: 0.9823 - val_loss: 0.5826 - val_accuracy: 0.8511\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0542 - accuracy: 0.9870 - val_loss: 0.6316 - val_accuracy: 0.8404\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0883 - accuracy: 0.9717 - val_loss: 0.4024 - val_accuracy: 0.8723\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0564 - accuracy: 0.9776 - val_loss: 0.4484 - val_accuracy: 0.8936\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0484 - accuracy: 0.9847 - val_loss: 0.5616 - val_accuracy: 0.8191\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0704 - accuracy: 0.9776 - val_loss: 0.6136 - val_accuracy: 0.8191\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0790 - accuracy: 0.9717 - val_loss: 0.4304 - val_accuracy: 0.8936\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1056 - accuracy: 0.9600 - val_loss: 0.4965 - val_accuracy: 0.8404\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0538 - accuracy: 0.9812 - val_loss: 0.3924 - val_accuracy: 0.8830\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0435 - accuracy: 0.9882 - val_loss: 0.5462 - val_accuracy: 0.8936\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0555 - accuracy: 0.9835 - val_loss: 0.7532 - val_accuracy: 0.8298\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0620 - accuracy: 0.9788 - val_loss: 0.3755 - val_accuracy: 0.9043\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0500 - accuracy: 0.9847 - val_loss: 0.6076 - val_accuracy: 0.8617\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0209 - accuracy: 0.9929 - val_loss: 0.6198 - val_accuracy: 0.8617\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0404 - accuracy: 0.9894 - val_loss: 0.4877 - val_accuracy: 0.8404\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0687 - accuracy: 0.9753 - val_loss: 0.4481 - val_accuracy: 0.8511\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0260 - accuracy: 0.9906 - val_loss: 0.5287 - val_accuracy: 0.8723\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0317 - accuracy: 0.9906 - val_loss: 0.4911 - val_accuracy: 0.8830\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0505 - accuracy: 0.9800 - val_loss: 0.4732 - val_accuracy: 0.8936\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0588 - accuracy: 0.9776 - val_loss: 0.4596 - val_accuracy: 0.8617\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0447 - accuracy: 0.9847 - val_loss: 0.7449 - val_accuracy: 0.8298\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0250 - accuracy: 0.9894 - val_loss: 0.4485 - val_accuracy: 0.8936\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0417 - accuracy: 0.9835 - val_loss: 0.5514 - val_accuracy: 0.8723\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0352 - accuracy: 0.9894 - val_loss: 0.5162 - val_accuracy: 0.8830\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0229 - accuracy: 0.9918 - val_loss: 0.4624 - val_accuracy: 0.8723\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0390 - accuracy: 0.9859 - val_loss: 0.4588 - val_accuracy: 0.8830\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0236 - accuracy: 0.9906 - val_loss: 0.5779 - val_accuracy: 0.8617\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0449 - accuracy: 0.9812 - val_loss: 0.5448 - val_accuracy: 0.8511\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0168 - accuracy: 0.9965 - val_loss: 0.4788 - val_accuracy: 0.8830\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0211 - accuracy: 0.9929 - val_loss: 0.6319 - val_accuracy: 0.8404\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0179 - accuracy: 0.9918 - val_loss: 0.5974 - val_accuracy: 0.8404\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0320 - accuracy: 0.9882 - val_loss: 0.4845 - val_accuracy: 0.8404\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0369 - accuracy: 0.9918 - val_loss: 0.4356 - val_accuracy: 0.8830\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0362 - accuracy: 0.9823 - val_loss: 0.5205 - val_accuracy: 0.8617\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0358 - accuracy: 0.9870 - val_loss: 0.4431 - val_accuracy: 0.8830\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0210 - accuracy: 0.9906 - val_loss: 0.5880 - val_accuracy: 0.8511\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0259 - accuracy: 0.9918 - val_loss: 0.4582 - val_accuracy: 0.8830\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0294 - accuracy: 0.9882 - val_loss: 0.4918 - val_accuracy: 0.8617\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0124 - accuracy: 0.9965 - val_loss: 0.4307 - val_accuracy: 0.8830\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0150 - accuracy: 0.9965 - val_loss: 0.4585 - val_accuracy: 0.8723\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0456 - accuracy: 0.9847 - val_loss: 0.4488 - val_accuracy: 0.8936\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0308 - accuracy: 0.9894 - val_loss: 0.3980 - val_accuracy: 0.8936\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0104 - accuracy: 0.9976 - val_loss: 0.4183 - val_accuracy: 0.9043\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0246 - accuracy: 0.9941 - val_loss: 0.6005 - val_accuracy: 0.8830\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0294 - accuracy: 0.9906 - val_loss: 0.5183 - val_accuracy: 0.8936\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0286 - accuracy: 0.9870 - val_loss: 0.4816 - val_accuracy: 0.8830\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0267 - accuracy: 0.9906 - val_loss: 0.4677 - val_accuracy: 0.8936\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0446 - accuracy: 0.9835 - val_loss: 0.6628 - val_accuracy: 0.8511\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0200 - accuracy: 0.9953 - val_loss: 0.5088 - val_accuracy: 0.8723\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0288 - accuracy: 0.9870 - val_loss: 0.7817 - val_accuracy: 0.8617\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0312 - accuracy: 0.9882 - val_loss: 0.5145 - val_accuracy: 0.8617\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0225 - accuracy: 0.9941 - val_loss: 0.4536 - val_accuracy: 0.8830\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0246 - accuracy: 0.9918 - val_loss: 0.4981 - val_accuracy: 0.8723\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0361 - accuracy: 0.9847 - val_loss: 0.4930 - val_accuracy: 0.8723\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0402 - accuracy: 0.9859 - val_loss: 0.5305 - val_accuracy: 0.8723\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0163 - accuracy: 0.9941 - val_loss: 0.5636 - val_accuracy: 0.8723\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0136 - accuracy: 0.9965 - val_loss: 0.4602 - val_accuracy: 0.8936\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0219 - accuracy: 0.9882 - val_loss: 0.5043 - val_accuracy: 0.8723\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0138 - accuracy: 0.9976 - val_loss: 0.5306 - val_accuracy: 0.8830\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0314 - accuracy: 0.9823 - val_loss: 0.4362 - val_accuracy: 0.8936\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0149 - accuracy: 0.9953 - val_loss: 0.4936 - val_accuracy: 0.8723\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0267 - accuracy: 0.9941 - val_loss: 0.6795 - val_accuracy: 0.8617\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.5020 - val_accuracy: 0.8830\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0099 - accuracy: 0.9965 - val_loss: 0.5342 - val_accuracy: 0.8723\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 23s 109ms/step - loss: 0.0180 - accuracy: 0.9929 - val_loss: 0.4851 - val_accuracy: 0.8936\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 24s 112ms/step - loss: 0.0132 - accuracy: 0.9953 - val_loss: 0.5082 - val_accuracy: 0.8723\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 23s 110ms/step - loss: 0.0160 - accuracy: 0.9929 - val_loss: 0.5070 - val_accuracy: 0.8830\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 23s 110ms/step - loss: 0.0146 - accuracy: 0.9953 - val_loss: 0.5051 - val_accuracy: 0.8723\n",
      "Score for fold 5: loss of 0.5051325559616089; accuracy of 87.2340440750122%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 29s 111ms/step - loss: 0.9459 - accuracy: 0.6007 - val_loss: 1.9944 - val_accuracy: 0.3191\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.5611 - accuracy: 0.7750 - val_loss: 0.7876 - val_accuracy: 0.7447\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.4741 - accuracy: 0.7797 - val_loss: 1.2517 - val_accuracy: 0.7766\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.4479 - accuracy: 0.8269 - val_loss: 1.8975 - val_accuracy: 0.4255\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3752 - accuracy: 0.8528 - val_loss: 1.6257 - val_accuracy: 0.6170\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3499 - accuracy: 0.8539 - val_loss: 0.3559 - val_accuracy: 0.8617\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3037 - accuracy: 0.8799 - val_loss: 0.6180 - val_accuracy: 0.8298\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2731 - accuracy: 0.8940 - val_loss: 0.2871 - val_accuracy: 0.9043\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 24s 111ms/step - loss: 0.2527 - accuracy: 0.8987 - val_loss: 0.5913 - val_accuracy: 0.8085\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.2587 - accuracy: 0.8940 - val_loss: 0.4106 - val_accuracy: 0.8404\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.2032 - accuracy: 0.9105 - val_loss: 0.6341 - val_accuracy: 0.8298\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1950 - accuracy: 0.9282 - val_loss: 2.7218 - val_accuracy: 0.6277\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.2246 - accuracy: 0.9164 - val_loss: 0.4395 - val_accuracy: 0.8830\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.2073 - accuracy: 0.9081 - val_loss: 1.5319 - val_accuracy: 0.7021\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1193 - accuracy: 0.9541 - val_loss: 0.7378 - val_accuracy: 0.8298\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1300 - accuracy: 0.9470 - val_loss: 1.6012 - val_accuracy: 0.6489\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1170 - accuracy: 0.9541 - val_loss: 1.0703 - val_accuracy: 0.7766\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1235 - accuracy: 0.9576 - val_loss: 0.4348 - val_accuracy: 0.8936\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1327 - accuracy: 0.9576 - val_loss: 0.4479 - val_accuracy: 0.8617\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1415 - accuracy: 0.9552 - val_loss: 0.4126 - val_accuracy: 0.8936\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1259 - accuracy: 0.9541 - val_loss: 0.5466 - val_accuracy: 0.8617\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1154 - accuracy: 0.9623 - val_loss: 1.9972 - val_accuracy: 0.5638\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0972 - accuracy: 0.9706 - val_loss: 0.6967 - val_accuracy: 0.8511\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0785 - accuracy: 0.9800 - val_loss: 0.5956 - val_accuracy: 0.8298\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0847 - accuracy: 0.9694 - val_loss: 0.9769 - val_accuracy: 0.7660\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0691 - accuracy: 0.9753 - val_loss: 0.9037 - val_accuracy: 0.7660\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0609 - accuracy: 0.9800 - val_loss: 0.7012 - val_accuracy: 0.8085\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0595 - accuracy: 0.9859 - val_loss: 0.6582 - val_accuracy: 0.8085\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0631 - accuracy: 0.9835 - val_loss: 0.7183 - val_accuracy: 0.7660\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0351 - accuracy: 0.9870 - val_loss: 1.6357 - val_accuracy: 0.8191\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0682 - accuracy: 0.9812 - val_loss: 1.1788 - val_accuracy: 0.7660\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0607 - accuracy: 0.9812 - val_loss: 1.5386 - val_accuracy: 0.8085\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0686 - accuracy: 0.9776 - val_loss: 0.4716 - val_accuracy: 0.9043\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0616 - accuracy: 0.9823 - val_loss: 0.5579 - val_accuracy: 0.8617\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 24s 111ms/step - loss: 0.0522 - accuracy: 0.9812 - val_loss: 0.7821 - val_accuracy: 0.8298\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0510 - accuracy: 0.9823 - val_loss: 0.8722 - val_accuracy: 0.8191\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0624 - accuracy: 0.9753 - val_loss: 0.4740 - val_accuracy: 0.8830\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0478 - accuracy: 0.9870 - val_loss: 0.3919 - val_accuracy: 0.8936\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0587 - accuracy: 0.9741 - val_loss: 0.6478 - val_accuracy: 0.8830\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0517 - accuracy: 0.9800 - val_loss: 0.8015 - val_accuracy: 0.8298\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0542 - accuracy: 0.9823 - val_loss: 0.5166 - val_accuracy: 0.8723\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0549 - accuracy: 0.9812 - val_loss: 0.4326 - val_accuracy: 0.8723\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0480 - accuracy: 0.9823 - val_loss: 0.4775 - val_accuracy: 0.8830\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0384 - accuracy: 0.9870 - val_loss: 0.2878 - val_accuracy: 0.8936\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0360 - accuracy: 0.9870 - val_loss: 0.6251 - val_accuracy: 0.8830\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0261 - accuracy: 0.9906 - val_loss: 0.3969 - val_accuracy: 0.9043\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0730 - accuracy: 0.9717 - val_loss: 0.3592 - val_accuracy: 0.9149\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0325 - accuracy: 0.9906 - val_loss: 0.4376 - val_accuracy: 0.9043\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0388 - accuracy: 0.9882 - val_loss: 0.4975 - val_accuracy: 0.8830\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0304 - accuracy: 0.9894 - val_loss: 0.4171 - val_accuracy: 0.8936\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0293 - accuracy: 0.9882 - val_loss: 0.4809 - val_accuracy: 0.8830\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 26s 121ms/step - loss: 0.0334 - accuracy: 0.9882 - val_loss: 0.5397 - val_accuracy: 0.8404\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0370 - accuracy: 0.9847 - val_loss: 0.5702 - val_accuracy: 0.8723\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0349 - accuracy: 0.9882 - val_loss: 0.5023 - val_accuracy: 0.8936\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0310 - accuracy: 0.9906 - val_loss: 0.4590 - val_accuracy: 0.9043\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0293 - accuracy: 0.9929 - val_loss: 0.3645 - val_accuracy: 0.8936\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0139 - accuracy: 0.9941 - val_loss: 0.6146 - val_accuracy: 0.8830\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0260 - accuracy: 0.9894 - val_loss: 0.4859 - val_accuracy: 0.8617\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0244 - accuracy: 0.9906 - val_loss: 0.5467 - val_accuracy: 0.9043\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0296 - accuracy: 0.9859 - val_loss: 0.4479 - val_accuracy: 0.8936\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0310 - accuracy: 0.9882 - val_loss: 0.5226 - val_accuracy: 0.9043\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0253 - accuracy: 0.9918 - val_loss: 0.4424 - val_accuracy: 0.8936\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0321 - accuracy: 0.9882 - val_loss: 0.8161 - val_accuracy: 0.8617\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0222 - accuracy: 0.9894 - val_loss: 0.5073 - val_accuracy: 0.8617\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0183 - accuracy: 0.9929 - val_loss: 0.4661 - val_accuracy: 0.8830\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0432 - accuracy: 0.9800 - val_loss: 0.4629 - val_accuracy: 0.8723\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0116 - accuracy: 0.9976 - val_loss: 0.4893 - val_accuracy: 0.8723\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0368 - accuracy: 0.9859 - val_loss: 0.4174 - val_accuracy: 0.8830\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0155 - accuracy: 0.9941 - val_loss: 0.4816 - val_accuracy: 0.9043\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0246 - accuracy: 0.9906 - val_loss: 0.6662 - val_accuracy: 0.8723\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0336 - accuracy: 0.9882 - val_loss: 0.5628 - val_accuracy: 0.8723\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 23s 109ms/step - loss: 0.0270 - accuracy: 0.9882 - val_loss: 0.5348 - val_accuracy: 0.8830\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0261 - accuracy: 0.9906 - val_loss: 0.4966 - val_accuracy: 0.8723\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0260 - accuracy: 0.9882 - val_loss: 0.5492 - val_accuracy: 0.8830\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0270 - accuracy: 0.9882 - val_loss: 0.4977 - val_accuracy: 0.8936\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0185 - accuracy: 0.9941 - val_loss: 0.4558 - val_accuracy: 0.8936\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 25s 115ms/step - loss: 0.0234 - accuracy: 0.9906 - val_loss: 0.4615 - val_accuracy: 0.9043\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 23s 109ms/step - loss: 0.0149 - accuracy: 0.9941 - val_loss: 0.4608 - val_accuracy: 0.8830\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0177 - accuracy: 0.9918 - val_loss: 0.4686 - val_accuracy: 0.8936\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0097 - accuracy: 0.9988 - val_loss: 0.4888 - val_accuracy: 0.8936\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0073 - accuracy: 0.9976 - val_loss: 0.5557 - val_accuracy: 0.8830\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0176 - accuracy: 0.9929 - val_loss: 0.5481 - val_accuracy: 0.8936\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.4557 - val_accuracy: 0.8830\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0228 - accuracy: 0.9941 - val_loss: 0.4606 - val_accuracy: 0.8936\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0124 - accuracy: 0.9965 - val_loss: 0.6856 - val_accuracy: 0.8830\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0267 - accuracy: 0.9870 - val_loss: 0.4727 - val_accuracy: 0.8936\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0204 - accuracy: 0.9929 - val_loss: 0.4718 - val_accuracy: 0.8830\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0174 - accuracy: 0.9941 - val_loss: 0.4471 - val_accuracy: 0.8830\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0099 - accuracy: 0.9988 - val_loss: 0.5162 - val_accuracy: 0.8723\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0084 - accuracy: 0.9988 - val_loss: 0.5588 - val_accuracy: 0.8723\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0139 - accuracy: 0.9965 - val_loss: 0.5668 - val_accuracy: 0.8936\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0134 - accuracy: 0.9965 - val_loss: 0.4522 - val_accuracy: 0.8723\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0212 - accuracy: 0.9941 - val_loss: 0.5503 - val_accuracy: 0.9043\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0171 - accuracy: 0.9929 - val_loss: 0.5556 - val_accuracy: 0.9043\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0223 - accuracy: 0.9906 - val_loss: 0.4896 - val_accuracy: 0.8936\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0183 - accuracy: 0.9929 - val_loss: 0.4727 - val_accuracy: 0.9043\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0102 - accuracy: 0.9976 - val_loss: 0.6314 - val_accuracy: 0.8723\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0187 - accuracy: 0.9929 - val_loss: 0.8010 - val_accuracy: 0.8723\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0170 - accuracy: 0.9941 - val_loss: 0.5223 - val_accuracy: 0.8936\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0088 - accuracy: 0.9976 - val_loss: 0.5218 - val_accuracy: 0.9043\n",
      "Score for fold 6: loss of 0.5218180418014526; accuracy of 90.42553305625916%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 29s 111ms/step - loss: 0.8797 - accuracy: 0.6419 - val_loss: 5.2525 - val_accuracy: 0.3404\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.5130 - accuracy: 0.7715 - val_loss: 1.1990 - val_accuracy: 0.6064\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.4325 - accuracy: 0.8245 - val_loss: 1.2237 - val_accuracy: 0.6489\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.4220 - accuracy: 0.8339 - val_loss: 0.8979 - val_accuracy: 0.7872\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3631 - accuracy: 0.8645 - val_loss: 0.3008 - val_accuracy: 0.8936\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3485 - accuracy: 0.8657 - val_loss: 0.4202 - val_accuracy: 0.7766\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3765 - accuracy: 0.8457 - val_loss: 7.7488 - val_accuracy: 0.4043\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3381 - accuracy: 0.8716 - val_loss: 0.3310 - val_accuracy: 0.8723\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2690 - accuracy: 0.8975 - val_loss: 3.3700 - val_accuracy: 0.6489\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2852 - accuracy: 0.8822 - val_loss: 0.3073 - val_accuracy: 0.8617\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2440 - accuracy: 0.9140 - val_loss: 0.4243 - val_accuracy: 0.8723\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.2193 - accuracy: 0.9223 - val_loss: 0.3453 - val_accuracy: 0.8617\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.2218 - accuracy: 0.9187 - val_loss: 3.7257 - val_accuracy: 0.6596\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1754 - accuracy: 0.9258 - val_loss: 1.6538 - val_accuracy: 0.6702\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.2138 - accuracy: 0.9199 - val_loss: 0.8155 - val_accuracy: 0.7660\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1222 - accuracy: 0.9505 - val_loss: 0.2671 - val_accuracy: 0.8723\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1935 - accuracy: 0.9340 - val_loss: 0.2787 - val_accuracy: 0.8936\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1590 - accuracy: 0.9423 - val_loss: 0.3434 - val_accuracy: 0.8830\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 30s 141ms/step - loss: 0.1141 - accuracy: 0.9623 - val_loss: 0.4148 - val_accuracy: 0.8298\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1546 - accuracy: 0.9435 - val_loss: 0.5528 - val_accuracy: 0.8191\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 50s 236ms/step - loss: 0.1325 - accuracy: 0.9529 - val_loss: 2.5687 - val_accuracy: 0.5638\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0846 - accuracy: 0.9717 - val_loss: 3.1466 - val_accuracy: 0.8723\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1129 - accuracy: 0.9682 - val_loss: 1.6064 - val_accuracy: 0.7660\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0874 - accuracy: 0.9776 - val_loss: 1.8868 - val_accuracy: 0.8085\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1190 - accuracy: 0.9623 - val_loss: 0.7531 - val_accuracy: 0.8723\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0405 - accuracy: 0.9847 - val_loss: 1.0145 - val_accuracy: 0.7660\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0778 - accuracy: 0.9776 - val_loss: 0.2896 - val_accuracy: 0.9255\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0779 - accuracy: 0.9717 - val_loss: 0.9812 - val_accuracy: 0.8511\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0779 - accuracy: 0.9717 - val_loss: 0.6328 - val_accuracy: 0.8723\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0882 - accuracy: 0.9741 - val_loss: 0.8200 - val_accuracy: 0.8723\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0435 - accuracy: 0.9847 - val_loss: 0.2630 - val_accuracy: 0.9362\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0573 - accuracy: 0.9812 - val_loss: 1.1104 - val_accuracy: 0.8617\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0697 - accuracy: 0.9753 - val_loss: 0.2982 - val_accuracy: 0.9149\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0709 - accuracy: 0.9717 - val_loss: 0.3262 - val_accuracy: 0.9043\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 23s 109ms/step - loss: 0.0408 - accuracy: 0.9882 - val_loss: 0.2406 - val_accuracy: 0.9149\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0581 - accuracy: 0.9812 - val_loss: 1.3869 - val_accuracy: 0.8085\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0560 - accuracy: 0.9812 - val_loss: 0.4649 - val_accuracy: 0.9362\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0566 - accuracy: 0.9835 - val_loss: 0.2732 - val_accuracy: 0.9362\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0348 - accuracy: 0.9870 - val_loss: 0.2048 - val_accuracy: 0.9362\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0335 - accuracy: 0.9906 - val_loss: 0.5309 - val_accuracy: 0.8617\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0478 - accuracy: 0.9859 - val_loss: 0.3298 - val_accuracy: 0.9255\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0563 - accuracy: 0.9800 - val_loss: 0.2239 - val_accuracy: 0.9255\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0480 - accuracy: 0.9835 - val_loss: 0.2939 - val_accuracy: 0.9149\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0300 - accuracy: 0.9941 - val_loss: 0.2320 - val_accuracy: 0.9468\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0484 - accuracy: 0.9835 - val_loss: 0.2580 - val_accuracy: 0.9468\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0393 - accuracy: 0.9906 - val_loss: 0.3312 - val_accuracy: 0.8936\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0388 - accuracy: 0.9870 - val_loss: 0.2359 - val_accuracy: 0.9362\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0313 - accuracy: 0.9835 - val_loss: 0.2212 - val_accuracy: 0.9362\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0271 - accuracy: 0.9894 - val_loss: 0.2133 - val_accuracy: 0.9574\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0314 - accuracy: 0.9918 - val_loss: 0.2893 - val_accuracy: 0.9362\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0151 - accuracy: 0.9953 - val_loss: 0.1847 - val_accuracy: 0.9362\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0643 - accuracy: 0.9753 - val_loss: 0.2254 - val_accuracy: 0.9468\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0442 - accuracy: 0.9823 - val_loss: 0.2141 - val_accuracy: 0.9362\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0438 - accuracy: 0.9859 - val_loss: 0.2603 - val_accuracy: 0.9255\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0259 - accuracy: 0.9929 - val_loss: 0.3064 - val_accuracy: 0.8936\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0131 - accuracy: 0.9965 - val_loss: 0.3694 - val_accuracy: 0.9362\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0284 - accuracy: 0.9882 - val_loss: 0.4767 - val_accuracy: 0.8404\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0300 - accuracy: 0.9847 - val_loss: 0.7010 - val_accuracy: 0.8830\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0430 - accuracy: 0.9812 - val_loss: 0.3742 - val_accuracy: 0.8511\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0278 - accuracy: 0.9929 - val_loss: 0.2406 - val_accuracy: 0.9149\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0235 - accuracy: 0.9918 - val_loss: 0.2047 - val_accuracy: 0.9574\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0198 - accuracy: 0.9918 - val_loss: 0.2085 - val_accuracy: 0.9468\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0176 - accuracy: 0.9953 - val_loss: 0.2349 - val_accuracy: 0.9149\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0257 - accuracy: 0.9894 - val_loss: 0.2312 - val_accuracy: 0.9043\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0319 - accuracy: 0.9859 - val_loss: 0.4602 - val_accuracy: 0.9149\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0192 - accuracy: 0.9918 - val_loss: 0.3862 - val_accuracy: 0.8298\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0284 - accuracy: 0.9906 - val_loss: 0.6067 - val_accuracy: 0.9043\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0282 - accuracy: 0.9882 - val_loss: 0.2849 - val_accuracy: 0.9362\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0263 - accuracy: 0.9918 - val_loss: 0.2220 - val_accuracy: 0.9468\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0297 - accuracy: 0.9906 - val_loss: 0.2595 - val_accuracy: 0.9362\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0270 - accuracy: 0.9918 - val_loss: 0.2106 - val_accuracy: 0.9468\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0203 - accuracy: 0.9941 - val_loss: 0.2148 - val_accuracy: 0.9468\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0179 - accuracy: 0.9918 - val_loss: 0.2519 - val_accuracy: 0.9149\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0260 - accuracy: 0.9929 - val_loss: 0.2336 - val_accuracy: 0.9468\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0317 - accuracy: 0.9847 - val_loss: 0.2480 - val_accuracy: 0.9255\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0114 - accuracy: 0.9953 - val_loss: 0.4215 - val_accuracy: 0.8723\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0169 - accuracy: 0.9941 - val_loss: 0.2599 - val_accuracy: 0.9362\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0094 - accuracy: 0.9988 - val_loss: 0.2937 - val_accuracy: 0.9149\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0175 - accuracy: 0.9906 - val_loss: 0.2812 - val_accuracy: 0.9255\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0145 - accuracy: 0.9953 - val_loss: 0.2928 - val_accuracy: 0.9362\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0211 - accuracy: 0.9929 - val_loss: 0.2922 - val_accuracy: 0.8936\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0154 - accuracy: 0.9941 - val_loss: 0.2489 - val_accuracy: 0.9149\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0195 - accuracy: 0.9929 - val_loss: 0.2293 - val_accuracy: 0.9362\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0209 - accuracy: 0.9918 - val_loss: 0.2286 - val_accuracy: 0.8936\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0174 - accuracy: 0.9941 - val_loss: 0.2563 - val_accuracy: 0.9468\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0096 - accuracy: 0.9965 - val_loss: 0.2289 - val_accuracy: 0.9255\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0127 - accuracy: 0.9941 - val_loss: 0.2086 - val_accuracy: 0.9149\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0182 - accuracy: 0.9965 - val_loss: 0.5010 - val_accuracy: 0.8298\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0164 - accuracy: 0.9953 - val_loss: 0.2163 - val_accuracy: 0.9468\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0496 - accuracy: 0.9823 - val_loss: 0.2159 - val_accuracy: 0.9468\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0388 - accuracy: 0.9882 - val_loss: 0.2932 - val_accuracy: 0.8617\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0218 - accuracy: 0.9953 - val_loss: 0.2433 - val_accuracy: 0.9574\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0222 - accuracy: 0.9906 - val_loss: 0.2029 - val_accuracy: 0.9362\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0159 - accuracy: 0.9976 - val_loss: 0.2081 - val_accuracy: 0.9362\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0409 - accuracy: 0.9835 - val_loss: 0.7198 - val_accuracy: 0.8085\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0195 - accuracy: 0.9941 - val_loss: 0.2109 - val_accuracy: 0.9255\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0128 - accuracy: 0.9976 - val_loss: 0.2328 - val_accuracy: 0.9362\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.2183 - val_accuracy: 0.9255\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0160 - accuracy: 0.9953 - val_loss: 0.2695 - val_accuracy: 0.9255\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0150 - accuracy: 0.9976 - val_loss: 0.2294 - val_accuracy: 0.9468\n",
      "Score for fold 7: loss of 0.22938407957553864; accuracy of 94.68085169792175%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 29s 110ms/step - loss: 0.8397 - accuracy: 0.6396 - val_loss: 2.8415 - val_accuracy: 0.3298\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.5636 - accuracy: 0.7691 - val_loss: 1.9915 - val_accuracy: 0.3723\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.4824 - accuracy: 0.8080 - val_loss: 7.0995 - val_accuracy: 0.5957\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.4273 - accuracy: 0.8151 - val_loss: 0.6513 - val_accuracy: 0.6915\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3574 - accuracy: 0.8551 - val_loss: 0.4388 - val_accuracy: 0.8191\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3588 - accuracy: 0.8645 - val_loss: 0.3623 - val_accuracy: 0.8723\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3292 - accuracy: 0.8810 - val_loss: 0.6121 - val_accuracy: 0.8298\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2402 - accuracy: 0.9128 - val_loss: 0.7010 - val_accuracy: 0.7872\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2829 - accuracy: 0.8846 - val_loss: 1.3525 - val_accuracy: 0.7021\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2714 - accuracy: 0.9022 - val_loss: 1.0523 - val_accuracy: 0.7660\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2299 - accuracy: 0.9081 - val_loss: 0.4035 - val_accuracy: 0.7872\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.2084 - accuracy: 0.9293 - val_loss: 0.5132 - val_accuracy: 0.7872\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1926 - accuracy: 0.9258 - val_loss: 0.4896 - val_accuracy: 0.7872\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.1372 - accuracy: 0.9517 - val_loss: 1.1513 - val_accuracy: 0.7766\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1718 - accuracy: 0.9423 - val_loss: 0.7741 - val_accuracy: 0.8085\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1796 - accuracy: 0.9293 - val_loss: 0.5283 - val_accuracy: 0.8511\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1584 - accuracy: 0.9470 - val_loss: 0.4333 - val_accuracy: 0.8511\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.1529 - accuracy: 0.9352 - val_loss: 0.3019 - val_accuracy: 0.8723\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.1146 - accuracy: 0.9576 - val_loss: 0.4160 - val_accuracy: 0.8511\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0917 - accuracy: 0.9717 - val_loss: 0.5510 - val_accuracy: 0.8617\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.1120 - accuracy: 0.9600 - val_loss: 0.7452 - val_accuracy: 0.7979\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0873 - accuracy: 0.9741 - val_loss: 0.5954 - val_accuracy: 0.8404\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0955 - accuracy: 0.9635 - val_loss: 0.6505 - val_accuracy: 0.8085\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0910 - accuracy: 0.9670 - val_loss: 0.4932 - val_accuracy: 0.8511\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.1036 - accuracy: 0.9623 - val_loss: 0.4190 - val_accuracy: 0.8617\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0843 - accuracy: 0.9741 - val_loss: 0.9179 - val_accuracy: 0.8298\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1153 - accuracy: 0.9600 - val_loss: 1.0955 - val_accuracy: 0.5638\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0558 - accuracy: 0.9823 - val_loss: 0.6651 - val_accuracy: 0.7872\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0623 - accuracy: 0.9812 - val_loss: 1.5283 - val_accuracy: 0.8298\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0671 - accuracy: 0.9823 - val_loss: 0.5037 - val_accuracy: 0.8298\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0626 - accuracy: 0.9800 - val_loss: 0.4785 - val_accuracy: 0.8617\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0703 - accuracy: 0.9800 - val_loss: 0.6680 - val_accuracy: 0.8191\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0798 - accuracy: 0.9706 - val_loss: 0.5881 - val_accuracy: 0.8404\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0689 - accuracy: 0.9788 - val_loss: 0.7128 - val_accuracy: 0.7979\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0609 - accuracy: 0.9776 - val_loss: 1.5126 - val_accuracy: 0.7766\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0374 - accuracy: 0.9894 - val_loss: 0.5724 - val_accuracy: 0.8298\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0371 - accuracy: 0.9882 - val_loss: 0.5013 - val_accuracy: 0.8723\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0596 - accuracy: 0.9788 - val_loss: 0.5311 - val_accuracy: 0.7979\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0353 - accuracy: 0.9918 - val_loss: 0.6099 - val_accuracy: 0.8617\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0618 - accuracy: 0.9776 - val_loss: 0.8748 - val_accuracy: 0.8404\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0502 - accuracy: 0.9847 - val_loss: 0.5549 - val_accuracy: 0.8298\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0516 - accuracy: 0.9812 - val_loss: 0.3787 - val_accuracy: 0.8617\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0454 - accuracy: 0.9847 - val_loss: 1.0332 - val_accuracy: 0.8511\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0328 - accuracy: 0.9870 - val_loss: 0.4654 - val_accuracy: 0.8404\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0325 - accuracy: 0.9882 - val_loss: 0.7619 - val_accuracy: 0.8511\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0346 - accuracy: 0.9847 - val_loss: 0.4214 - val_accuracy: 0.8617\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0265 - accuracy: 0.9906 - val_loss: 0.4979 - val_accuracy: 0.8511\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0388 - accuracy: 0.9882 - val_loss: 0.4279 - val_accuracy: 0.8298\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0337 - accuracy: 0.9906 - val_loss: 0.6539 - val_accuracy: 0.8617\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0282 - accuracy: 0.9906 - val_loss: 0.6228 - val_accuracy: 0.8511\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0211 - accuracy: 0.9929 - val_loss: 0.4824 - val_accuracy: 0.8511\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0401 - accuracy: 0.9812 - val_loss: 1.2068 - val_accuracy: 0.7979\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0195 - accuracy: 0.9953 - val_loss: 0.5986 - val_accuracy: 0.8511\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0353 - accuracy: 0.9882 - val_loss: 0.8817 - val_accuracy: 0.8723\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0550 - accuracy: 0.9776 - val_loss: 0.7466 - val_accuracy: 0.8617\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0315 - accuracy: 0.9870 - val_loss: 0.4348 - val_accuracy: 0.8617\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0463 - accuracy: 0.9859 - val_loss: 0.4808 - val_accuracy: 0.8617\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0172 - accuracy: 0.9953 - val_loss: 0.6875 - val_accuracy: 0.8085\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0254 - accuracy: 0.9941 - val_loss: 0.5978 - val_accuracy: 0.8511\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0144 - accuracy: 0.9965 - val_loss: 0.5988 - val_accuracy: 0.8511\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0361 - accuracy: 0.9835 - val_loss: 0.7172 - val_accuracy: 0.8617\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0381 - accuracy: 0.9870 - val_loss: 0.6550 - val_accuracy: 0.8085\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0333 - accuracy: 0.9847 - val_loss: 0.7937 - val_accuracy: 0.8511\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0125 - accuracy: 0.9988 - val_loss: 0.5970 - val_accuracy: 0.8404\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0266 - accuracy: 0.9918 - val_loss: 0.7021 - val_accuracy: 0.8511\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0200 - accuracy: 0.9929 - val_loss: 0.6776 - val_accuracy: 0.8511\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0286 - accuracy: 0.9894 - val_loss: 0.5079 - val_accuracy: 0.8191\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0193 - accuracy: 0.9929 - val_loss: 0.4402 - val_accuracy: 0.8404\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0304 - accuracy: 0.9870 - val_loss: 0.5257 - val_accuracy: 0.8404\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0230 - accuracy: 0.9929 - val_loss: 0.5706 - val_accuracy: 0.8404\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0426 - accuracy: 0.9859 - val_loss: 0.4836 - val_accuracy: 0.8617\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0277 - accuracy: 0.9918 - val_loss: 0.5381 - val_accuracy: 0.8404\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0190 - accuracy: 0.9941 - val_loss: 0.5400 - val_accuracy: 0.8404\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0295 - accuracy: 0.9882 - val_loss: 0.5089 - val_accuracy: 0.8511\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0124 - accuracy: 0.9976 - val_loss: 0.6407 - val_accuracy: 0.8511\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0336 - accuracy: 0.9859 - val_loss: 0.5134 - val_accuracy: 0.8404\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0314 - accuracy: 0.9906 - val_loss: 0.5095 - val_accuracy: 0.8723\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0401 - accuracy: 0.9906 - val_loss: 0.5238 - val_accuracy: 0.8404\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0098 - accuracy: 0.9976 - val_loss: 0.5523 - val_accuracy: 0.8723\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0134 - accuracy: 0.9941 - val_loss: 0.5256 - val_accuracy: 0.8404\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0309 - accuracy: 0.9894 - val_loss: 0.5831 - val_accuracy: 0.8404\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0309 - accuracy: 0.9859 - val_loss: 0.6636 - val_accuracy: 0.8298\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0362 - accuracy: 0.9870 - val_loss: 0.5033 - val_accuracy: 0.8617\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0327 - accuracy: 0.9882 - val_loss: 0.5505 - val_accuracy: 0.8404\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0248 - accuracy: 0.9929 - val_loss: 0.8732 - val_accuracy: 0.7660\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0178 - accuracy: 0.9941 - val_loss: 0.5218 - val_accuracy: 0.8617\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0106 - accuracy: 0.9965 - val_loss: 0.5169 - val_accuracy: 0.8617\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0278 - accuracy: 0.9906 - val_loss: 0.4693 - val_accuracy: 0.8830\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0262 - accuracy: 0.9906 - val_loss: 0.6050 - val_accuracy: 0.8511\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.5253 - val_accuracy: 0.8511\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0135 - accuracy: 0.9965 - val_loss: 0.5418 - val_accuracy: 0.8617\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0157 - accuracy: 0.9965 - val_loss: 0.5147 - val_accuracy: 0.8511\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0272 - accuracy: 0.9894 - val_loss: 0.4651 - val_accuracy: 0.8617\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0249 - accuracy: 0.9894 - val_loss: 0.5494 - val_accuracy: 0.8511\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0217 - accuracy: 0.9941 - val_loss: 0.5315 - val_accuracy: 0.8511\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 23s 105ms/step - loss: 0.0171 - accuracy: 0.9941 - val_loss: 0.5402 - val_accuracy: 0.8511\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0241 - accuracy: 0.9906 - val_loss: 0.7653 - val_accuracy: 0.8085\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0142 - accuracy: 0.9988 - val_loss: 0.5344 - val_accuracy: 0.8404\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0241 - accuracy: 0.9929 - val_loss: 0.4640 - val_accuracy: 0.8617\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.0182 - accuracy: 0.9976 - val_loss: 0.5303 - val_accuracy: 0.8723\n",
      "Score for fold 8: loss of 0.5303049087524414; accuracy of 87.2340440750122%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 29s 110ms/step - loss: 0.8385 - accuracy: 0.6572 - val_loss: 4.2455 - val_accuracy: 0.3191\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.6296 - accuracy: 0.7479 - val_loss: 0.6858 - val_accuracy: 0.7128\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.4701 - accuracy: 0.7986 - val_loss: 34.5425 - val_accuracy: 0.3723\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.4402 - accuracy: 0.8115 - val_loss: 0.6059 - val_accuracy: 0.8511\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.4231 - accuracy: 0.8221 - val_loss: 0.5280 - val_accuracy: 0.8511\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3373 - accuracy: 0.8657 - val_loss: 0.7358 - val_accuracy: 0.7553\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3221 - accuracy: 0.8704 - val_loss: 0.7844 - val_accuracy: 0.7447\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3160 - accuracy: 0.8728 - val_loss: 2.2572 - val_accuracy: 0.5745\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3113 - accuracy: 0.8940 - val_loss: 0.8396 - val_accuracy: 0.7447\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2750 - accuracy: 0.9011 - val_loss: 0.3667 - val_accuracy: 0.8830\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2110 - accuracy: 0.9211 - val_loss: 0.6044 - val_accuracy: 0.8617\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2109 - accuracy: 0.9140 - val_loss: 2.7156 - val_accuracy: 0.5851\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2041 - accuracy: 0.9258 - val_loss: 1.1966 - val_accuracy: 0.7128\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1655 - accuracy: 0.9388 - val_loss: 0.7649 - val_accuracy: 0.7872\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1942 - accuracy: 0.9329 - val_loss: 1.0946 - val_accuracy: 0.7553\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1846 - accuracy: 0.9435 - val_loss: 0.8148 - val_accuracy: 0.6915\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1070 - accuracy: 0.9670 - val_loss: 0.5989 - val_accuracy: 0.7979\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2016 - accuracy: 0.9317 - val_loss: 0.6155 - val_accuracy: 0.8298\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1394 - accuracy: 0.9552 - val_loss: 0.4222 - val_accuracy: 0.8298\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1369 - accuracy: 0.9529 - val_loss: 0.4458 - val_accuracy: 0.8191\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1250 - accuracy: 0.9541 - val_loss: 0.6170 - val_accuracy: 0.8830\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1225 - accuracy: 0.9552 - val_loss: 0.7401 - val_accuracy: 0.8511\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1001 - accuracy: 0.9682 - val_loss: 0.5481 - val_accuracy: 0.8511\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0780 - accuracy: 0.9729 - val_loss: 0.5540 - val_accuracy: 0.8936\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1060 - accuracy: 0.9658 - val_loss: 0.6717 - val_accuracy: 0.8511\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1020 - accuracy: 0.9611 - val_loss: 0.5249 - val_accuracy: 0.8404\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0925 - accuracy: 0.9729 - val_loss: 0.6381 - val_accuracy: 0.8617\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0573 - accuracy: 0.9823 - val_loss: 0.4631 - val_accuracy: 0.8830\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0721 - accuracy: 0.9800 - val_loss: 0.5038 - val_accuracy: 0.8191\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0622 - accuracy: 0.9859 - val_loss: 0.4837 - val_accuracy: 0.8617\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0776 - accuracy: 0.9741 - val_loss: 0.7137 - val_accuracy: 0.8617\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0758 - accuracy: 0.9694 - val_loss: 0.7328 - val_accuracy: 0.8723\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0920 - accuracy: 0.9635 - val_loss: 0.6461 - val_accuracy: 0.8511\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0524 - accuracy: 0.9788 - val_loss: 0.7107 - val_accuracy: 0.7979\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0447 - accuracy: 0.9870 - val_loss: 1.0732 - val_accuracy: 0.7447\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0569 - accuracy: 0.9812 - val_loss: 0.4958 - val_accuracy: 0.8617\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0450 - accuracy: 0.9870 - val_loss: 0.6845 - val_accuracy: 0.8511\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0329 - accuracy: 0.9906 - val_loss: 0.7701 - val_accuracy: 0.8723\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0845 - accuracy: 0.9682 - val_loss: 0.8149 - val_accuracy: 0.8617\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0421 - accuracy: 0.9835 - val_loss: 1.0337 - val_accuracy: 0.7872\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0434 - accuracy: 0.9835 - val_loss: 0.5540 - val_accuracy: 0.8723\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0450 - accuracy: 0.9812 - val_loss: 0.7306 - val_accuracy: 0.8511\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0459 - accuracy: 0.9870 - val_loss: 0.6017 - val_accuracy: 0.9043\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0519 - accuracy: 0.9800 - val_loss: 0.7676 - val_accuracy: 0.8511\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0563 - accuracy: 0.9847 - val_loss: 0.7383 - val_accuracy: 0.7872\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0387 - accuracy: 0.9906 - val_loss: 0.5727 - val_accuracy: 0.8830\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0450 - accuracy: 0.9812 - val_loss: 0.8424 - val_accuracy: 0.8404\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0304 - accuracy: 0.9941 - val_loss: 0.6704 - val_accuracy: 0.8617\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0222 - accuracy: 0.9953 - val_loss: 0.5128 - val_accuracy: 0.8830\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0540 - accuracy: 0.9812 - val_loss: 0.5482 - val_accuracy: 0.8830\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0159 - accuracy: 0.9976 - val_loss: 0.6367 - val_accuracy: 0.8723\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0463 - accuracy: 0.9823 - val_loss: 0.5607 - val_accuracy: 0.8936\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0314 - accuracy: 0.9894 - val_loss: 0.7775 - val_accuracy: 0.8617\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0571 - accuracy: 0.9776 - val_loss: 0.4864 - val_accuracy: 0.8511\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0419 - accuracy: 0.9835 - val_loss: 0.7638 - val_accuracy: 0.8511\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0204 - accuracy: 0.9965 - val_loss: 0.6610 - val_accuracy: 0.8830\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0329 - accuracy: 0.9906 - val_loss: 0.6024 - val_accuracy: 0.8617\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0367 - accuracy: 0.9870 - val_loss: 0.6612 - val_accuracy: 0.9043\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0219 - accuracy: 0.9953 - val_loss: 0.5751 - val_accuracy: 0.8830\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0302 - accuracy: 0.9894 - val_loss: 0.5401 - val_accuracy: 0.8936\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.0172 - accuracy: 0.9976 - val_loss: 0.4817 - val_accuracy: 0.9043\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0203 - accuracy: 0.9929 - val_loss: 0.5459 - val_accuracy: 0.9043\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0351 - accuracy: 0.9870 - val_loss: 0.7855 - val_accuracy: 0.8723\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0182 - accuracy: 0.9953 - val_loss: 0.6745 - val_accuracy: 0.8723\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0290 - accuracy: 0.9882 - val_loss: 1.0094 - val_accuracy: 0.8511\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0363 - accuracy: 0.9847 - val_loss: 0.6385 - val_accuracy: 0.8936\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0262 - accuracy: 0.9918 - val_loss: 0.6021 - val_accuracy: 0.8404\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0344 - accuracy: 0.9859 - val_loss: 0.5331 - val_accuracy: 0.8936\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0177 - accuracy: 0.9953 - val_loss: 0.5989 - val_accuracy: 0.8830\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0322 - accuracy: 0.9835 - val_loss: 0.8132 - val_accuracy: 0.8723\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0270 - accuracy: 0.9906 - val_loss: 0.6208 - val_accuracy: 0.8936\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0090 - accuracy: 0.9976 - val_loss: 0.5710 - val_accuracy: 0.8936\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0393 - accuracy: 0.9823 - val_loss: 0.6536 - val_accuracy: 0.8936\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0462 - accuracy: 0.9823 - val_loss: 0.4345 - val_accuracy: 0.8830\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0150 - accuracy: 0.9953 - val_loss: 0.6160 - val_accuracy: 0.8830\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0337 - accuracy: 0.9870 - val_loss: 0.6970 - val_accuracy: 0.8511\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0196 - accuracy: 0.9918 - val_loss: 0.5624 - val_accuracy: 0.8830\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0284 - accuracy: 0.9882 - val_loss: 0.6818 - val_accuracy: 0.8723\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0309 - accuracy: 0.9847 - val_loss: 0.5449 - val_accuracy: 0.8830\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0169 - accuracy: 0.9976 - val_loss: 0.5742 - val_accuracy: 0.8830\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0223 - accuracy: 0.9918 - val_loss: 0.5421 - val_accuracy: 0.8830\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0332 - accuracy: 0.9870 - val_loss: 0.8685 - val_accuracy: 0.8404\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0204 - accuracy: 0.9953 - val_loss: 0.6524 - val_accuracy: 0.8830\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0099 - accuracy: 0.9976 - val_loss: 0.7557 - val_accuracy: 0.8830\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0126 - accuracy: 0.9965 - val_loss: 0.7566 - val_accuracy: 0.8617\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0296 - accuracy: 0.9870 - val_loss: 0.6827 - val_accuracy: 0.8617\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0289 - accuracy: 0.9882 - val_loss: 0.7002 - val_accuracy: 0.8830\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0148 - accuracy: 0.9941 - val_loss: 0.7051 - val_accuracy: 0.8830\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0243 - accuracy: 0.9918 - val_loss: 0.7093 - val_accuracy: 0.8936\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0299 - accuracy: 0.9918 - val_loss: 0.6925 - val_accuracy: 0.8936\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0182 - accuracy: 0.9976 - val_loss: 0.6202 - val_accuracy: 0.8936\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0248 - accuracy: 0.9918 - val_loss: 0.6627 - val_accuracy: 0.8830\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0177 - accuracy: 0.9941 - val_loss: 0.6514 - val_accuracy: 0.8830\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0127 - accuracy: 0.9965 - val_loss: 0.5912 - val_accuracy: 0.8830\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0191 - accuracy: 0.9953 - val_loss: 0.7306 - val_accuracy: 0.8298\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0187 - accuracy: 0.9941 - val_loss: 0.7439 - val_accuracy: 0.8723\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0144 - accuracy: 0.9965 - val_loss: 0.6577 - val_accuracy: 0.8830\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0081 - accuracy: 0.9988 - val_loss: 0.6626 - val_accuracy: 0.8830\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0209 - accuracy: 0.9941 - val_loss: 0.6113 - val_accuracy: 0.8723\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0082 - accuracy: 0.9976 - val_loss: 0.6948 - val_accuracy: 0.8511\n",
      "Score for fold 9: loss of 0.694849967956543; accuracy of 85.10638475418091%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 29s 110ms/step - loss: 0.9094 - accuracy: 0.6502 - val_loss: 1.5627 - val_accuracy: 0.2553\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.5582 - accuracy: 0.7715 - val_loss: 0.5450 - val_accuracy: 0.7872\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.4579 - accuracy: 0.8045 - val_loss: 1.0170 - val_accuracy: 0.7660\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.4325 - accuracy: 0.8092 - val_loss: 1.1142 - val_accuracy: 0.6915\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3116 - accuracy: 0.8704 - val_loss: 0.9157 - val_accuracy: 0.7660\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3605 - accuracy: 0.8610 - val_loss: 2.1725 - val_accuracy: 0.7447\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3621 - accuracy: 0.8492 - val_loss: 0.3347 - val_accuracy: 0.9043\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3344 - accuracy: 0.8834 - val_loss: 0.3755 - val_accuracy: 0.9043\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2437 - accuracy: 0.9069 - val_loss: 0.9165 - val_accuracy: 0.7128\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1956 - accuracy: 0.9246 - val_loss: 0.7454 - val_accuracy: 0.8617\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.2295 - accuracy: 0.9117 - val_loss: 0.8629 - val_accuracy: 0.7553\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1890 - accuracy: 0.9270 - val_loss: 1.4834 - val_accuracy: 0.8723\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1974 - accuracy: 0.9293 - val_loss: 0.4795 - val_accuracy: 0.8723\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1705 - accuracy: 0.9505 - val_loss: 0.3466 - val_accuracy: 0.9043\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1798 - accuracy: 0.9376 - val_loss: 0.3809 - val_accuracy: 0.8511\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1928 - accuracy: 0.9270 - val_loss: 0.9330 - val_accuracy: 0.7021\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1763 - accuracy: 0.9293 - val_loss: 0.5501 - val_accuracy: 0.8511\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1084 - accuracy: 0.9635 - val_loss: 0.3312 - val_accuracy: 0.8936\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1338 - accuracy: 0.9600 - val_loss: 0.7428 - val_accuracy: 0.8085\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1420 - accuracy: 0.9494 - val_loss: 0.4235 - val_accuracy: 0.9043\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1341 - accuracy: 0.9494 - val_loss: 0.4406 - val_accuracy: 0.8723\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0825 - accuracy: 0.9706 - val_loss: 1.0528 - val_accuracy: 0.7766\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0721 - accuracy: 0.9788 - val_loss: 0.3944 - val_accuracy: 0.9149\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1395 - accuracy: 0.9470 - val_loss: 0.6980 - val_accuracy: 0.8830\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.1202 - accuracy: 0.9623 - val_loss: 0.5666 - val_accuracy: 0.8404\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0725 - accuracy: 0.9800 - val_loss: 0.9848 - val_accuracy: 0.8936\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0509 - accuracy: 0.9882 - val_loss: 0.9424 - val_accuracy: 0.8511\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0821 - accuracy: 0.9788 - val_loss: 0.7065 - val_accuracy: 0.8404\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0794 - accuracy: 0.9788 - val_loss: 0.4495 - val_accuracy: 0.8936\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0713 - accuracy: 0.9694 - val_loss: 0.4892 - val_accuracy: 0.8936\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0596 - accuracy: 0.9812 - val_loss: 1.7225 - val_accuracy: 0.7553\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0918 - accuracy: 0.9706 - val_loss: 1.7603 - val_accuracy: 0.7553\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0619 - accuracy: 0.9800 - val_loss: 0.7509 - val_accuracy: 0.8511\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0496 - accuracy: 0.9847 - val_loss: 0.6535 - val_accuracy: 0.7979\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0478 - accuracy: 0.9870 - val_loss: 0.6025 - val_accuracy: 0.8830\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0523 - accuracy: 0.9823 - val_loss: 1.0818 - val_accuracy: 0.8404\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0677 - accuracy: 0.9800 - val_loss: 0.6238 - val_accuracy: 0.8085\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0362 - accuracy: 0.9918 - val_loss: 0.4850 - val_accuracy: 0.8936\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0316 - accuracy: 0.9918 - val_loss: 0.8074 - val_accuracy: 0.8723\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0361 - accuracy: 0.9906 - val_loss: 0.4743 - val_accuracy: 0.8830\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0660 - accuracy: 0.9741 - val_loss: 1.6816 - val_accuracy: 0.8191\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0389 - accuracy: 0.9894 - val_loss: 0.7665 - val_accuracy: 0.8511\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0372 - accuracy: 0.9870 - val_loss: 0.7405 - val_accuracy: 0.8723\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0469 - accuracy: 0.9847 - val_loss: 0.6070 - val_accuracy: 0.8723\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0421 - accuracy: 0.9859 - val_loss: 0.6979 - val_accuracy: 0.8511\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0266 - accuracy: 0.9906 - val_loss: 0.8313 - val_accuracy: 0.8617\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0416 - accuracy: 0.9859 - val_loss: 0.6778 - val_accuracy: 0.8830\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0582 - accuracy: 0.9788 - val_loss: 0.4315 - val_accuracy: 0.9043\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0134 - accuracy: 0.9965 - val_loss: 0.4291 - val_accuracy: 0.8830\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0325 - accuracy: 0.9882 - val_loss: 0.5192 - val_accuracy: 0.9255\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0343 - accuracy: 0.9870 - val_loss: 0.4182 - val_accuracy: 0.8936\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0593 - accuracy: 0.9753 - val_loss: 0.3993 - val_accuracy: 0.8830\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0404 - accuracy: 0.9847 - val_loss: 0.3746 - val_accuracy: 0.9149\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0256 - accuracy: 0.9882 - val_loss: 0.4771 - val_accuracy: 0.9149\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0511 - accuracy: 0.9800 - val_loss: 0.4847 - val_accuracy: 0.8511\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0450 - accuracy: 0.9859 - val_loss: 0.4260 - val_accuracy: 0.9043\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0162 - accuracy: 0.9965 - val_loss: 0.3655 - val_accuracy: 0.9043\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0302 - accuracy: 0.9882 - val_loss: 0.5005 - val_accuracy: 0.9043\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0157 - accuracy: 0.9953 - val_loss: 0.4754 - val_accuracy: 0.8936\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0364 - accuracy: 0.9847 - val_loss: 1.2216 - val_accuracy: 0.8085\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0384 - accuracy: 0.9859 - val_loss: 0.4425 - val_accuracy: 0.8830\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0332 - accuracy: 0.9953 - val_loss: 0.4316 - val_accuracy: 0.9043\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0334 - accuracy: 0.9918 - val_loss: 0.8032 - val_accuracy: 0.8511\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0206 - accuracy: 0.9941 - val_loss: 0.5862 - val_accuracy: 0.8617\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0288 - accuracy: 0.9894 - val_loss: 0.4549 - val_accuracy: 0.8830\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0353 - accuracy: 0.9870 - val_loss: 0.5653 - val_accuracy: 0.8404\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0405 - accuracy: 0.9870 - val_loss: 0.5487 - val_accuracy: 0.8936\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0354 - accuracy: 0.9882 - val_loss: 0.5817 - val_accuracy: 0.8511\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0401 - accuracy: 0.9847 - val_loss: 0.4902 - val_accuracy: 0.8617\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0153 - accuracy: 0.9941 - val_loss: 0.4988 - val_accuracy: 0.9043\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0374 - accuracy: 0.9835 - val_loss: 0.4610 - val_accuracy: 0.8830\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0147 - accuracy: 0.9965 - val_loss: 0.7794 - val_accuracy: 0.8511\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0176 - accuracy: 0.9918 - val_loss: 0.6455 - val_accuracy: 0.8936\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0282 - accuracy: 0.9882 - val_loss: 0.5382 - val_accuracy: 0.8936\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0221 - accuracy: 0.9941 - val_loss: 0.7805 - val_accuracy: 0.8723\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0252 - accuracy: 0.9906 - val_loss: 0.5125 - val_accuracy: 0.9149\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0238 - accuracy: 0.9918 - val_loss: 0.6075 - val_accuracy: 0.8936\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0180 - accuracy: 0.9929 - val_loss: 0.6326 - val_accuracy: 0.9149\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0061 - accuracy: 0.9988 - val_loss: 0.5710 - val_accuracy: 0.9255\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0221 - accuracy: 0.9906 - val_loss: 0.7426 - val_accuracy: 0.8723\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0173 - accuracy: 0.9941 - val_loss: 0.4798 - val_accuracy: 0.9149\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0224 - accuracy: 0.9918 - val_loss: 0.5762 - val_accuracy: 0.8936\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.5808 - val_accuracy: 0.9043\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0233 - accuracy: 0.9894 - val_loss: 0.6753 - val_accuracy: 0.8723\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0054 - accuracy: 0.9976 - val_loss: 0.6086 - val_accuracy: 0.8936\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0143 - accuracy: 0.9941 - val_loss: 0.4498 - val_accuracy: 0.9043\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0172 - accuracy: 0.9953 - val_loss: 0.5223 - val_accuracy: 0.8723\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0195 - accuracy: 0.9929 - val_loss: 0.4705 - val_accuracy: 0.9043\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.0138 - accuracy: 0.9976 - val_loss: 0.5204 - val_accuracy: 0.9149\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0327 - accuracy: 0.9882 - val_loss: 0.5787 - val_accuracy: 0.8936\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0168 - accuracy: 0.9929 - val_loss: 0.4141 - val_accuracy: 0.9149\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0233 - accuracy: 0.9918 - val_loss: 0.5740 - val_accuracy: 0.8936\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0340 - accuracy: 0.9859 - val_loss: 0.5030 - val_accuracy: 0.9255\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0225 - accuracy: 0.9929 - val_loss: 0.5976 - val_accuracy: 0.8404\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0203 - accuracy: 0.9953 - val_loss: 0.5892 - val_accuracy: 0.9149\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0163 - accuracy: 0.9941 - val_loss: 0.6372 - val_accuracy: 0.9149\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0222 - accuracy: 0.9882 - val_loss: 0.7456 - val_accuracy: 0.8830\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0249 - accuracy: 0.9870 - val_loss: 0.5208 - val_accuracy: 0.9043\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0171 - accuracy: 0.9953 - val_loss: 0.4315 - val_accuracy: 0.8936\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.0188 - accuracy: 0.9929 - val_loss: 0.4786 - val_accuracy: 0.8936\n",
      "Score for fold 10: loss of 0.47860458493232727; accuracy of 89.3617033958435%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.16335202753543854 - Accuracy: 94.73684430122375%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.40351244807243347 - Accuracy: 87.36842274665833%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.3259102702140808 - Accuracy: 91.57894849777222%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.295211523771286 - Accuracy: 91.4893627166748%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5051325559616089 - Accuracy: 87.2340440750122%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.5218180418014526 - Accuracy: 90.42553305625916%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.22938407957553864 - Accuracy: 94.68085169792175%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.5303049087524414 - Accuracy: 87.2340440750122%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.694849967956543 - Accuracy: 85.10638475418091%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.47860458493232727 - Accuracy: 89.3617033958435%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 89.92161393165588 (+- 3.094512833460576)\n",
      "> Loss: 0.41480804085731504\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    for train, test in kfold.split(X_train, y_train):\n",
    "        print(train.shape, test.shape)\n",
    "        \n",
    "        np.savetxt('E:/Result/ver.3.28/MTF/train_/' + f'train_{fold_no}.csv', train, delimiter=\",\")\n",
    "        np.savetxt('E:/Result/ver.3.28/MTF/test_/' + f'test_{fold_no}.csv', test, delimiter=\",\")\n",
    "\n",
    "        input = Input(shape=(300, 300, 2))\n",
    "        model = DenseNet121(input_tensor=input, include_top=False, weights=None, pooling='avg')\n",
    "        \n",
    "        x = model.output\n",
    "\n",
    "        x = Dense(3, activation='softmax', name='softmax', kernel_initializer='he_normal')(x)\n",
    "        model = Model(model.input, x)\n",
    "\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.SGD(lr=0.01, decay=1e-3, momentum=0.9, nesterov=True)\n",
    "        #optimizer = optimizers.Adam(lr=0.001)\n",
    "        \n",
    "        callbacks_list = [LearningRateSchedule([20,40])]\n",
    "        \n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        \n",
    "        history = model.fit(inputs[train], targets[train], \n",
    "                            batch_size=4, \n",
    "                            epochs=100, \n",
    "                            verbose=1,\n",
    "                            validation_data=(inputs[test], targets[test]),\n",
    "                            callbacks = tensorboard_callback) #  Validation set  ?\n",
    "        \n",
    "        scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "        print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "        acc_per_fold.append(scores[1] * 100)\n",
    "        loss_per_fold.append(scores[0])\n",
    "        \n",
    "        model.save('E:/Result/ver.3.28/MTF/weight_/' + f'MTF_{fold_no}.h5',fold_no)\n",
    "        \n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training','validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training','validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c7ec7915fb6663623914b2919607c696d31d8503b403c6d54bcb30c0bb224a7"
  },
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
