{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "WARNING:tensorflow:From C:\\Users\\JWH\\AppData\\Local\\Temp/ipykernel_2660/3517717524.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "print(tf.__version__) \n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8048545769459327681\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6300696576\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1031789658732914003\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from math import ceil\n",
    "from numba import njit, prange\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array\n",
    "from pyts.preprocessing import MinMaxScaler\n",
    "from pyts.approximation import PiecewiseAggregateApproximation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import np_utils\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Welding_data = np.load('E:/3.17/MTF/MTF.npz')\n",
    "\n",
    "X_data = Welding_data['X_data']\n",
    "y_data = Welding_data['y_data']\n",
    "i_data = Welding_data['i_data']\n",
    "\n",
    "Welding_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 ... 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, i_train, i_test = train_test_split(X_data,y_data,i_data, test_size = 0.2, shuffle = True, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 300, 300, 2)\n",
      "(2400,)\n",
      "(600, 300, 300, 2)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate((X_train,X_test))\n",
    "targets = np.concatenate((y_train,y_test))\n",
    "index = np.concatenate((i_train,i_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "targets = np_utils.to_categorical(targets)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import Dense, Input, Activation, Flatten, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization.batch_normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 300, 300, 2  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " zero_padding2d (ZeroPadding2D)  (None, 306, 306, 2)  0          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1/conv (Conv2D)            (None, 150, 150, 64  6272        ['zero_padding2d[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1/bn (BatchNormalization)  (None, 150, 150, 64  256         ['conv1/conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1/relu (Activation)        (None, 150, 150, 64  0           ['conv1/bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " zero_padding2d_1 (ZeroPadding2  (None, 152, 152, 64  0          ['conv1/relu[0][0]']             \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " pool1 (MaxPooling2D)           (None, 75, 75, 64)   0           ['zero_padding2d_1[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 75, 75, 64)  256         ['pool1[0][0]']                  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_0_relu (Activatio  (None, 75, 75, 64)  0           ['conv2_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 75, 75, 128)  8192        ['conv2_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_concat (Concatena  (None, 75, 75, 96)  0           ['pool1[0][0]',                  \n",
      " te)                                                              'conv2_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_0_bn (BatchNormal  (None, 75, 75, 96)  384         ['conv2_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_0_relu (Activatio  (None, 75, 75, 96)  0           ['conv2_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 75, 75, 128)  12288       ['conv2_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_concat (Concatena  (None, 75, 75, 128)  0          ['conv2_block1_concat[0][0]',    \n",
      " te)                                                              'conv2_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_0_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_0_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 75, 75, 128)  16384       ['conv2_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_concat (Concatena  (None, 75, 75, 160)  0          ['conv2_block2_concat[0][0]',    \n",
      " te)                                                              'conv2_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_0_bn (BatchNormal  (None, 75, 75, 160)  640        ['conv2_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_0_relu (Activatio  (None, 75, 75, 160)  0          ['conv2_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_1_conv (Conv2D)   (None, 75, 75, 128)  20480       ['conv2_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_concat (Concatena  (None, 75, 75, 192)  0          ['conv2_block3_concat[0][0]',    \n",
      " te)                                                              'conv2_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_0_bn (BatchNormal  (None, 75, 75, 192)  768        ['conv2_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_0_relu (Activatio  (None, 75, 75, 192)  0          ['conv2_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_1_conv (Conv2D)   (None, 75, 75, 128)  24576       ['conv2_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_concat (Concatena  (None, 75, 75, 224)  0          ['conv2_block4_concat[0][0]',    \n",
      " te)                                                              'conv2_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_0_bn (BatchNormal  (None, 75, 75, 224)  896        ['conv2_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_0_relu (Activatio  (None, 75, 75, 224)  0          ['conv2_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_1_conv (Conv2D)   (None, 75, 75, 128)  28672       ['conv2_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_concat (Concatena  (None, 75, 75, 256)  0          ['conv2_block5_concat[0][0]',    \n",
      " te)                                                              'conv2_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_bn (BatchNormalization)  (None, 75, 75, 256)  1024        ['conv2_block6_concat[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_relu (Activation)        (None, 75, 75, 256)  0           ['pool2_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool2_conv (Conv2D)            (None, 75, 75, 128)  32768       ['pool2_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool2_pool (AveragePooling2D)  (None, 37, 37, 128)  0           ['pool2_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 37, 37, 128)  512        ['pool2_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_0_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 37, 37, 128)  16384       ['conv3_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_concat (Concatena  (None, 37, 37, 160)  0          ['pool2_pool[0][0]',             \n",
      " te)                                                              'conv3_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_0_bn (BatchNormal  (None, 37, 37, 160)  640        ['conv3_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_0_relu (Activatio  (None, 37, 37, 160)  0          ['conv3_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 37, 37, 128)  20480       ['conv3_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_concat (Concatena  (None, 37, 37, 192)  0          ['conv3_block1_concat[0][0]',    \n",
      " te)                                                              'conv3_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_0_bn (BatchNormal  (None, 37, 37, 192)  768        ['conv3_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_0_relu (Activatio  (None, 37, 37, 192)  0          ['conv3_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 37, 37, 128)  24576       ['conv3_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_concat (Concatena  (None, 37, 37, 224)  0          ['conv3_block2_concat[0][0]',    \n",
      " te)                                                              'conv3_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_0_bn (BatchNormal  (None, 37, 37, 224)  896        ['conv3_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_0_relu (Activatio  (None, 37, 37, 224)  0          ['conv3_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 37, 37, 128)  28672       ['conv3_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_concat (Concatena  (None, 37, 37, 256)  0          ['conv3_block3_concat[0][0]',    \n",
      " te)                                                              'conv3_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_0_bn (BatchNormal  (None, 37, 37, 256)  1024       ['conv3_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_0_relu (Activatio  (None, 37, 37, 256)  0          ['conv3_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_1_conv (Conv2D)   (None, 37, 37, 128)  32768       ['conv3_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_concat (Concatena  (None, 37, 37, 288)  0          ['conv3_block4_concat[0][0]',    \n",
      " te)                                                              'conv3_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_0_bn (BatchNormal  (None, 37, 37, 288)  1152       ['conv3_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_0_relu (Activatio  (None, 37, 37, 288)  0          ['conv3_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_1_conv (Conv2D)   (None, 37, 37, 128)  36864       ['conv3_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_concat (Concatena  (None, 37, 37, 320)  0          ['conv3_block5_concat[0][0]',    \n",
      " te)                                                              'conv3_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_0_bn (BatchNormal  (None, 37, 37, 320)  1280       ['conv3_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_0_relu (Activatio  (None, 37, 37, 320)  0          ['conv3_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_1_conv (Conv2D)   (None, 37, 37, 128)  40960       ['conv3_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_concat (Concatena  (None, 37, 37, 352)  0          ['conv3_block6_concat[0][0]',    \n",
      " te)                                                              'conv3_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_0_bn (BatchNormal  (None, 37, 37, 352)  1408       ['conv3_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_0_relu (Activatio  (None, 37, 37, 352)  0          ['conv3_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_1_conv (Conv2D)   (None, 37, 37, 128)  45056       ['conv3_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_concat (Concatena  (None, 37, 37, 384)  0          ['conv3_block7_concat[0][0]',    \n",
      " te)                                                              'conv3_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_0_bn (BatchNormal  (None, 37, 37, 384)  1536       ['conv3_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_0_relu (Activatio  (None, 37, 37, 384)  0          ['conv3_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_1_conv (Conv2D)   (None, 37, 37, 128)  49152       ['conv3_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_concat (Concatena  (None, 37, 37, 416)  0          ['conv3_block8_concat[0][0]',    \n",
      " te)                                                              'conv3_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block10_0_bn (BatchNorma  (None, 37, 37, 416)  1664       ['conv3_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_0_relu (Activati  (None, 37, 37, 416)  0          ['conv3_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block10_1_conv (Conv2D)  (None, 37, 37, 128)  53248       ['conv3_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_1_bn (BatchNorma  (None, 37, 37, 128)  512        ['conv3_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_1_relu (Activati  (None, 37, 37, 128)  0          ['conv3_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block10_2_conv (Conv2D)  (None, 37, 37, 32)   36864       ['conv3_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_concat (Concaten  (None, 37, 37, 448)  0          ['conv3_block9_concat[0][0]',    \n",
      " ate)                                                             'conv3_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_0_bn (BatchNorma  (None, 37, 37, 448)  1792       ['conv3_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_0_relu (Activati  (None, 37, 37, 448)  0          ['conv3_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_1_conv (Conv2D)  (None, 37, 37, 128)  57344       ['conv3_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_1_bn (BatchNorma  (None, 37, 37, 128)  512        ['conv3_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_1_relu (Activati  (None, 37, 37, 128)  0          ['conv3_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_2_conv (Conv2D)  (None, 37, 37, 32)   36864       ['conv3_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_concat (Concaten  (None, 37, 37, 480)  0          ['conv3_block10_concat[0][0]',   \n",
      " ate)                                                             'conv3_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_0_bn (BatchNorma  (None, 37, 37, 480)  1920       ['conv3_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_0_relu (Activati  (None, 37, 37, 480)  0          ['conv3_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_1_conv (Conv2D)  (None, 37, 37, 128)  61440       ['conv3_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_1_bn (BatchNorma  (None, 37, 37, 128)  512        ['conv3_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_1_relu (Activati  (None, 37, 37, 128)  0          ['conv3_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_2_conv (Conv2D)  (None, 37, 37, 32)   36864       ['conv3_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_concat (Concaten  (None, 37, 37, 512)  0          ['conv3_block11_concat[0][0]',   \n",
      " ate)                                                             'conv3_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_bn (BatchNormalization)  (None, 37, 37, 512)  2048        ['conv3_block12_concat[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_relu (Activation)        (None, 37, 37, 512)  0           ['pool3_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool3_conv (Conv2D)            (None, 37, 37, 256)  131072      ['pool3_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool3_pool (AveragePooling2D)  (None, 18, 18, 256)  0           ['pool3_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 18, 18, 256)  1024       ['pool3_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_0_relu (Activatio  (None, 18, 18, 256)  0          ['conv4_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 18, 18, 128)  32768       ['conv4_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_concat (Concatena  (None, 18, 18, 288)  0          ['pool3_pool[0][0]',             \n",
      " te)                                                              'conv4_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_0_bn (BatchNormal  (None, 18, 18, 288)  1152       ['conv4_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_0_relu (Activatio  (None, 18, 18, 288)  0          ['conv4_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 18, 18, 128)  36864       ['conv4_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_concat (Concatena  (None, 18, 18, 320)  0          ['conv4_block1_concat[0][0]',    \n",
      " te)                                                              'conv4_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_0_bn (BatchNormal  (None, 18, 18, 320)  1280       ['conv4_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_0_relu (Activatio  (None, 18, 18, 320)  0          ['conv4_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 18, 18, 128)  40960       ['conv4_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_concat (Concatena  (None, 18, 18, 352)  0          ['conv4_block2_concat[0][0]',    \n",
      " te)                                                              'conv4_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_0_bn (BatchNormal  (None, 18, 18, 352)  1408       ['conv4_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_0_relu (Activatio  (None, 18, 18, 352)  0          ['conv4_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 18, 18, 128)  45056       ['conv4_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_concat (Concatena  (None, 18, 18, 384)  0          ['conv4_block3_concat[0][0]',    \n",
      " te)                                                              'conv4_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_0_bn (BatchNormal  (None, 18, 18, 384)  1536       ['conv4_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_0_relu (Activatio  (None, 18, 18, 384)  0          ['conv4_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 18, 18, 128)  49152       ['conv4_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_concat (Concatena  (None, 18, 18, 416)  0          ['conv4_block4_concat[0][0]',    \n",
      " te)                                                              'conv4_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_0_bn (BatchNormal  (None, 18, 18, 416)  1664       ['conv4_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_0_relu (Activatio  (None, 18, 18, 416)  0          ['conv4_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 18, 18, 128)  53248       ['conv4_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_concat (Concatena  (None, 18, 18, 448)  0          ['conv4_block5_concat[0][0]',    \n",
      " te)                                                              'conv4_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_0_bn (BatchNormal  (None, 18, 18, 448)  1792       ['conv4_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_0_relu (Activatio  (None, 18, 18, 448)  0          ['conv4_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_1_conv (Conv2D)   (None, 18, 18, 128)  57344       ['conv4_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_concat (Concatena  (None, 18, 18, 480)  0          ['conv4_block6_concat[0][0]',    \n",
      " te)                                                              'conv4_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_0_bn (BatchNormal  (None, 18, 18, 480)  1920       ['conv4_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_0_relu (Activatio  (None, 18, 18, 480)  0          ['conv4_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_1_conv (Conv2D)   (None, 18, 18, 128)  61440       ['conv4_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_concat (Concatena  (None, 18, 18, 512)  0          ['conv4_block7_concat[0][0]',    \n",
      " te)                                                              'conv4_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_0_bn (BatchNormal  (None, 18, 18, 512)  2048       ['conv4_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_0_relu (Activatio  (None, 18, 18, 512)  0          ['conv4_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_1_conv (Conv2D)   (None, 18, 18, 128)  65536       ['conv4_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_concat (Concatena  (None, 18, 18, 544)  0          ['conv4_block8_concat[0][0]',    \n",
      " te)                                                              'conv4_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block10_0_bn (BatchNorma  (None, 18, 18, 544)  2176       ['conv4_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_0_relu (Activati  (None, 18, 18, 544)  0          ['conv4_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_1_conv (Conv2D)  (None, 18, 18, 128)  69632       ['conv4_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_concat (Concaten  (None, 18, 18, 576)  0          ['conv4_block9_concat[0][0]',    \n",
      " ate)                                                             'conv4_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_0_bn (BatchNorma  (None, 18, 18, 576)  2304       ['conv4_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_0_relu (Activati  (None, 18, 18, 576)  0          ['conv4_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_1_conv (Conv2D)  (None, 18, 18, 128)  73728       ['conv4_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_concat (Concaten  (None, 18, 18, 608)  0          ['conv4_block10_concat[0][0]',   \n",
      " ate)                                                             'conv4_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_0_bn (BatchNorma  (None, 18, 18, 608)  2432       ['conv4_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_0_relu (Activati  (None, 18, 18, 608)  0          ['conv4_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_1_conv (Conv2D)  (None, 18, 18, 128)  77824       ['conv4_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_concat (Concaten  (None, 18, 18, 640)  0          ['conv4_block11_concat[0][0]',   \n",
      " ate)                                                             'conv4_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_0_bn (BatchNorma  (None, 18, 18, 640)  2560       ['conv4_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_0_relu (Activati  (None, 18, 18, 640)  0          ['conv4_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_1_conv (Conv2D)  (None, 18, 18, 128)  81920       ['conv4_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_concat (Concaten  (None, 18, 18, 672)  0          ['conv4_block12_concat[0][0]',   \n",
      " ate)                                                             'conv4_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_0_bn (BatchNorma  (None, 18, 18, 672)  2688       ['conv4_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_0_relu (Activati  (None, 18, 18, 672)  0          ['conv4_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_1_conv (Conv2D)  (None, 18, 18, 128)  86016       ['conv4_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_concat (Concaten  (None, 18, 18, 704)  0          ['conv4_block13_concat[0][0]',   \n",
      " ate)                                                             'conv4_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_0_bn (BatchNorma  (None, 18, 18, 704)  2816       ['conv4_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_0_relu (Activati  (None, 18, 18, 704)  0          ['conv4_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_1_conv (Conv2D)  (None, 18, 18, 128)  90112       ['conv4_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block15_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_concat (Concaten  (None, 18, 18, 736)  0          ['conv4_block14_concat[0][0]',   \n",
      " ate)                                                             'conv4_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_0_bn (BatchNorma  (None, 18, 18, 736)  2944       ['conv4_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_0_relu (Activati  (None, 18, 18, 736)  0          ['conv4_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_1_conv (Conv2D)  (None, 18, 18, 128)  94208       ['conv4_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_concat (Concaten  (None, 18, 18, 768)  0          ['conv4_block15_concat[0][0]',   \n",
      " ate)                                                             'conv4_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_0_bn (BatchNorma  (None, 18, 18, 768)  3072       ['conv4_block16_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_0_relu (Activati  (None, 18, 18, 768)  0          ['conv4_block17_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_1_conv (Conv2D)  (None, 18, 18, 128)  98304       ['conv4_block17_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block17_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block17_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block17_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_concat (Concaten  (None, 18, 18, 800)  0          ['conv4_block16_concat[0][0]',   \n",
      " ate)                                                             'conv4_block17_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_0_bn (BatchNorma  (None, 18, 18, 800)  3200       ['conv4_block17_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_0_relu (Activati  (None, 18, 18, 800)  0          ['conv4_block18_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_1_conv (Conv2D)  (None, 18, 18, 128)  102400      ['conv4_block18_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block18_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block18_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block18_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_concat (Concaten  (None, 18, 18, 832)  0          ['conv4_block17_concat[0][0]',   \n",
      " ate)                                                             'conv4_block18_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_0_bn (BatchNorma  (None, 18, 18, 832)  3328       ['conv4_block18_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_0_relu (Activati  (None, 18, 18, 832)  0          ['conv4_block19_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_1_conv (Conv2D)  (None, 18, 18, 128)  106496      ['conv4_block19_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block19_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block19_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block19_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_concat (Concaten  (None, 18, 18, 864)  0          ['conv4_block18_concat[0][0]',   \n",
      " ate)                                                             'conv4_block19_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_0_bn (BatchNorma  (None, 18, 18, 864)  3456       ['conv4_block19_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block20_0_relu (Activati  (None, 18, 18, 864)  0          ['conv4_block20_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_1_conv (Conv2D)  (None, 18, 18, 128)  110592      ['conv4_block20_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block20_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block20_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block20_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block20_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_concat (Concaten  (None, 18, 18, 896)  0          ['conv4_block19_concat[0][0]',   \n",
      " ate)                                                             'conv4_block20_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_0_bn (BatchNorma  (None, 18, 18, 896)  3584       ['conv4_block20_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_0_relu (Activati  (None, 18, 18, 896)  0          ['conv4_block21_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_1_conv (Conv2D)  (None, 18, 18, 128)  114688      ['conv4_block21_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block21_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block21_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block21_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_concat (Concaten  (None, 18, 18, 928)  0          ['conv4_block20_concat[0][0]',   \n",
      " ate)                                                             'conv4_block21_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_0_bn (BatchNorma  (None, 18, 18, 928)  3712       ['conv4_block21_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_0_relu (Activati  (None, 18, 18, 928)  0          ['conv4_block22_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_1_conv (Conv2D)  (None, 18, 18, 128)  118784      ['conv4_block22_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block22_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block22_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block22_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_concat (Concaten  (None, 18, 18, 960)  0          ['conv4_block21_concat[0][0]',   \n",
      " ate)                                                             'conv4_block22_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_0_bn (BatchNorma  (None, 18, 18, 960)  3840       ['conv4_block22_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_0_relu (Activati  (None, 18, 18, 960)  0          ['conv4_block23_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_1_conv (Conv2D)  (None, 18, 18, 128)  122880      ['conv4_block23_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block23_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block23_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block23_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_concat (Concaten  (None, 18, 18, 992)  0          ['conv4_block22_concat[0][0]',   \n",
      " ate)                                                             'conv4_block23_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_0_bn (BatchNorma  (None, 18, 18, 992)  3968       ['conv4_block23_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block24_0_relu (Activati  (None, 18, 18, 992)  0          ['conv4_block24_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_1_conv (Conv2D)  (None, 18, 18, 128)  126976      ['conv4_block24_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block24_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block24_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block24_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block24_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_concat (Concaten  (None, 18, 18, 1024  0          ['conv4_block23_concat[0][0]',   \n",
      " ate)                           )                                 'conv4_block24_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool4_bn (BatchNormalization)  (None, 18, 18, 1024  4096        ['conv4_block24_concat[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool4_relu (Activation)        (None, 18, 18, 1024  0           ['pool4_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool4_conv (Conv2D)            (None, 18, 18, 512)  524288      ['pool4_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool4_pool (AveragePooling2D)  (None, 9, 9, 512)    0           ['pool4_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 9, 9, 512)   2048        ['pool4_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_0_relu (Activatio  (None, 9, 9, 512)   0           ['conv5_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 9, 9, 128)    65536       ['conv5_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_concat (Concatena  (None, 9, 9, 544)   0           ['pool4_pool[0][0]',             \n",
      " te)                                                              'conv5_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_0_bn (BatchNormal  (None, 9, 9, 544)   2176        ['conv5_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_0_relu (Activatio  (None, 9, 9, 544)   0           ['conv5_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 9, 9, 128)    69632       ['conv5_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_concat (Concatena  (None, 9, 9, 576)   0           ['conv5_block1_concat[0][0]',    \n",
      " te)                                                              'conv5_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_0_bn (BatchNormal  (None, 9, 9, 576)   2304        ['conv5_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_0_relu (Activatio  (None, 9, 9, 576)   0           ['conv5_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 9, 9, 128)    73728       ['conv5_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_concat (Concatena  (None, 9, 9, 608)   0           ['conv5_block2_concat[0][0]',    \n",
      " te)                                                              'conv5_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_0_bn (BatchNormal  (None, 9, 9, 608)   2432        ['conv5_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_0_relu (Activatio  (None, 9, 9, 608)   0           ['conv5_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_1_conv (Conv2D)   (None, 9, 9, 128)    77824       ['conv5_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_concat (Concatena  (None, 9, 9, 640)   0           ['conv5_block3_concat[0][0]',    \n",
      " te)                                                              'conv5_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_0_bn (BatchNormal  (None, 9, 9, 640)   2560        ['conv5_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_0_relu (Activatio  (None, 9, 9, 640)   0           ['conv5_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_1_conv (Conv2D)   (None, 9, 9, 128)    81920       ['conv5_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_concat (Concatena  (None, 9, 9, 672)   0           ['conv5_block4_concat[0][0]',    \n",
      " te)                                                              'conv5_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_0_bn (BatchNormal  (None, 9, 9, 672)   2688        ['conv5_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_0_relu (Activatio  (None, 9, 9, 672)   0           ['conv5_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_1_conv (Conv2D)   (None, 9, 9, 128)    86016       ['conv5_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_concat (Concatena  (None, 9, 9, 704)   0           ['conv5_block5_concat[0][0]',    \n",
      " te)                                                              'conv5_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_0_bn (BatchNormal  (None, 9, 9, 704)   2816        ['conv5_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_0_relu (Activatio  (None, 9, 9, 704)   0           ['conv5_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_1_conv (Conv2D)   (None, 9, 9, 128)    90112       ['conv5_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_concat (Concatena  (None, 9, 9, 736)   0           ['conv5_block6_concat[0][0]',    \n",
      " te)                                                              'conv5_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_0_bn (BatchNormal  (None, 9, 9, 736)   2944        ['conv5_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_0_relu (Activatio  (None, 9, 9, 736)   0           ['conv5_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block8_1_conv (Conv2D)   (None, 9, 9, 128)    94208       ['conv5_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block8_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_concat (Concatena  (None, 9, 9, 768)   0           ['conv5_block7_concat[0][0]',    \n",
      " te)                                                              'conv5_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_0_bn (BatchNormal  (None, 9, 9, 768)   3072        ['conv5_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_0_relu (Activatio  (None, 9, 9, 768)   0           ['conv5_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_1_conv (Conv2D)   (None, 9, 9, 128)    98304       ['conv5_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_concat (Concatena  (None, 9, 9, 800)   0           ['conv5_block8_concat[0][0]',    \n",
      " te)                                                              'conv5_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block10_0_bn (BatchNorma  (None, 9, 9, 800)   3200        ['conv5_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_0_relu (Activati  (None, 9, 9, 800)   0           ['conv5_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_1_conv (Conv2D)  (None, 9, 9, 128)    102400      ['conv5_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_concat (Concaten  (None, 9, 9, 832)   0           ['conv5_block9_concat[0][0]',    \n",
      " ate)                                                             'conv5_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_0_bn (BatchNorma  (None, 9, 9, 832)   3328        ['conv5_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_0_relu (Activati  (None, 9, 9, 832)   0           ['conv5_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_1_conv (Conv2D)  (None, 9, 9, 128)    106496      ['conv5_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_concat (Concaten  (None, 9, 9, 864)   0           ['conv5_block10_concat[0][0]',   \n",
      " ate)                                                             'conv5_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_0_bn (BatchNorma  (None, 9, 9, 864)   3456        ['conv5_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_0_relu (Activati  (None, 9, 9, 864)   0           ['conv5_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_1_conv (Conv2D)  (None, 9, 9, 128)    110592      ['conv5_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_concat (Concaten  (None, 9, 9, 896)   0           ['conv5_block11_concat[0][0]',   \n",
      " ate)                                                             'conv5_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_0_bn (BatchNorma  (None, 9, 9, 896)   3584        ['conv5_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block13_0_relu (Activati  (None, 9, 9, 896)   0           ['conv5_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_1_conv (Conv2D)  (None, 9, 9, 128)    114688      ['conv5_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block13_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_concat (Concaten  (None, 9, 9, 928)   0           ['conv5_block12_concat[0][0]',   \n",
      " ate)                                                             'conv5_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_0_bn (BatchNorma  (None, 9, 9, 928)   3712        ['conv5_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_0_relu (Activati  (None, 9, 9, 928)   0           ['conv5_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_1_conv (Conv2D)  (None, 9, 9, 128)    118784      ['conv5_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_concat (Concaten  (None, 9, 9, 960)   0           ['conv5_block13_concat[0][0]',   \n",
      " ate)                                                             'conv5_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_0_bn (BatchNorma  (None, 9, 9, 960)   3840        ['conv5_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_0_relu (Activati  (None, 9, 9, 960)   0           ['conv5_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_1_conv (Conv2D)  (None, 9, 9, 128)    122880      ['conv5_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block15_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_concat (Concaten  (None, 9, 9, 992)   0           ['conv5_block14_concat[0][0]',   \n",
      " ate)                                                             'conv5_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_0_bn (BatchNorma  (None, 9, 9, 992)   3968        ['conv5_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_0_relu (Activati  (None, 9, 9, 992)   0           ['conv5_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_1_conv (Conv2D)  (None, 9, 9, 128)    126976      ['conv5_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_concat (Concaten  (None, 9, 9, 1024)  0           ['conv5_block15_concat[0][0]',   \n",
      " ate)                                                             'conv5_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " bn (BatchNormalization)        (None, 9, 9, 1024)   4096        ['conv5_block16_concat[0][0]']   \n",
      "                                                                                                  \n",
      " relu (Activation)              (None, 9, 9, 1024)   0           ['bn[0][0]']                     \n",
      "                                                                                                  \n",
      " avg_pool (GlobalAveragePooling  (None, 1024)        0           ['relu[0][0]']                   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['avg_pool[0][0]']               \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 3)            3075        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,037,443\n",
      "Trainable params: 6,953,795\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(300, 300, 2))\n",
    "model = DenseNet121(input_tensor=input, include_top=False, weights=None, pooling='avg')\n",
    " \n",
    "x = model.output\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(3, activation='softmax', name='softmax')(x)\n",
    "\n",
    "model = Model(model.input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class LearningRateSchedule(Callback):\n",
    "    def __init__(self, selected_epochs=[]):\n",
    "        self.selected_epochs = selected_epochs\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch+1) in self.selected_epochs:\n",
    "            lr = K.get_value(self.model.optimizer.lr)\n",
    "            K.set_value(self.model.optimizer.lr, lr*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "2700\n",
      "300\n",
      "2700\n",
      "300\n",
      "2700\n",
      "300\n",
      "2700\n",
      "300\n",
      "2700\n",
      "300\n",
      "2700\n",
      "300\n",
      "2700\n",
      "300\n",
      "2700\n",
      "300\n",
      "2700\n",
      "300\n",
      "2700\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "test = []\n",
    "train= []\n",
    "test_ = []\n",
    "train_ = []\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    print(len(test))\n",
    "    print(len(train))\n",
    "    for i in zip(test):\n",
    "        test_.append(i)\n",
    "    for i in zip(train):\n",
    "        train_.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_[0:2700]\n",
    "train = np.reshape(train, 2700)\n",
    "test = test_[0:300]\n",
    "test = np.reshape(test, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   34   36   70   99  118  121  125  142  151  172  180  186  190\n",
      "  203  206  224  259  278  280  296  314  318  325  340  356  361  372\n",
      "  375  377  386  412  414  415  419  420  423  426  429  430  433  440\n",
      "  454  456  468  474  514  526  532  536  539  561  562  568  582  584\n",
      "  587  592  631  637  643  658  672  674  684  699  707  718  720  727\n",
      "  729  732  734  735  737  750  752  770  773  785  802  803  804  807\n",
      "  819  832  852  855  858  868  871  879  883  890  891  928  933  964\n",
      "  974  979  983  992 1038 1051 1054 1057 1073 1077 1098 1102 1104 1109\n",
      " 1111 1123 1140 1141 1159 1162 1163 1164 1166 1181 1205 1210 1224 1229\n",
      " 1235 1242 1271 1306 1327 1351 1363 1377 1381 1389 1391 1396 1426 1435\n",
      " 1443 1448 1459 1463 1476 1483 1495 1513 1528 1530 1541 1543 1547 1564\n",
      " 1598 1600 1609 1611 1622 1634 1647 1667 1670 1698 1705 1707 1732 1734\n",
      " 1740 1742 1745 1759 1784 1806 1813 1845 1853 1877 1897 1898 1910 1963\n",
      " 1966 1968 1989 1992 2004 2037 2043 2051 2052 2054 2077 2087 2090 2101\n",
      " 2106 2108 2112 2129 2134 2136 2144 2149 2153 2171 2179 2186 2187 2205\n",
      " 2214 2216 2228 2233 2235 2259 2264 2268 2272 2282 2294 2307 2312 2314\n",
      " 2317 2336 2353 2358 2378 2385 2392 2409 2440 2449 2452 2453 2460 2467\n",
      " 2478 2479 2498 2502 2532 2533 2544 2551 2562 2565 2584 2585 2591 2592\n",
      " 2610 2614 2620 2649 2651 2674 2693 2715 2726 2730 2743 2747 2748 2767\n",
      " 2769 2771 2772 2773 2774 2783 2785 2799 2803 2805 2812 2815 2820 2823\n",
      " 2834 2837 2850 2862 2865 2866 2892 2902 2912 2930 2933 2946 2953 2957\n",
      " 2961 2962 2974 2976 2985 2995]\n",
      "[   0    1    3 ... 2997 2998 2999]\n",
      "['D1_244' 'D2_836' 'N_273' 'D2_42' 'N_748' 'N_348' 'D2_674' 'D2_666'\n",
      " 'D2_630' 'N_236' 'N_401' 'D1_538' 'D2_877' 'D2_517' 'D2_272' 'D2_185'\n",
      " 'N_515' 'N_205' 'N_123' 'D1_314' 'D2_996' 'N_805' 'D1_656' 'N_652'\n",
      " 'D1_239' 'N_580' 'D2_124' 'N_447' 'D1_640' 'N_229' 'D1_304' 'D2_709'\n",
      " 'D2_791' 'D2_276' 'D1_824' 'N_259' 'D1_175' 'D2_540' 'D1_245' 'D2_280'\n",
      " 'N_835' 'N_73' 'N_684' 'N_871' 'N_730' 'D1_167' 'N_721' 'D2_15' 'D2_930'\n",
      " 'D1_919' 'N_414' 'D1_355' 'D2_305' 'D2_475' 'N_257' 'D1_411' 'D1_626'\n",
      " 'D1_826' 'D1_288' 'D1_3' 'D2_190' 'D2_194' 'D2_278' 'D2_961' 'N_623'\n",
      " 'D1_456' 'D2_288' 'D1_204' 'D1_287' 'N_226' 'D1_662' 'D2_979' 'D2_667'\n",
      " 'D1_727' 'D2_0' 'D1_147' 'N_82' 'D2_333' 'D1_889' 'D1_46' 'D2_40'\n",
      " 'D2_301' 'D2_825' 'D2_313' 'D1_451' 'D1_748' 'D2_810' 'D2_535' 'D2_344'\n",
      " 'D2_229' 'D1_521' 'D1_199' 'N_137' 'D1_483' 'D1_916' 'D1_343' 'D1_671'\n",
      " 'D1_855' 'D1_234' 'D1_716' 'D2_649' 'D2_978' 'D2_20' 'D2_672' 'D2_590'\n",
      " 'N_997' 'N_810' 'D2_213' 'D2_670' 'D2_798' 'N_591' 'D2_665' 'N_699'\n",
      " 'N_495' 'N_766' 'N_976' 'D1_195' 'D2_735' 'N_424' 'D1_286' 'N_980'\n",
      " 'D2_881' 'N_579' 'N_599' 'D2_885' 'D2_118' 'N_572' 'D2_773' 'D1_630'\n",
      " 'D1_759' 'D1_392' 'N_631' 'D1_406' 'D1_582' 'D2_921' 'D2_769' 'D1_212'\n",
      " 'D1_213' 'N_588' 'D2_702' 'D1_753' 'N_762' 'D2_755' 'N_42' 'D1_158'\n",
      " 'D1_261' 'D2_218' 'D2_425' 'D1_933' 'D2_478' 'D2_251' 'N_770' 'D1_21'\n",
      " 'D1_918' 'N_45' 'D2_779' 'D2_66' 'D1_595' 'D1_289' 'D1_963' 'D1_558'\n",
      " 'N_180' 'D1_895' 'D1_129' 'D1_744' 'D1_787' 'D2_570' 'D2_277' 'D1_416'\n",
      " 'D2_455' 'D1_762' 'D2_537' 'N_185' 'D1_196' 'N_672' 'D2_683' 'D2_919'\n",
      " 'D2_24' 'D2_800' 'D2_257' 'D2_640' 'N_826' 'N_429' 'N_951' 'N_813'\n",
      " 'D2_692' 'D1_577' 'D1_878' 'D1_358' 'N_12' 'D1_254' 'D1_227' 'D1_317'\n",
      " 'D1_634' 'N_71' 'N_263' 'D2_99' 'D1_667' 'D2_762' 'N_430' 'N_600'\n",
      " 'D1_563' 'D2_628' 'D2_843' 'D2_226' 'N_260' 'D2_128' 'D2_207' 'D2_121'\n",
      " 'D2_427' 'D2_847' 'D2_599' 'D2_451' 'N_0' 'N_265' 'N_179' 'D1_450'\n",
      " 'N_902' 'D2_842' 'D1_633' 'N_465' 'D1_814' 'D1_726' 'D2_11' 'D2_801'\n",
      " 'D1_5' 'N_892' 'D1_73' 'D2_23' 'D2_896' 'D2_224' 'D2_943' 'D2_994'\n",
      " 'N_789' 'N_682' 'N_841' 'D1_578' 'D2_6' 'N_615' 'D1_402' 'D2_281'\n",
      " 'D1_781' 'D2_28' 'D1_182' 'D1_107' 'D1_780' 'D1_815' 'D2_718' 'D2_442'\n",
      " 'N_64' 'D1_980' 'D2_75' 'N_125' 'N_136' 'D2_676' 'N_966' 'D2_360'\n",
      " 'D2_112' 'N_315' 'D1_138' 'D2_151' 'D2_562' 'N_888' 'D2_765' 'D1_846'\n",
      " 'N_704' 'D1_601' 'D1_66' 'D2_895' 'N_638' 'N_560' 'D2_934' 'D2_477'\n",
      " 'D2_405' 'D2_74' 'D1_248' 'N_863' 'N_48' 'D2_566' 'D1_61' 'D1_307'\n",
      " 'D1_794' 'D2_148' 'N_121' 'D1_792' 'D1_444' 'N_356' 'D1_559' 'D2_385'\n",
      " 'D1_535' 'D1_151' 'D1_383' 'N_116' 'D2_401' 'D1_884' 'D2_244' 'N_426'\n",
      " 'D1_949' 'N_272' 'D2_573']\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(train)\n",
    "print(index[test])\n",
    "print(targets[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.mkdir('E:/3.18/MTF2' + '/' + 'train')\n",
    "os.mkdir('E:/3.18/MTF2' + '/' + 'test')\n",
    "os.mkdir('E:/3.18/MTF2/' + 'weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2160,) (240,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JWH\\anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 81s 121ms/step - loss: 0.3120 - accuracy: 0.8042 - val_loss: 0.6473 - val_accuracy: 0.6333\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 64s 118ms/step - loss: 0.2219 - accuracy: 0.8690 - val_loss: 0.1361 - val_accuracy: 0.9292\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.1930 - accuracy: 0.8894 - val_loss: 0.2196 - val_accuracy: 0.8000\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.1716 - accuracy: 0.9019 - val_loss: 0.1538 - val_accuracy: 0.9125\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.1539 - accuracy: 0.9204 - val_loss: 0.1362 - val_accuracy: 0.9250\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.1346 - accuracy: 0.9315 - val_loss: 0.1303 - val_accuracy: 0.9250\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.1289 - accuracy: 0.9366 - val_loss: 0.1363 - val_accuracy: 0.9083\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.1199 - accuracy: 0.9394 - val_loss: 0.1651 - val_accuracy: 0.9167\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.1149 - accuracy: 0.9398 - val_loss: 0.2078 - val_accuracy: 0.9000\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.1002 - accuracy: 0.9486 - val_loss: 0.1121 - val_accuracy: 0.9167\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0989 - accuracy: 0.9500 - val_loss: 0.1478 - val_accuracy: 0.9292\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.1019 - accuracy: 0.9505 - val_loss: 0.1433 - val_accuracy: 0.9333\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0861 - accuracy: 0.9602 - val_loss: 0.1055 - val_accuracy: 0.9250\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0802 - accuracy: 0.9644 - val_loss: 0.1270 - val_accuracy: 0.9083\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0829 - accuracy: 0.9606 - val_loss: 0.1606 - val_accuracy: 0.8875\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0695 - accuracy: 0.9681 - val_loss: 0.1191 - val_accuracy: 0.9250\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0677 - accuracy: 0.9699 - val_loss: 0.1497 - val_accuracy: 0.9208\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0703 - accuracy: 0.9676 - val_loss: 0.1740 - val_accuracy: 0.9250\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0628 - accuracy: 0.9699 - val_loss: 0.1603 - val_accuracy: 0.9292\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0565 - accuracy: 0.9727 - val_loss: 0.1451 - val_accuracy: 0.9000\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0543 - accuracy: 0.9769 - val_loss: 0.1297 - val_accuracy: 0.9333\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0418 - accuracy: 0.9838 - val_loss: 0.1315 - val_accuracy: 0.9333\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0348 - accuracy: 0.9907 - val_loss: 0.1296 - val_accuracy: 0.9292\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0353 - accuracy: 0.9894 - val_loss: 0.1294 - val_accuracy: 0.9250\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0379 - accuracy: 0.9843 - val_loss: 0.1457 - val_accuracy: 0.9333\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0318 - accuracy: 0.9903 - val_loss: 0.1246 - val_accuracy: 0.9292\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0353 - accuracy: 0.9880 - val_loss: 0.1330 - val_accuracy: 0.9292\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0349 - accuracy: 0.9894 - val_loss: 0.1355 - val_accuracy: 0.9333\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0339 - accuracy: 0.9880 - val_loss: 0.1334 - val_accuracy: 0.9292\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0353 - accuracy: 0.9898 - val_loss: 0.1276 - val_accuracy: 0.9292\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0335 - accuracy: 0.9866 - val_loss: 0.1290 - val_accuracy: 0.9250\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0327 - accuracy: 0.9880 - val_loss: 0.1364 - val_accuracy: 0.9292\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0337 - accuracy: 0.9898 - val_loss: 0.1338 - val_accuracy: 0.9292\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0354 - accuracy: 0.9898 - val_loss: 0.1439 - val_accuracy: 0.9292\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.0301 - accuracy: 0.9912 - val_loss: 0.1383 - val_accuracy: 0.9292\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0306 - accuracy: 0.9912 - val_loss: 0.1369 - val_accuracy: 0.9292\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0299 - accuracy: 0.9912 - val_loss: 0.1273 - val_accuracy: 0.9250\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0300 - accuracy: 0.9917 - val_loss: 0.1407 - val_accuracy: 0.9292\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0312 - accuracy: 0.9884 - val_loss: 0.1370 - val_accuracy: 0.9292\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0316 - accuracy: 0.9912 - val_loss: 0.1337 - val_accuracy: 0.9292\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0333 - accuracy: 0.9889 - val_loss: 0.1328 - val_accuracy: 0.9292\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0327 - accuracy: 0.9912 - val_loss: 0.1417 - val_accuracy: 0.9292\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0324 - accuracy: 0.9889 - val_loss: 0.1356 - val_accuracy: 0.9292\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0250 - accuracy: 0.9954 - val_loss: 0.1419 - val_accuracy: 0.9292\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0322 - accuracy: 0.9907 - val_loss: 0.1408 - val_accuracy: 0.9292\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0275 - accuracy: 0.9940 - val_loss: 0.1384 - val_accuracy: 0.9292\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0262 - accuracy: 0.9921 - val_loss: 0.1467 - val_accuracy: 0.9292\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0250 - accuracy: 0.9954 - val_loss: 0.1467 - val_accuracy: 0.9292\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0308 - accuracy: 0.9907 - val_loss: 0.1384 - val_accuracy: 0.9292\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0332 - accuracy: 0.9898 - val_loss: 0.1391 - val_accuracy: 0.9292\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0317 - accuracy: 0.9907 - val_loss: 0.1387 - val_accuracy: 0.9292\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0372 - accuracy: 0.9861 - val_loss: 0.1458 - val_accuracy: 0.9292\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 63s 118ms/step - loss: 0.0263 - accuracy: 0.9921 - val_loss: 0.1432 - val_accuracy: 0.9292\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0262 - accuracy: 0.9931 - val_loss: 0.1426 - val_accuracy: 0.9292\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0283 - accuracy: 0.9935 - val_loss: 0.1442 - val_accuracy: 0.9292\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0272 - accuracy: 0.9944 - val_loss: 0.1466 - val_accuracy: 0.9292\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0299 - accuracy: 0.9912 - val_loss: 0.1402 - val_accuracy: 0.9292\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0290 - accuracy: 0.9912 - val_loss: 0.1437 - val_accuracy: 0.9292\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0285 - accuracy: 0.9917 - val_loss: 0.1403 - val_accuracy: 0.9292\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0280 - accuracy: 0.9921 - val_loss: 0.1414 - val_accuracy: 0.9292\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0282 - accuracy: 0.9917 - val_loss: 0.1354 - val_accuracy: 0.9292\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0281 - accuracy: 0.9921 - val_loss: 0.1412 - val_accuracy: 0.9292\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0318 - accuracy: 0.9898 - val_loss: 0.1442 - val_accuracy: 0.9292\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0300 - accuracy: 0.9917 - val_loss: 0.1397 - val_accuracy: 0.9292\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0249 - accuracy: 0.9926 - val_loss: 0.1388 - val_accuracy: 0.9292\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0294 - accuracy: 0.9917 - val_loss: 0.1411 - val_accuracy: 0.9292\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0269 - accuracy: 0.9907 - val_loss: 0.1440 - val_accuracy: 0.9292\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.0243 - accuracy: 0.9935 - val_loss: 0.1377 - val_accuracy: 0.9292\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0292 - accuracy: 0.9917 - val_loss: 0.1401 - val_accuracy: 0.9292\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.0341 - accuracy: 0.9898 - val_loss: 0.1360 - val_accuracy: 0.9292\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0276 - accuracy: 0.9912 - val_loss: 0.1389 - val_accuracy: 0.9292\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0272 - accuracy: 0.9931 - val_loss: 0.1369 - val_accuracy: 0.9292\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0291 - accuracy: 0.9912 - val_loss: 0.1412 - val_accuracy: 0.9292\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 61s 112ms/step - loss: 0.0265 - accuracy: 0.9972 - val_loss: 0.1388 - val_accuracy: 0.9292\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0296 - accuracy: 0.9921 - val_loss: 0.1400 - val_accuracy: 0.9292\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0301 - accuracy: 0.9903 - val_loss: 0.1391 - val_accuracy: 0.9292\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0253 - accuracy: 0.9921 - val_loss: 0.1423 - val_accuracy: 0.9292\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0308 - accuracy: 0.9903 - val_loss: 0.1395 - val_accuracy: 0.9292\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0300 - accuracy: 0.9894 - val_loss: 0.1421 - val_accuracy: 0.9292\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0303 - accuracy: 0.9917 - val_loss: 0.1413 - val_accuracy: 0.9292\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0270 - accuracy: 0.9949 - val_loss: 0.1404 - val_accuracy: 0.9292\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0290 - accuracy: 0.9898 - val_loss: 0.1429 - val_accuracy: 0.9292\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0283 - accuracy: 0.9917 - val_loss: 0.1383 - val_accuracy: 0.9292\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0276 - accuracy: 0.9912 - val_loss: 0.1438 - val_accuracy: 0.9292\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0245 - accuracy: 0.9926 - val_loss: 0.1378 - val_accuracy: 0.9292\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0275 - accuracy: 0.9907 - val_loss: 0.1419 - val_accuracy: 0.9292\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0304 - accuracy: 0.9921 - val_loss: 0.1396 - val_accuracy: 0.9292\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0290 - accuracy: 0.9917 - val_loss: 0.1420 - val_accuracy: 0.9292\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0271 - accuracy: 0.9917 - val_loss: 0.1437 - val_accuracy: 0.9292\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0338 - accuracy: 0.9907 - val_loss: 0.1389 - val_accuracy: 0.9292\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0311 - accuracy: 0.9894 - val_loss: 0.1512 - val_accuracy: 0.9333\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0224 - accuracy: 0.9972 - val_loss: 0.1412 - val_accuracy: 0.9292\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0300 - accuracy: 0.9898 - val_loss: 0.1396 - val_accuracy: 0.9292\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0303 - accuracy: 0.9903 - val_loss: 0.1455 - val_accuracy: 0.9292\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 59s 110ms/step - loss: 0.0245 - accuracy: 0.9949 - val_loss: 0.1413 - val_accuracy: 0.9292\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0253 - accuracy: 0.9935 - val_loss: 0.1432 - val_accuracy: 0.9292\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0335 - accuracy: 0.9894 - val_loss: 0.1377 - val_accuracy: 0.9292\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0309 - accuracy: 0.9903 - val_loss: 0.1435 - val_accuracy: 0.9292\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0273 - accuracy: 0.9949 - val_loss: 0.1451 - val_accuracy: 0.9292\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0314 - accuracy: 0.9903 - val_loss: 0.1407 - val_accuracy: 0.9292\n",
      "Score for fold 1: loss of 0.14074088633060455; accuracy of 92.91666746139526%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 69s 111ms/step - loss: 0.3222 - accuracy: 0.7833 - val_loss: 0.2270 - val_accuracy: 0.8958\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.2264 - accuracy: 0.8657 - val_loss: 0.1849 - val_accuracy: 0.9000\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.2000 - accuracy: 0.8875 - val_loss: 0.2607 - val_accuracy: 0.8667\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1705 - accuracy: 0.9130 - val_loss: 0.1078 - val_accuracy: 0.9375\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1548 - accuracy: 0.9190 - val_loss: 0.1144 - val_accuracy: 0.9375\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1346 - accuracy: 0.9273 - val_loss: 0.2602 - val_accuracy: 0.7875\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.1297 - accuracy: 0.9338 - val_loss: 0.0981 - val_accuracy: 0.9458\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.1219 - accuracy: 0.9407 - val_loss: 0.1093 - val_accuracy: 0.9458\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.1118 - accuracy: 0.9454 - val_loss: 0.1026 - val_accuracy: 0.9500\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.1027 - accuracy: 0.9505 - val_loss: 0.1017 - val_accuracy: 0.9458\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0975 - accuracy: 0.9537 - val_loss: 0.0946 - val_accuracy: 0.9417\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.1016 - accuracy: 0.9481 - val_loss: 0.0845 - val_accuracy: 0.9542\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0862 - accuracy: 0.9611 - val_loss: 0.0927 - val_accuracy: 0.9500\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0869 - accuracy: 0.9579 - val_loss: 0.0954 - val_accuracy: 0.9417\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0868 - accuracy: 0.9583 - val_loss: 0.1147 - val_accuracy: 0.9417\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0760 - accuracy: 0.9625 - val_loss: 0.1018 - val_accuracy: 0.9417\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0682 - accuracy: 0.9713 - val_loss: 0.0850 - val_accuracy: 0.9542\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0649 - accuracy: 0.9750 - val_loss: 0.1418 - val_accuracy: 0.9500\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0650 - accuracy: 0.9708 - val_loss: 0.1195 - val_accuracy: 0.9500\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0608 - accuracy: 0.9773 - val_loss: 0.1002 - val_accuracy: 0.9250\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0505 - accuracy: 0.9782 - val_loss: 0.0925 - val_accuracy: 0.9458\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0514 - accuracy: 0.9796 - val_loss: 0.0885 - val_accuracy: 0.9500\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0425 - accuracy: 0.9861 - val_loss: 0.0949 - val_accuracy: 0.9458\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0373 - accuracy: 0.9870 - val_loss: 0.0911 - val_accuracy: 0.9458\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0347 - accuracy: 0.9889 - val_loss: 0.0944 - val_accuracy: 0.9458\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0343 - accuracy: 0.9912 - val_loss: 0.0918 - val_accuracy: 0.9458\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0373 - accuracy: 0.9898 - val_loss: 0.0933 - val_accuracy: 0.9458\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0340 - accuracy: 0.9921 - val_loss: 0.0967 - val_accuracy: 0.9458\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 59s 110ms/step - loss: 0.0431 - accuracy: 0.9815 - val_loss: 0.0939 - val_accuracy: 0.9458\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0325 - accuracy: 0.9889 - val_loss: 0.0933 - val_accuracy: 0.9458\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0333 - accuracy: 0.9903 - val_loss: 0.1009 - val_accuracy: 0.9500\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0383 - accuracy: 0.9875 - val_loss: 0.1033 - val_accuracy: 0.9458\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0317 - accuracy: 0.9894 - val_loss: 0.0991 - val_accuracy: 0.9458\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0286 - accuracy: 0.9921 - val_loss: 0.1020 - val_accuracy: 0.9458\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0329 - accuracy: 0.9898 - val_loss: 0.1013 - val_accuracy: 0.9500\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0325 - accuracy: 0.9875 - val_loss: 0.1017 - val_accuracy: 0.9500\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 59s 110ms/step - loss: 0.0287 - accuracy: 0.9921 - val_loss: 0.1016 - val_accuracy: 0.9458\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 59s 110ms/step - loss: 0.0362 - accuracy: 0.9866 - val_loss: 0.0973 - val_accuracy: 0.9500\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0328 - accuracy: 0.9889 - val_loss: 0.1030 - val_accuracy: 0.9458\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0309 - accuracy: 0.9912 - val_loss: 0.1015 - val_accuracy: 0.9458\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0300 - accuracy: 0.9926 - val_loss: 0.1032 - val_accuracy: 0.9458\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0320 - accuracy: 0.9907 - val_loss: 0.1016 - val_accuracy: 0.9417\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0311 - accuracy: 0.9907 - val_loss: 0.1023 - val_accuracy: 0.9458\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0329 - accuracy: 0.9875 - val_loss: 0.1012 - val_accuracy: 0.9458\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0325 - accuracy: 0.9907 - val_loss: 0.1019 - val_accuracy: 0.9417\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0319 - accuracy: 0.9903 - val_loss: 0.1035 - val_accuracy: 0.9458\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0270 - accuracy: 0.9954 - val_loss: 0.1029 - val_accuracy: 0.9417\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0251 - accuracy: 0.9931 - val_loss: 0.1012 - val_accuracy: 0.9417\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0304 - accuracy: 0.9912 - val_loss: 0.1028 - val_accuracy: 0.9458\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0244 - accuracy: 0.9931 - val_loss: 0.1042 - val_accuracy: 0.9458\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0324 - accuracy: 0.9880 - val_loss: 0.1039 - val_accuracy: 0.9417\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0254 - accuracy: 0.9944 - val_loss: 0.1053 - val_accuracy: 0.9458\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0304 - accuracy: 0.9894 - val_loss: 0.1026 - val_accuracy: 0.9417\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0289 - accuracy: 0.9921 - val_loss: 0.1028 - val_accuracy: 0.9458\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0297 - accuracy: 0.9912 - val_loss: 0.1051 - val_accuracy: 0.9458\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0278 - accuracy: 0.9944 - val_loss: 0.1028 - val_accuracy: 0.9417\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0287 - accuracy: 0.9917 - val_loss: 0.1054 - val_accuracy: 0.9458\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 60s 110ms/step - loss: 0.0374 - accuracy: 0.9843 - val_loss: 0.1043 - val_accuracy: 0.9458\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0344 - accuracy: 0.9875 - val_loss: 0.1040 - val_accuracy: 0.9458\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0268 - accuracy: 0.9931 - val_loss: 0.1032 - val_accuracy: 0.9458\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0292 - accuracy: 0.9903 - val_loss: 0.1022 - val_accuracy: 0.9458\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0291 - accuracy: 0.9921 - val_loss: 0.1031 - val_accuracy: 0.9458\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0318 - accuracy: 0.9889 - val_loss: 0.1013 - val_accuracy: 0.9417\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0386 - accuracy: 0.9866 - val_loss: 0.1022 - val_accuracy: 0.9458\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0348 - accuracy: 0.9870 - val_loss: 0.1046 - val_accuracy: 0.9417\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0349 - accuracy: 0.9880 - val_loss: 0.1023 - val_accuracy: 0.9417\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0318 - accuracy: 0.9917 - val_loss: 0.1039 - val_accuracy: 0.9458\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0336 - accuracy: 0.9884 - val_loss: 0.1030 - val_accuracy: 0.9458\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0267 - accuracy: 0.9940 - val_loss: 0.1034 - val_accuracy: 0.9458\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0364 - accuracy: 0.9866 - val_loss: 0.1047 - val_accuracy: 0.9458\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0356 - accuracy: 0.9894 - val_loss: 0.1031 - val_accuracy: 0.9458\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0276 - accuracy: 0.9931 - val_loss: 0.1042 - val_accuracy: 0.9417\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0338 - accuracy: 0.9884 - val_loss: 0.1037 - val_accuracy: 0.9417\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0236 - accuracy: 0.9944 - val_loss: 0.1036 - val_accuracy: 0.9417\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0278 - accuracy: 0.9926 - val_loss: 0.1057 - val_accuracy: 0.9458\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0270 - accuracy: 0.9931 - val_loss: 0.1030 - val_accuracy: 0.9417\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0330 - accuracy: 0.9880 - val_loss: 0.1044 - val_accuracy: 0.9417\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0285 - accuracy: 0.9949 - val_loss: 0.1044 - val_accuracy: 0.9417\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0324 - accuracy: 0.9898 - val_loss: 0.1031 - val_accuracy: 0.9417\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0303 - accuracy: 0.9917 - val_loss: 0.1042 - val_accuracy: 0.9458\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0278 - accuracy: 0.9907 - val_loss: 0.1034 - val_accuracy: 0.9458\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0323 - accuracy: 0.9907 - val_loss: 0.1032 - val_accuracy: 0.9417\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0339 - accuracy: 0.9912 - val_loss: 0.1043 - val_accuracy: 0.9458\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0312 - accuracy: 0.9912 - val_loss: 0.1044 - val_accuracy: 0.9458\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0327 - accuracy: 0.9907 - val_loss: 0.1044 - val_accuracy: 0.9458\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0262 - accuracy: 0.9926 - val_loss: 0.1044 - val_accuracy: 0.9458\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0294 - accuracy: 0.9926 - val_loss: 0.1037 - val_accuracy: 0.9458\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0298 - accuracy: 0.9898 - val_loss: 0.1035 - val_accuracy: 0.9417\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0309 - accuracy: 0.9889 - val_loss: 0.1034 - val_accuracy: 0.9458\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0274 - accuracy: 0.9917 - val_loss: 0.1018 - val_accuracy: 0.9417\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0304 - accuracy: 0.9898 - val_loss: 0.1051 - val_accuracy: 0.9417\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0302 - accuracy: 0.9903 - val_loss: 0.1024 - val_accuracy: 0.9417\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0287 - accuracy: 0.9903 - val_loss: 0.1040 - val_accuracy: 0.9417\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0303 - accuracy: 0.9917 - val_loss: 0.1037 - val_accuracy: 0.9458\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0280 - accuracy: 0.9921 - val_loss: 0.1049 - val_accuracy: 0.9458\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0301 - accuracy: 0.9880 - val_loss: 0.1029 - val_accuracy: 0.9417\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0276 - accuracy: 0.9940 - val_loss: 0.1018 - val_accuracy: 0.9417\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0321 - accuracy: 0.9884 - val_loss: 0.1029 - val_accuracy: 0.9417\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0307 - accuracy: 0.9903 - val_loss: 0.1015 - val_accuracy: 0.9417\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0290 - accuracy: 0.9917 - val_loss: 0.1022 - val_accuracy: 0.9417\n",
      "Score for fold 2: loss of 0.10217005759477615; accuracy of 94.16666626930237%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 68s 111ms/step - loss: 0.3148 - accuracy: 0.8000 - val_loss: 0.5671 - val_accuracy: 0.6708\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.2273 - accuracy: 0.8722 - val_loss: 0.1377 - val_accuracy: 0.9250\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1970 - accuracy: 0.8819 - val_loss: 0.1127 - val_accuracy: 0.9375\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1693 - accuracy: 0.9019 - val_loss: 0.0862 - val_accuracy: 0.9500\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.1561 - accuracy: 0.9199 - val_loss: 0.1025 - val_accuracy: 0.9375\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.1396 - accuracy: 0.9190 - val_loss: 0.0811 - val_accuracy: 0.9667\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.1352 - accuracy: 0.9250 - val_loss: 0.1006 - val_accuracy: 0.9625\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1242 - accuracy: 0.9315 - val_loss: 0.0891 - val_accuracy: 0.9500\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1114 - accuracy: 0.9472 - val_loss: 0.0914 - val_accuracy: 0.9500\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 60s 112ms/step - loss: 0.1167 - accuracy: 0.9352 - val_loss: 0.6089 - val_accuracy: 0.6667\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.1070 - accuracy: 0.9444 - val_loss: 0.0837 - val_accuracy: 0.9625\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 61s 113ms/step - loss: 0.1037 - accuracy: 0.9491 - val_loss: 0.1043 - val_accuracy: 0.9333\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1022 - accuracy: 0.9458 - val_loss: 0.1225 - val_accuracy: 0.9167\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0905 - accuracy: 0.9556 - val_loss: 0.0750 - val_accuracy: 0.9625\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0813 - accuracy: 0.9593 - val_loss: 0.0895 - val_accuracy: 0.9583\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0870 - accuracy: 0.9574 - val_loss: 0.0966 - val_accuracy: 0.9583\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0725 - accuracy: 0.9722 - val_loss: 0.1026 - val_accuracy: 0.9458\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0722 - accuracy: 0.9648 - val_loss: 0.1289 - val_accuracy: 0.9333\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0674 - accuracy: 0.9708 - val_loss: 0.0887 - val_accuracy: 0.9667\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0568 - accuracy: 0.9778 - val_loss: 0.1071 - val_accuracy: 0.9292\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0489 - accuracy: 0.9829 - val_loss: 0.0845 - val_accuracy: 0.9625\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0399 - accuracy: 0.9856 - val_loss: 0.0890 - val_accuracy: 0.9583\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0419 - accuracy: 0.9838 - val_loss: 0.0890 - val_accuracy: 0.9583\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0416 - accuracy: 0.9861 - val_loss: 0.0982 - val_accuracy: 0.9542\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0437 - accuracy: 0.9861 - val_loss: 0.0906 - val_accuracy: 0.9625\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0402 - accuracy: 0.9852 - val_loss: 0.1009 - val_accuracy: 0.9583\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0387 - accuracy: 0.9889 - val_loss: 0.0944 - val_accuracy: 0.9667\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0377 - accuracy: 0.9884 - val_loss: 0.0953 - val_accuracy: 0.9667\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0358 - accuracy: 0.9889 - val_loss: 0.0915 - val_accuracy: 0.9667\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0330 - accuracy: 0.9898 - val_loss: 0.0973 - val_accuracy: 0.9667\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0299 - accuracy: 0.9894 - val_loss: 0.0938 - val_accuracy: 0.9667\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0321 - accuracy: 0.9889 - val_loss: 0.0957 - val_accuracy: 0.9667\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0314 - accuracy: 0.9903 - val_loss: 0.0979 - val_accuracy: 0.9667\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0348 - accuracy: 0.9894 - val_loss: 0.0961 - val_accuracy: 0.9667\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 58s 106ms/step - loss: 0.0295 - accuracy: 0.9898 - val_loss: 0.0995 - val_accuracy: 0.9583\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0281 - accuracy: 0.9912 - val_loss: 0.1040 - val_accuracy: 0.9625\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0306 - accuracy: 0.9926 - val_loss: 0.0963 - val_accuracy: 0.9625\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0265 - accuracy: 0.9921 - val_loss: 0.0997 - val_accuracy: 0.9625\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0295 - accuracy: 0.9907 - val_loss: 0.0987 - val_accuracy: 0.9625\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0324 - accuracy: 0.9898 - val_loss: 0.0979 - val_accuracy: 0.9625\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0322 - accuracy: 0.9907 - val_loss: 0.0992 - val_accuracy: 0.9667\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0305 - accuracy: 0.9935 - val_loss: 0.0996 - val_accuracy: 0.9625\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0309 - accuracy: 0.9935 - val_loss: 0.0987 - val_accuracy: 0.9625\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0346 - accuracy: 0.9884 - val_loss: 0.0988 - val_accuracy: 0.9667\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0235 - accuracy: 0.9940 - val_loss: 0.0983 - val_accuracy: 0.9667\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0315 - accuracy: 0.9912 - val_loss: 0.0990 - val_accuracy: 0.9667\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0277 - accuracy: 0.9917 - val_loss: 0.0974 - val_accuracy: 0.9625\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0351 - accuracy: 0.9875 - val_loss: 0.0991 - val_accuracy: 0.9625\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0312 - accuracy: 0.9907 - val_loss: 0.0978 - val_accuracy: 0.9667\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0315 - accuracy: 0.9903 - val_loss: 0.0976 - val_accuracy: 0.9667\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0277 - accuracy: 0.9903 - val_loss: 0.0980 - val_accuracy: 0.9667\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0287 - accuracy: 0.9917 - val_loss: 0.0988 - val_accuracy: 0.9667\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0270 - accuracy: 0.9921 - val_loss: 0.0971 - val_accuracy: 0.9667\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0284 - accuracy: 0.9903 - val_loss: 0.0981 - val_accuracy: 0.9667\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0276 - accuracy: 0.9921 - val_loss: 0.0993 - val_accuracy: 0.9667\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0329 - accuracy: 0.9884 - val_loss: 0.0989 - val_accuracy: 0.9625\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0323 - accuracy: 0.9898 - val_loss: 0.0981 - val_accuracy: 0.9667\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0294 - accuracy: 0.9903 - val_loss: 0.0986 - val_accuracy: 0.9667\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0293 - accuracy: 0.9903 - val_loss: 0.0981 - val_accuracy: 0.9667\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0280 - accuracy: 0.9917 - val_loss: 0.0990 - val_accuracy: 0.9667\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0257 - accuracy: 0.9926 - val_loss: 0.0981 - val_accuracy: 0.9667\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0296 - accuracy: 0.9917 - val_loss: 0.0991 - val_accuracy: 0.9625\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0300 - accuracy: 0.9926 - val_loss: 0.0986 - val_accuracy: 0.9667\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0279 - accuracy: 0.9926 - val_loss: 0.0978 - val_accuracy: 0.9667\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0301 - accuracy: 0.9903 - val_loss: 0.0972 - val_accuracy: 0.9667\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0338 - accuracy: 0.9931 - val_loss: 0.0988 - val_accuracy: 0.9625\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0316 - accuracy: 0.9889 - val_loss: 0.0985 - val_accuracy: 0.9625\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0268 - accuracy: 0.9931 - val_loss: 0.0984 - val_accuracy: 0.9667\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0242 - accuracy: 0.9912 - val_loss: 0.0993 - val_accuracy: 0.9667\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0288 - accuracy: 0.9917 - val_loss: 0.0985 - val_accuracy: 0.9667\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0333 - accuracy: 0.9889 - val_loss: 0.0980 - val_accuracy: 0.9667\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0292 - accuracy: 0.9903 - val_loss: 0.0976 - val_accuracy: 0.9667\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0315 - accuracy: 0.9903 - val_loss: 0.0977 - val_accuracy: 0.9667\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0279 - accuracy: 0.9926 - val_loss: 0.0985 - val_accuracy: 0.9667\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0339 - accuracy: 0.9894 - val_loss: 0.0981 - val_accuracy: 0.9667\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0273 - accuracy: 0.9912 - val_loss: 0.0981 - val_accuracy: 0.9667\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0258 - accuracy: 0.9944 - val_loss: 0.0990 - val_accuracy: 0.9667\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0301 - accuracy: 0.9912 - val_loss: 0.0982 - val_accuracy: 0.9667\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0299 - accuracy: 0.9903 - val_loss: 0.0981 - val_accuracy: 0.9667\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0332 - accuracy: 0.9903 - val_loss: 0.0982 - val_accuracy: 0.9667\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0329 - accuracy: 0.9907 - val_loss: 0.0979 - val_accuracy: 0.9667\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0296 - accuracy: 0.9898 - val_loss: 0.0983 - val_accuracy: 0.9667\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0299 - accuracy: 0.9912 - val_loss: 0.0990 - val_accuracy: 0.9667\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0264 - accuracy: 0.9931 - val_loss: 0.0987 - val_accuracy: 0.9667\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0286 - accuracy: 0.9912 - val_loss: 0.0993 - val_accuracy: 0.9625\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0281 - accuracy: 0.9926 - val_loss: 0.0988 - val_accuracy: 0.9667\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0269 - accuracy: 0.9931 - val_loss: 0.0985 - val_accuracy: 0.9667\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0308 - accuracy: 0.9921 - val_loss: 0.0995 - val_accuracy: 0.9667\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0297 - accuracy: 0.9926 - val_loss: 0.0985 - val_accuracy: 0.9667\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0301 - accuracy: 0.9912 - val_loss: 0.0981 - val_accuracy: 0.9667\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0284 - accuracy: 0.9912 - val_loss: 0.0990 - val_accuracy: 0.9625\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0268 - accuracy: 0.9912 - val_loss: 0.0995 - val_accuracy: 0.9625\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0298 - accuracy: 0.9889 - val_loss: 0.0989 - val_accuracy: 0.9667\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0307 - accuracy: 0.9884 - val_loss: 0.0979 - val_accuracy: 0.9667\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0309 - accuracy: 0.9903 - val_loss: 0.0983 - val_accuracy: 0.9667\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 58s 106ms/step - loss: 0.0312 - accuracy: 0.9917 - val_loss: 0.0983 - val_accuracy: 0.9625\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0277 - accuracy: 0.9917 - val_loss: 0.0995 - val_accuracy: 0.9667\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0281 - accuracy: 0.9912 - val_loss: 0.0976 - val_accuracy: 0.9667\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0261 - accuracy: 0.9935 - val_loss: 0.0964 - val_accuracy: 0.9667\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0262 - accuracy: 0.9921 - val_loss: 0.0984 - val_accuracy: 0.9667\n",
      "Score for fold 3: loss of 0.09843650460243225; accuracy of 96.66666388511658%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 68s 110ms/step - loss: 0.3033 - accuracy: 0.8088 - val_loss: 0.7613 - val_accuracy: 0.4542\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.2273 - accuracy: 0.8704 - val_loss: 0.1710 - val_accuracy: 0.9000\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1930 - accuracy: 0.8949 - val_loss: 0.1631 - val_accuracy: 0.9208\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1677 - accuracy: 0.9102 - val_loss: 0.1488 - val_accuracy: 0.9000\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1511 - accuracy: 0.9204 - val_loss: 0.1608 - val_accuracy: 0.8750\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1388 - accuracy: 0.9296 - val_loss: 0.3233 - val_accuracy: 0.7333\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1298 - accuracy: 0.9329 - val_loss: 0.1527 - val_accuracy: 0.9250\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.1220 - accuracy: 0.9417 - val_loss: 0.1108 - val_accuracy: 0.9292\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1131 - accuracy: 0.9440 - val_loss: 0.1278 - val_accuracy: 0.9292\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1079 - accuracy: 0.9421 - val_loss: 0.2322 - val_accuracy: 0.8042\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0963 - accuracy: 0.9472 - val_loss: 0.1078 - val_accuracy: 0.9250\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0997 - accuracy: 0.9495 - val_loss: 0.1741 - val_accuracy: 0.9208\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0928 - accuracy: 0.9546 - val_loss: 0.1059 - val_accuracy: 0.9417\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 60s 110ms/step - loss: 0.0809 - accuracy: 0.9602 - val_loss: 0.0998 - val_accuracy: 0.9333\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0888 - accuracy: 0.9556 - val_loss: 0.1060 - val_accuracy: 0.9417\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0783 - accuracy: 0.9648 - val_loss: 0.1021 - val_accuracy: 0.9333\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0736 - accuracy: 0.9620 - val_loss: 0.1045 - val_accuracy: 0.9333\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0641 - accuracy: 0.9694 - val_loss: 0.1324 - val_accuracy: 0.9333\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0705 - accuracy: 0.9685 - val_loss: 0.1110 - val_accuracy: 0.9333\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0574 - accuracy: 0.9773 - val_loss: 0.1047 - val_accuracy: 0.9292\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0513 - accuracy: 0.9824 - val_loss: 0.0991 - val_accuracy: 0.9333\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0446 - accuracy: 0.9824 - val_loss: 0.0991 - val_accuracy: 0.9375\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0320 - accuracy: 0.9921 - val_loss: 0.1019 - val_accuracy: 0.9375\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0348 - accuracy: 0.9889 - val_loss: 0.1025 - val_accuracy: 0.9375\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0415 - accuracy: 0.9861 - val_loss: 0.1019 - val_accuracy: 0.9417\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0396 - accuracy: 0.9870 - val_loss: 0.1056 - val_accuracy: 0.9417\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0357 - accuracy: 0.9903 - val_loss: 0.1029 - val_accuracy: 0.9417\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0305 - accuracy: 0.9903 - val_loss: 0.1065 - val_accuracy: 0.9292\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0346 - accuracy: 0.9884 - val_loss: 0.1092 - val_accuracy: 0.9375\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0366 - accuracy: 0.9875 - val_loss: 0.1092 - val_accuracy: 0.9375\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0358 - accuracy: 0.9894 - val_loss: 0.1070 - val_accuracy: 0.9375\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0353 - accuracy: 0.9894 - val_loss: 0.1074 - val_accuracy: 0.9375\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0386 - accuracy: 0.9880 - val_loss: 0.1069 - val_accuracy: 0.9417\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0307 - accuracy: 0.9907 - val_loss: 0.1092 - val_accuracy: 0.9417\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0361 - accuracy: 0.9847 - val_loss: 0.1094 - val_accuracy: 0.9458\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0421 - accuracy: 0.9824 - val_loss: 0.1136 - val_accuracy: 0.9375\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0323 - accuracy: 0.9907 - val_loss: 0.1089 - val_accuracy: 0.9417\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0244 - accuracy: 0.9958 - val_loss: 0.1155 - val_accuracy: 0.9417\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0291 - accuracy: 0.9903 - val_loss: 0.1112 - val_accuracy: 0.9458\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0294 - accuracy: 0.9894 - val_loss: 0.1111 - val_accuracy: 0.9417\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0259 - accuracy: 0.9931 - val_loss: 0.1100 - val_accuracy: 0.9417\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0259 - accuracy: 0.9940 - val_loss: 0.1102 - val_accuracy: 0.9417\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0296 - accuracy: 0.9921 - val_loss: 0.1080 - val_accuracy: 0.9333\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0260 - accuracy: 0.9935 - val_loss: 0.1112 - val_accuracy: 0.9417\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0271 - accuracy: 0.9931 - val_loss: 0.1096 - val_accuracy: 0.9458\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0263 - accuracy: 0.9926 - val_loss: 0.1111 - val_accuracy: 0.9417\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0316 - accuracy: 0.9903 - val_loss: 0.1096 - val_accuracy: 0.9417\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0312 - accuracy: 0.9921 - val_loss: 0.1101 - val_accuracy: 0.9333\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0314 - accuracy: 0.9907 - val_loss: 0.1088 - val_accuracy: 0.9333\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0298 - accuracy: 0.9921 - val_loss: 0.1120 - val_accuracy: 0.9417\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0337 - accuracy: 0.9889 - val_loss: 0.1119 - val_accuracy: 0.9417\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0305 - accuracy: 0.9889 - val_loss: 0.1088 - val_accuracy: 0.9333\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0296 - accuracy: 0.9921 - val_loss: 0.1123 - val_accuracy: 0.9458\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0293 - accuracy: 0.9926 - val_loss: 0.1124 - val_accuracy: 0.9458\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0324 - accuracy: 0.9880 - val_loss: 0.1123 - val_accuracy: 0.9417\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0364 - accuracy: 0.9875 - val_loss: 0.1101 - val_accuracy: 0.9375\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0316 - accuracy: 0.9898 - val_loss: 0.1117 - val_accuracy: 0.9417\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0284 - accuracy: 0.9940 - val_loss: 0.1094 - val_accuracy: 0.9417\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0285 - accuracy: 0.9926 - val_loss: 0.1105 - val_accuracy: 0.9375\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0327 - accuracy: 0.9884 - val_loss: 0.1098 - val_accuracy: 0.9333\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0315 - accuracy: 0.9907 - val_loss: 0.1122 - val_accuracy: 0.9417\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0294 - accuracy: 0.9894 - val_loss: 0.1132 - val_accuracy: 0.9417\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0366 - accuracy: 0.9861 - val_loss: 0.1148 - val_accuracy: 0.9375\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0251 - accuracy: 0.9958 - val_loss: 0.1121 - val_accuracy: 0.9375\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0334 - accuracy: 0.9894 - val_loss: 0.1116 - val_accuracy: 0.9417\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0241 - accuracy: 0.9949 - val_loss: 0.1105 - val_accuracy: 0.9333\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0320 - accuracy: 0.9898 - val_loss: 0.1115 - val_accuracy: 0.9375\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0268 - accuracy: 0.9940 - val_loss: 0.1128 - val_accuracy: 0.9417\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0295 - accuracy: 0.9926 - val_loss: 0.1136 - val_accuracy: 0.9417\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0254 - accuracy: 0.9921 - val_loss: 0.1143 - val_accuracy: 0.9417\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0265 - accuracy: 0.9931 - val_loss: 0.1119 - val_accuracy: 0.9417\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0269 - accuracy: 0.9935 - val_loss: 0.1121 - val_accuracy: 0.9375\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 60s 110ms/step - loss: 0.0335 - accuracy: 0.9884 - val_loss: 0.1113 - val_accuracy: 0.9375\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0338 - accuracy: 0.9894 - val_loss: 0.1118 - val_accuracy: 0.9375\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0286 - accuracy: 0.9898 - val_loss: 0.1115 - val_accuracy: 0.9333\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0252 - accuracy: 0.9940 - val_loss: 0.1116 - val_accuracy: 0.9375\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0342 - accuracy: 0.9875 - val_loss: 0.1143 - val_accuracy: 0.9417\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0295 - accuracy: 0.9931 - val_loss: 0.1131 - val_accuracy: 0.9375\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0260 - accuracy: 0.9944 - val_loss: 0.1127 - val_accuracy: 0.9375\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0328 - accuracy: 0.9884 - val_loss: 0.1116 - val_accuracy: 0.9375\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0274 - accuracy: 0.9921 - val_loss: 0.1120 - val_accuracy: 0.9375\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0231 - accuracy: 0.9949 - val_loss: 0.1144 - val_accuracy: 0.9417\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0282 - accuracy: 0.9903 - val_loss: 0.1147 - val_accuracy: 0.9417\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 58s 106ms/step - loss: 0.0232 - accuracy: 0.9940 - val_loss: 0.1128 - val_accuracy: 0.9417\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0302 - accuracy: 0.9912 - val_loss: 0.1123 - val_accuracy: 0.9417\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0350 - accuracy: 0.9880 - val_loss: 0.1109 - val_accuracy: 0.9333\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0285 - accuracy: 0.9907 - val_loss: 0.1133 - val_accuracy: 0.9417\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0285 - accuracy: 0.9921 - val_loss: 0.1111 - val_accuracy: 0.9417\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0255 - accuracy: 0.9935 - val_loss: 0.1114 - val_accuracy: 0.9375\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0268 - accuracy: 0.9921 - val_loss: 0.1138 - val_accuracy: 0.9375\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0240 - accuracy: 0.9954 - val_loss: 0.1122 - val_accuracy: 0.9333\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0263 - accuracy: 0.9917 - val_loss: 0.1119 - val_accuracy: 0.9333\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0343 - accuracy: 0.9894 - val_loss: 0.1112 - val_accuracy: 0.9375\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0219 - accuracy: 0.9963 - val_loss: 0.1120 - val_accuracy: 0.9375\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0309 - accuracy: 0.9903 - val_loss: 0.1105 - val_accuracy: 0.9333\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0347 - accuracy: 0.9866 - val_loss: 0.1142 - val_accuracy: 0.9417\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0316 - accuracy: 0.9894 - val_loss: 0.1128 - val_accuracy: 0.9333\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0314 - accuracy: 0.9912 - val_loss: 0.1142 - val_accuracy: 0.9417\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0264 - accuracy: 0.9954 - val_loss: 0.1116 - val_accuracy: 0.9375\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0278 - accuracy: 0.9903 - val_loss: 0.1122 - val_accuracy: 0.9375\n",
      "Score for fold 4: loss of 0.11224793642759323; accuracy of 93.75%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 69s 112ms/step - loss: 0.3168 - accuracy: 0.7815 - val_loss: 0.4849 - val_accuracy: 0.6458\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.2231 - accuracy: 0.8745 - val_loss: 0.1891 - val_accuracy: 0.9000\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.1950 - accuracy: 0.8926 - val_loss: 0.1292 - val_accuracy: 0.9292\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.1708 - accuracy: 0.9079 - val_loss: 0.1932 - val_accuracy: 0.9000\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 61s 113ms/step - loss: 0.1656 - accuracy: 0.9106 - val_loss: 0.1249 - val_accuracy: 0.9417\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1452 - accuracy: 0.9250 - val_loss: 0.1086 - val_accuracy: 0.9375\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.1327 - accuracy: 0.9282 - val_loss: 0.1150 - val_accuracy: 0.9500\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1230 - accuracy: 0.9375 - val_loss: 0.1549 - val_accuracy: 0.8833\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1146 - accuracy: 0.9394 - val_loss: 0.1070 - val_accuracy: 0.9500\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.1028 - accuracy: 0.9491 - val_loss: 0.0997 - val_accuracy: 0.9417\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.1160 - accuracy: 0.9421 - val_loss: 0.0888 - val_accuracy: 0.9542\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0974 - accuracy: 0.9509 - val_loss: 0.1072 - val_accuracy: 0.9333\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0891 - accuracy: 0.9560 - val_loss: 0.1963 - val_accuracy: 0.8417\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0896 - accuracy: 0.9505 - val_loss: 0.2216 - val_accuracy: 0.8542\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0950 - accuracy: 0.9546 - val_loss: 0.1172 - val_accuracy: 0.9500\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0790 - accuracy: 0.9625 - val_loss: 0.1853 - val_accuracy: 0.8583\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0752 - accuracy: 0.9588 - val_loss: 0.1013 - val_accuracy: 0.9542\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0691 - accuracy: 0.9667 - val_loss: 0.1118 - val_accuracy: 0.9167\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0656 - accuracy: 0.9694 - val_loss: 0.1033 - val_accuracy: 0.9500\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0586 - accuracy: 0.9713 - val_loss: 0.1106 - val_accuracy: 0.9417\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0636 - accuracy: 0.9685 - val_loss: 0.1035 - val_accuracy: 0.9375\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0523 - accuracy: 0.9806 - val_loss: 0.1073 - val_accuracy: 0.9375\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0485 - accuracy: 0.9787 - val_loss: 0.1104 - val_accuracy: 0.9333\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0431 - accuracy: 0.9856 - val_loss: 0.1056 - val_accuracy: 0.9458\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0420 - accuracy: 0.9843 - val_loss: 0.1068 - val_accuracy: 0.9458\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0342 - accuracy: 0.9903 - val_loss: 0.1038 - val_accuracy: 0.9417\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0479 - accuracy: 0.9810 - val_loss: 0.1058 - val_accuracy: 0.9458\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0394 - accuracy: 0.9870 - val_loss: 0.1041 - val_accuracy: 0.9417\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 59s 110ms/step - loss: 0.0346 - accuracy: 0.9880 - val_loss: 0.1073 - val_accuracy: 0.9375\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0437 - accuracy: 0.9838 - val_loss: 0.1042 - val_accuracy: 0.9417\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0424 - accuracy: 0.9856 - val_loss: 0.1054 - val_accuracy: 0.9417\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0369 - accuracy: 0.9889 - val_loss: 0.1058 - val_accuracy: 0.9417\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0415 - accuracy: 0.9843 - val_loss: 0.1062 - val_accuracy: 0.9417\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0330 - accuracy: 0.9912 - val_loss: 0.1067 - val_accuracy: 0.9417\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0323 - accuracy: 0.9903 - val_loss: 0.1124 - val_accuracy: 0.9458\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0404 - accuracy: 0.9847 - val_loss: 0.1087 - val_accuracy: 0.9417\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0376 - accuracy: 0.9870 - val_loss: 0.1095 - val_accuracy: 0.9417\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0352 - accuracy: 0.9889 - val_loss: 0.1123 - val_accuracy: 0.9417\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0294 - accuracy: 0.9931 - val_loss: 0.1099 - val_accuracy: 0.9417\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0356 - accuracy: 0.9889 - val_loss: 0.1084 - val_accuracy: 0.9417\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0264 - accuracy: 0.9949 - val_loss: 0.1093 - val_accuracy: 0.9417\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0391 - accuracy: 0.9861 - val_loss: 0.1086 - val_accuracy: 0.9417\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0376 - accuracy: 0.9898 - val_loss: 0.1100 - val_accuracy: 0.9417\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0336 - accuracy: 0.9875 - val_loss: 0.1102 - val_accuracy: 0.9417\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0351 - accuracy: 0.9866 - val_loss: 0.1110 - val_accuracy: 0.9417\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0371 - accuracy: 0.9861 - val_loss: 0.1096 - val_accuracy: 0.9417\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0318 - accuracy: 0.9903 - val_loss: 0.1095 - val_accuracy: 0.9417\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0287 - accuracy: 0.9935 - val_loss: 0.1088 - val_accuracy: 0.9417\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0347 - accuracy: 0.9894 - val_loss: 0.1109 - val_accuracy: 0.9417\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0335 - accuracy: 0.9875 - val_loss: 0.1093 - val_accuracy: 0.9417\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0328 - accuracy: 0.9898 - val_loss: 0.1103 - val_accuracy: 0.9417\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0304 - accuracy: 0.9944 - val_loss: 0.1105 - val_accuracy: 0.9417\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0336 - accuracy: 0.9894 - val_loss: 0.1101 - val_accuracy: 0.9417\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0395 - accuracy: 0.9870 - val_loss: 0.1104 - val_accuracy: 0.9417\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0322 - accuracy: 0.9912 - val_loss: 0.1100 - val_accuracy: 0.9417\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0379 - accuracy: 0.9870 - val_loss: 0.1104 - val_accuracy: 0.9417\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0352 - accuracy: 0.9894 - val_loss: 0.1101 - val_accuracy: 0.9417\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0336 - accuracy: 0.9907 - val_loss: 0.1104 - val_accuracy: 0.9417\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0365 - accuracy: 0.9866 - val_loss: 0.1110 - val_accuracy: 0.9417\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 59s 110ms/step - loss: 0.0343 - accuracy: 0.9898 - val_loss: 0.1110 - val_accuracy: 0.9417\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0317 - accuracy: 0.9898 - val_loss: 0.1104 - val_accuracy: 0.9417\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0275 - accuracy: 0.9912 - val_loss: 0.1108 - val_accuracy: 0.9417\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0306 - accuracy: 0.9912 - val_loss: 0.1106 - val_accuracy: 0.9417\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0347 - accuracy: 0.9875 - val_loss: 0.1108 - val_accuracy: 0.9417\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0335 - accuracy: 0.9875 - val_loss: 0.1109 - val_accuracy: 0.9417\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0373 - accuracy: 0.9880 - val_loss: 0.1108 - val_accuracy: 0.9417\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0311 - accuracy: 0.9912 - val_loss: 0.1101 - val_accuracy: 0.9417\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0361 - accuracy: 0.9861 - val_loss: 0.1115 - val_accuracy: 0.9417\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0306 - accuracy: 0.9898 - val_loss: 0.1115 - val_accuracy: 0.9417\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0399 - accuracy: 0.9856 - val_loss: 0.1111 - val_accuracy: 0.9417\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0298 - accuracy: 0.9907 - val_loss: 0.1098 - val_accuracy: 0.9417\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0345 - accuracy: 0.9866 - val_loss: 0.1106 - val_accuracy: 0.9417\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0372 - accuracy: 0.9852 - val_loss: 0.1114 - val_accuracy: 0.9417\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0342 - accuracy: 0.9884 - val_loss: 0.1113 - val_accuracy: 0.9417\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0316 - accuracy: 0.9889 - val_loss: 0.1112 - val_accuracy: 0.9417\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0291 - accuracy: 0.9926 - val_loss: 0.1117 - val_accuracy: 0.9417\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0323 - accuracy: 0.9903 - val_loss: 0.1112 - val_accuracy: 0.9417\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0327 - accuracy: 0.9889 - val_loss: 0.1118 - val_accuracy: 0.9417\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0327 - accuracy: 0.9889 - val_loss: 0.1128 - val_accuracy: 0.9417\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0412 - accuracy: 0.9833 - val_loss: 0.1108 - val_accuracy: 0.9417\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0401 - accuracy: 0.9861 - val_loss: 0.1120 - val_accuracy: 0.9417\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0304 - accuracy: 0.9907 - val_loss: 0.1100 - val_accuracy: 0.9417\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0325 - accuracy: 0.9894 - val_loss: 0.1118 - val_accuracy: 0.9417\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0310 - accuracy: 0.9912 - val_loss: 0.1107 - val_accuracy: 0.9417\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0342 - accuracy: 0.9884 - val_loss: 0.1119 - val_accuracy: 0.9417\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0366 - accuracy: 0.9843 - val_loss: 0.1121 - val_accuracy: 0.9417\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0364 - accuracy: 0.9875 - val_loss: 0.1118 - val_accuracy: 0.9417\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0312 - accuracy: 0.9912 - val_loss: 0.1112 - val_accuracy: 0.9417\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0383 - accuracy: 0.9894 - val_loss: 0.1131 - val_accuracy: 0.9417\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0347 - accuracy: 0.9898 - val_loss: 0.1117 - val_accuracy: 0.9417\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0299 - accuracy: 0.9912 - val_loss: 0.1108 - val_accuracy: 0.9417\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0308 - accuracy: 0.9921 - val_loss: 0.1121 - val_accuracy: 0.9417\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0318 - accuracy: 0.9912 - val_loss: 0.1121 - val_accuracy: 0.9417\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0343 - accuracy: 0.9866 - val_loss: 0.1134 - val_accuracy: 0.9417\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0328 - accuracy: 0.9903 - val_loss: 0.1116 - val_accuracy: 0.9417\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 60s 110ms/step - loss: 0.0315 - accuracy: 0.9907 - val_loss: 0.1114 - val_accuracy: 0.9417\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0357 - accuracy: 0.9852 - val_loss: 0.1104 - val_accuracy: 0.9417\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0338 - accuracy: 0.9866 - val_loss: 0.1106 - val_accuracy: 0.9417\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0404 - accuracy: 0.9838 - val_loss: 0.1131 - val_accuracy: 0.9417\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0301 - accuracy: 0.9917 - val_loss: 0.1123 - val_accuracy: 0.9417\n",
      "Score for fold 5: loss of 0.11230198293924332; accuracy of 94.16666626930237%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 69s 112ms/step - loss: 0.3089 - accuracy: 0.8032 - val_loss: 0.2813 - val_accuracy: 0.8125\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.2215 - accuracy: 0.8769 - val_loss: 0.1743 - val_accuracy: 0.9250\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.2001 - accuracy: 0.8847 - val_loss: 0.1524 - val_accuracy: 0.9000\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.1765 - accuracy: 0.9069 - val_loss: 0.1403 - val_accuracy: 0.9208\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.1443 - accuracy: 0.9204 - val_loss: 0.1069 - val_accuracy: 0.9500\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.1474 - accuracy: 0.9185 - val_loss: 0.1050 - val_accuracy: 0.9375\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1387 - accuracy: 0.9259 - val_loss: 0.1133 - val_accuracy: 0.9375\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1190 - accuracy: 0.9389 - val_loss: 0.0998 - val_accuracy: 0.9417\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.1135 - accuracy: 0.9394 - val_loss: 0.1712 - val_accuracy: 0.9167\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.1071 - accuracy: 0.9417 - val_loss: 0.0928 - val_accuracy: 0.9458\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 60s 110ms/step - loss: 0.1069 - accuracy: 0.9444 - val_loss: 0.0966 - val_accuracy: 0.9458\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0935 - accuracy: 0.9542 - val_loss: 0.1634 - val_accuracy: 0.9167\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0946 - accuracy: 0.9537 - val_loss: 0.0901 - val_accuracy: 0.9542\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0851 - accuracy: 0.9574 - val_loss: 0.1084 - val_accuracy: 0.9250\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0872 - accuracy: 0.9542 - val_loss: 0.0934 - val_accuracy: 0.9500\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0724 - accuracy: 0.9685 - val_loss: 0.1965 - val_accuracy: 0.9250\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0648 - accuracy: 0.9708 - val_loss: 0.0918 - val_accuracy: 0.9500\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0812 - accuracy: 0.9611 - val_loss: 0.1013 - val_accuracy: 0.9458\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0603 - accuracy: 0.9755 - val_loss: 0.1773 - val_accuracy: 0.8833\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0709 - accuracy: 0.9671 - val_loss: 0.0948 - val_accuracy: 0.9375\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0599 - accuracy: 0.9750 - val_loss: 0.0874 - val_accuracy: 0.9417\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0450 - accuracy: 0.9838 - val_loss: 0.0880 - val_accuracy: 0.9458\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0404 - accuracy: 0.9884 - val_loss: 0.0900 - val_accuracy: 0.9417\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0337 - accuracy: 0.9898 - val_loss: 0.0847 - val_accuracy: 0.9417\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0372 - accuracy: 0.9861 - val_loss: 0.0853 - val_accuracy: 0.9458\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0331 - accuracy: 0.9898 - val_loss: 0.0869 - val_accuracy: 0.9500\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0399 - accuracy: 0.9852 - val_loss: 0.0826 - val_accuracy: 0.9500\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0297 - accuracy: 0.9926 - val_loss: 0.0853 - val_accuracy: 0.9500\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0439 - accuracy: 0.9829 - val_loss: 0.0894 - val_accuracy: 0.9458\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0339 - accuracy: 0.9907 - val_loss: 0.0841 - val_accuracy: 0.9417\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0347 - accuracy: 0.9884 - val_loss: 0.0953 - val_accuracy: 0.9417\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0341 - accuracy: 0.9875 - val_loss: 0.0944 - val_accuracy: 0.9417\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0322 - accuracy: 0.9926 - val_loss: 0.1000 - val_accuracy: 0.9458\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0356 - accuracy: 0.9884 - val_loss: 0.0884 - val_accuracy: 0.9417\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0362 - accuracy: 0.9847 - val_loss: 0.0934 - val_accuracy: 0.9458\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0296 - accuracy: 0.9912 - val_loss: 0.0937 - val_accuracy: 0.9417\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 60s 110ms/step - loss: 0.0292 - accuracy: 0.9940 - val_loss: 0.1017 - val_accuracy: 0.9458\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0310 - accuracy: 0.9894 - val_loss: 0.0909 - val_accuracy: 0.9500\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0278 - accuracy: 0.9926 - val_loss: 0.0946 - val_accuracy: 0.9458\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0244 - accuracy: 0.9940 - val_loss: 0.0959 - val_accuracy: 0.9417\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0277 - accuracy: 0.9926 - val_loss: 0.0957 - val_accuracy: 0.9458\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0260 - accuracy: 0.9917 - val_loss: 0.0948 - val_accuracy: 0.9417\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0289 - accuracy: 0.9921 - val_loss: 0.0942 - val_accuracy: 0.9458\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0300 - accuracy: 0.9921 - val_loss: 0.0952 - val_accuracy: 0.9417\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0338 - accuracy: 0.9884 - val_loss: 0.0938 - val_accuracy: 0.9458\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0279 - accuracy: 0.9926 - val_loss: 0.0935 - val_accuracy: 0.9458\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0308 - accuracy: 0.9903 - val_loss: 0.0930 - val_accuracy: 0.9458\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0335 - accuracy: 0.9894 - val_loss: 0.0997 - val_accuracy: 0.9458\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0283 - accuracy: 0.9926 - val_loss: 0.0947 - val_accuracy: 0.9458\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0313 - accuracy: 0.9921 - val_loss: 0.0979 - val_accuracy: 0.9458\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 59s 110ms/step - loss: 0.0313 - accuracy: 0.9907 - val_loss: 0.0955 - val_accuracy: 0.9417\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0324 - accuracy: 0.9894 - val_loss: 0.0935 - val_accuracy: 0.9458\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0237 - accuracy: 0.9940 - val_loss: 0.0949 - val_accuracy: 0.9458\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0359 - accuracy: 0.9894 - val_loss: 0.0964 - val_accuracy: 0.9417\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0311 - accuracy: 0.9917 - val_loss: 0.0949 - val_accuracy: 0.9417\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0266 - accuracy: 0.9931 - val_loss: 0.0950 - val_accuracy: 0.9417\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0369 - accuracy: 0.9884 - val_loss: 0.0961 - val_accuracy: 0.9417\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0362 - accuracy: 0.9880 - val_loss: 0.0925 - val_accuracy: 0.9500\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0266 - accuracy: 0.9921 - val_loss: 0.0961 - val_accuracy: 0.9417\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0309 - accuracy: 0.9894 - val_loss: 0.0932 - val_accuracy: 0.9458\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0319 - accuracy: 0.9917 - val_loss: 0.0948 - val_accuracy: 0.9458\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0300 - accuracy: 0.9903 - val_loss: 0.0954 - val_accuracy: 0.9417\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0299 - accuracy: 0.9907 - val_loss: 0.0949 - val_accuracy: 0.9458\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0240 - accuracy: 0.9940 - val_loss: 0.0951 - val_accuracy: 0.9417\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0327 - accuracy: 0.9880 - val_loss: 0.0932 - val_accuracy: 0.9458\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0247 - accuracy: 0.9954 - val_loss: 0.0946 - val_accuracy: 0.9417\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0298 - accuracy: 0.9921 - val_loss: 0.0924 - val_accuracy: 0.9417\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0324 - accuracy: 0.9903 - val_loss: 0.0932 - val_accuracy: 0.9500\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0322 - accuracy: 0.9880 - val_loss: 0.0934 - val_accuracy: 0.9417\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0282 - accuracy: 0.9931 - val_loss: 0.0962 - val_accuracy: 0.9417\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0288 - accuracy: 0.9912 - val_loss: 0.0999 - val_accuracy: 0.9417\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0280 - accuracy: 0.9921 - val_loss: 0.0939 - val_accuracy: 0.9458\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0304 - accuracy: 0.9912 - val_loss: 0.0969 - val_accuracy: 0.9417\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0257 - accuracy: 0.9949 - val_loss: 0.0938 - val_accuracy: 0.9458\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0263 - accuracy: 0.9926 - val_loss: 0.0912 - val_accuracy: 0.9500\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0260 - accuracy: 0.9926 - val_loss: 0.0954 - val_accuracy: 0.9417\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0276 - accuracy: 0.9940 - val_loss: 0.0944 - val_accuracy: 0.9417\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0312 - accuracy: 0.9917 - val_loss: 0.0954 - val_accuracy: 0.9417\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0286 - accuracy: 0.9931 - val_loss: 0.0983 - val_accuracy: 0.9417\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0320 - accuracy: 0.9894 - val_loss: 0.0947 - val_accuracy: 0.9417\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 60s 111ms/step - loss: 0.0294 - accuracy: 0.9894 - val_loss: 0.0907 - val_accuracy: 0.9500\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0342 - accuracy: 0.9907 - val_loss: 0.0974 - val_accuracy: 0.9417\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0248 - accuracy: 0.9944 - val_loss: 0.0931 - val_accuracy: 0.9458\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0375 - accuracy: 0.9847 - val_loss: 0.0952 - val_accuracy: 0.9417\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0295 - accuracy: 0.9912 - val_loss: 0.0923 - val_accuracy: 0.9458\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0283 - accuracy: 0.9907 - val_loss: 0.0930 - val_accuracy: 0.9417\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0247 - accuracy: 0.9944 - val_loss: 0.0915 - val_accuracy: 0.9458\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 59s 110ms/step - loss: 0.0349 - accuracy: 0.9870 - val_loss: 0.0938 - val_accuracy: 0.9458\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0305 - accuracy: 0.9898 - val_loss: 0.0966 - val_accuracy: 0.9458\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0312 - accuracy: 0.9898 - val_loss: 0.0930 - val_accuracy: 0.9500\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0328 - accuracy: 0.9894 - val_loss: 0.0951 - val_accuracy: 0.9458\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0271 - accuracy: 0.9926 - val_loss: 0.0942 - val_accuracy: 0.9417\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0282 - accuracy: 0.9880 - val_loss: 0.0923 - val_accuracy: 0.9458\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0277 - accuracy: 0.9921 - val_loss: 0.0915 - val_accuracy: 0.9458\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0329 - accuracy: 0.9889 - val_loss: 0.0952 - val_accuracy: 0.9417\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0295 - accuracy: 0.9903 - val_loss: 0.0905 - val_accuracy: 0.9500\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0262 - accuracy: 0.9931 - val_loss: 0.0948 - val_accuracy: 0.9417\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0267 - accuracy: 0.9931 - val_loss: 0.0944 - val_accuracy: 0.9417\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0247 - accuracy: 0.9940 - val_loss: 0.0921 - val_accuracy: 0.9458\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0280 - accuracy: 0.9935 - val_loss: 0.0969 - val_accuracy: 0.9417\n",
      "Score for fold 6: loss of 0.0969468206167221; accuracy of 94.16666626930237%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 68s 111ms/step - loss: 0.3159 - accuracy: 0.8023 - val_loss: 0.9705 - val_accuracy: 0.6333\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.2179 - accuracy: 0.8722 - val_loss: 0.1387 - val_accuracy: 0.9208\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.2082 - accuracy: 0.8856 - val_loss: 0.1263 - val_accuracy: 0.9500\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1808 - accuracy: 0.8968 - val_loss: 0.1212 - val_accuracy: 0.9375\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1493 - accuracy: 0.9236 - val_loss: 0.1224 - val_accuracy: 0.9125\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1497 - accuracy: 0.9194 - val_loss: 0.1378 - val_accuracy: 0.8958\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1301 - accuracy: 0.9319 - val_loss: 0.0949 - val_accuracy: 0.9542\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1189 - accuracy: 0.9431 - val_loss: 0.0979 - val_accuracy: 0.9542\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1204 - accuracy: 0.9333 - val_loss: 0.0786 - val_accuracy: 0.9583\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1074 - accuracy: 0.9444 - val_loss: 0.0807 - val_accuracy: 0.9583\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1063 - accuracy: 0.9440 - val_loss: 0.0784 - val_accuracy: 0.9500\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1034 - accuracy: 0.9500 - val_loss: 0.0917 - val_accuracy: 0.9583\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0858 - accuracy: 0.9597 - val_loss: 0.1630 - val_accuracy: 0.9292\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0889 - accuracy: 0.9593 - val_loss: 0.1141 - val_accuracy: 0.9375\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0916 - accuracy: 0.9569 - val_loss: 0.0859 - val_accuracy: 0.9583\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0903 - accuracy: 0.9583 - val_loss: 0.0881 - val_accuracy: 0.9542\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0729 - accuracy: 0.9708 - val_loss: 0.0851 - val_accuracy: 0.9458\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0827 - accuracy: 0.9588 - val_loss: 0.1076 - val_accuracy: 0.9542\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0686 - accuracy: 0.9690 - val_loss: 0.0842 - val_accuracy: 0.9458\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0653 - accuracy: 0.9704 - val_loss: 0.0902 - val_accuracy: 0.9500\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 59s 110ms/step - loss: 0.0572 - accuracy: 0.9750 - val_loss: 0.0782 - val_accuracy: 0.9417\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0519 - accuracy: 0.9824 - val_loss: 0.0761 - val_accuracy: 0.9500\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0481 - accuracy: 0.9829 - val_loss: 0.0767 - val_accuracy: 0.9500\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0393 - accuracy: 0.9898 - val_loss: 0.0767 - val_accuracy: 0.9500\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0396 - accuracy: 0.9884 - val_loss: 0.0751 - val_accuracy: 0.9542\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0424 - accuracy: 0.9852 - val_loss: 0.0746 - val_accuracy: 0.9500\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0466 - accuracy: 0.9838 - val_loss: 0.0810 - val_accuracy: 0.9583\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0440 - accuracy: 0.9870 - val_loss: 0.0799 - val_accuracy: 0.9542\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0451 - accuracy: 0.9810 - val_loss: 0.0777 - val_accuracy: 0.9500\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0387 - accuracy: 0.9875 - val_loss: 0.0790 - val_accuracy: 0.9500\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0363 - accuracy: 0.9898 - val_loss: 0.0799 - val_accuracy: 0.9500\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0345 - accuracy: 0.9912 - val_loss: 0.0805 - val_accuracy: 0.9500\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0378 - accuracy: 0.9894 - val_loss: 0.0816 - val_accuracy: 0.9458\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0359 - accuracy: 0.9884 - val_loss: 0.0839 - val_accuracy: 0.9583\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 59s 108ms/step - loss: 0.0331 - accuracy: 0.9903 - val_loss: 0.0809 - val_accuracy: 0.9542\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0390 - accuracy: 0.9852 - val_loss: 0.0828 - val_accuracy: 0.9500\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0367 - accuracy: 0.9870 - val_loss: 0.0805 - val_accuracy: 0.9500\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0382 - accuracy: 0.9870 - val_loss: 0.0830 - val_accuracy: 0.9542\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0355 - accuracy: 0.9903 - val_loss: 0.0809 - val_accuracy: 0.9500\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0359 - accuracy: 0.9889 - val_loss: 0.0803 - val_accuracy: 0.9500\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0325 - accuracy: 0.9903 - val_loss: 0.0816 - val_accuracy: 0.9500\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0417 - accuracy: 0.9838 - val_loss: 0.0804 - val_accuracy: 0.9500\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0312 - accuracy: 0.9935 - val_loss: 0.0817 - val_accuracy: 0.9458\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.0317 - accuracy: 0.9921 - val_loss: 0.0824 - val_accuracy: 0.9500\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0390 - accuracy: 0.9856 - val_loss: 0.0812 - val_accuracy: 0.9500\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.0296 - accuracy: 0.9912 - val_loss: 0.0837 - val_accuracy: 0.9500\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0351 - accuracy: 0.9866 - val_loss: 0.0821 - val_accuracy: 0.9458\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0319 - accuracy: 0.9912 - val_loss: 0.0811 - val_accuracy: 0.9500\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 64s 119ms/step - loss: 0.0311 - accuracy: 0.9917 - val_loss: 0.0826 - val_accuracy: 0.9458\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 61s 113ms/step - loss: 0.0384 - accuracy: 0.9880 - val_loss: 0.0827 - val_accuracy: 0.9458\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0298 - accuracy: 0.9921 - val_loss: 0.0818 - val_accuracy: 0.9458\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0300 - accuracy: 0.9921 - val_loss: 0.0802 - val_accuracy: 0.9500\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0374 - accuracy: 0.9870 - val_loss: 0.0809 - val_accuracy: 0.9500\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0276 - accuracy: 0.9949 - val_loss: 0.0810 - val_accuracy: 0.9500\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.0325 - accuracy: 0.9894 - val_loss: 0.0821 - val_accuracy: 0.9500\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0305 - accuracy: 0.9931 - val_loss: 0.0812 - val_accuracy: 0.9458\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0390 - accuracy: 0.9838 - val_loss: 0.0825 - val_accuracy: 0.9500\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0330 - accuracy: 0.9889 - val_loss: 0.0824 - val_accuracy: 0.9458\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0337 - accuracy: 0.9880 - val_loss: 0.0811 - val_accuracy: 0.9500\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0332 - accuracy: 0.9912 - val_loss: 0.0819 - val_accuracy: 0.9458\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0306 - accuracy: 0.9898 - val_loss: 0.0823 - val_accuracy: 0.9500\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0281 - accuracy: 0.9935 - val_loss: 0.0807 - val_accuracy: 0.9500\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0354 - accuracy: 0.9907 - val_loss: 0.0824 - val_accuracy: 0.9500\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0309 - accuracy: 0.9926 - val_loss: 0.0817 - val_accuracy: 0.9500\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0340 - accuracy: 0.9894 - val_loss: 0.0820 - val_accuracy: 0.9500\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0382 - accuracy: 0.9880 - val_loss: 0.0803 - val_accuracy: 0.9500\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0317 - accuracy: 0.9875 - val_loss: 0.0803 - val_accuracy: 0.9500\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0324 - accuracy: 0.9907 - val_loss: 0.0804 - val_accuracy: 0.9500\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0334 - accuracy: 0.9917 - val_loss: 0.0820 - val_accuracy: 0.9500\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0297 - accuracy: 0.9912 - val_loss: 0.0811 - val_accuracy: 0.9500\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0362 - accuracy: 0.9866 - val_loss: 0.0816 - val_accuracy: 0.9500\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0322 - accuracy: 0.9907 - val_loss: 0.0815 - val_accuracy: 0.9500\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0316 - accuracy: 0.9926 - val_loss: 0.0814 - val_accuracy: 0.9500\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 61s 113ms/step - loss: 0.0344 - accuracy: 0.9912 - val_loss: 0.0810 - val_accuracy: 0.9500\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 61s 113ms/step - loss: 0.0311 - accuracy: 0.9898 - val_loss: 0.0816 - val_accuracy: 0.9458\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 61s 113ms/step - loss: 0.0311 - accuracy: 0.9898 - val_loss: 0.0808 - val_accuracy: 0.9500\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0339 - accuracy: 0.9875 - val_loss: 0.0813 - val_accuracy: 0.9500\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0340 - accuracy: 0.9880 - val_loss: 0.0821 - val_accuracy: 0.9458\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0330 - accuracy: 0.9889 - val_loss: 0.0823 - val_accuracy: 0.9500\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0340 - accuracy: 0.9889 - val_loss: 0.0821 - val_accuracy: 0.9500\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 64s 118ms/step - loss: 0.0326 - accuracy: 0.9907 - val_loss: 0.0805 - val_accuracy: 0.9500\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0372 - accuracy: 0.9870 - val_loss: 0.0819 - val_accuracy: 0.9500\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0306 - accuracy: 0.9917 - val_loss: 0.0809 - val_accuracy: 0.9458\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0306 - accuracy: 0.9907 - val_loss: 0.0806 - val_accuracy: 0.9500\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0342 - accuracy: 0.9907 - val_loss: 0.0840 - val_accuracy: 0.9542\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0325 - accuracy: 0.9884 - val_loss: 0.0808 - val_accuracy: 0.9500\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 59s 109ms/step - loss: 0.0304 - accuracy: 0.9898 - val_loss: 0.0812 - val_accuracy: 0.9500\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0298 - accuracy: 0.9935 - val_loss: 0.0812 - val_accuracy: 0.9500\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0356 - accuracy: 0.9861 - val_loss: 0.0812 - val_accuracy: 0.9500\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0330 - accuracy: 0.9894 - val_loss: 0.0827 - val_accuracy: 0.9500\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0363 - accuracy: 0.9880 - val_loss: 0.0804 - val_accuracy: 0.9500\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0307 - accuracy: 0.9912 - val_loss: 0.0823 - val_accuracy: 0.9458\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0301 - accuracy: 0.9926 - val_loss: 0.0814 - val_accuracy: 0.9458\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0386 - accuracy: 0.9838 - val_loss: 0.0808 - val_accuracy: 0.9500\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0291 - accuracy: 0.9940 - val_loss: 0.0832 - val_accuracy: 0.9458\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0300 - accuracy: 0.9903 - val_loss: 0.0817 - val_accuracy: 0.9500\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0294 - accuracy: 0.9907 - val_loss: 0.0798 - val_accuracy: 0.9500\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 57s 106ms/step - loss: 0.0309 - accuracy: 0.9903 - val_loss: 0.0815 - val_accuracy: 0.9500\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.0317 - accuracy: 0.9912 - val_loss: 0.0817 - val_accuracy: 0.9500\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.0344 - accuracy: 0.9889 - val_loss: 0.0810 - val_accuracy: 0.9500\n",
      "Score for fold 7: loss of 0.08098980784416199; accuracy of 94.9999988079071%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 68s 110ms/step - loss: 0.3266 - accuracy: 0.7949 - val_loss: 0.6307 - val_accuracy: 0.5500\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.2277 - accuracy: 0.8634 - val_loss: 0.2363 - val_accuracy: 0.8000\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1922 - accuracy: 0.8949 - val_loss: 0.1240 - val_accuracy: 0.9333\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1721 - accuracy: 0.9019 - val_loss: 0.1259 - val_accuracy: 0.9375\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1473 - accuracy: 0.9190 - val_loss: 0.2637 - val_accuracy: 0.7625\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1491 - accuracy: 0.9190 - val_loss: 0.1050 - val_accuracy: 0.9375\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1399 - accuracy: 0.9213 - val_loss: 0.1103 - val_accuracy: 0.9458\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1276 - accuracy: 0.9296 - val_loss: 0.0818 - val_accuracy: 0.9625\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1251 - accuracy: 0.9380 - val_loss: 0.1444 - val_accuracy: 0.8958\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 58s 108ms/step - loss: 0.1155 - accuracy: 0.9421 - val_loss: 0.1564 - val_accuracy: 0.8667\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 58s 107ms/step - loss: 0.1061 - accuracy: 0.9412 - val_loss: 0.0833 - val_accuracy: 0.9625\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 64s 118ms/step - loss: 0.1075 - accuracy: 0.9431 - val_loss: 0.0889 - val_accuracy: 0.9583\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.1048 - accuracy: 0.9509 - val_loss: 0.1126 - val_accuracy: 0.9292\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0955 - accuracy: 0.9477 - val_loss: 0.0826 - val_accuracy: 0.9542\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0906 - accuracy: 0.9597 - val_loss: 0.0981 - val_accuracy: 0.9542\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0802 - accuracy: 0.9597 - val_loss: 0.0928 - val_accuracy: 0.9500\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0783 - accuracy: 0.9616 - val_loss: 0.1329 - val_accuracy: 0.9083\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0754 - accuracy: 0.9616 - val_loss: 0.0890 - val_accuracy: 0.9625\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0767 - accuracy: 0.9625 - val_loss: 0.1183 - val_accuracy: 0.9208\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0629 - accuracy: 0.9731 - val_loss: 0.1046 - val_accuracy: 0.9292\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0565 - accuracy: 0.9792 - val_loss: 0.0862 - val_accuracy: 0.9500\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0507 - accuracy: 0.9819 - val_loss: 0.0840 - val_accuracy: 0.9583\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0474 - accuracy: 0.9806 - val_loss: 0.0826 - val_accuracy: 0.9625\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0471 - accuracy: 0.9815 - val_loss: 0.0811 - val_accuracy: 0.9583\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0435 - accuracy: 0.9843 - val_loss: 0.0789 - val_accuracy: 0.9625\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0418 - accuracy: 0.9852 - val_loss: 0.0773 - val_accuracy: 0.9667\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0440 - accuracy: 0.9843 - val_loss: 0.0792 - val_accuracy: 0.9583\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0472 - accuracy: 0.9819 - val_loss: 0.0776 - val_accuracy: 0.9667\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0414 - accuracy: 0.9852 - val_loss: 0.0778 - val_accuracy: 0.9625\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 61s 113ms/step - loss: 0.0400 - accuracy: 0.9870 - val_loss: 0.0807 - val_accuracy: 0.9583\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0370 - accuracy: 0.9875 - val_loss: 0.0810 - val_accuracy: 0.9625\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0321 - accuracy: 0.9912 - val_loss: 0.0801 - val_accuracy: 0.9625\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0409 - accuracy: 0.9866 - val_loss: 0.0791 - val_accuracy: 0.9625\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0388 - accuracy: 0.9861 - val_loss: 0.0782 - val_accuracy: 0.9625\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0346 - accuracy: 0.9880 - val_loss: 0.0802 - val_accuracy: 0.9542\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0440 - accuracy: 0.9829 - val_loss: 0.0850 - val_accuracy: 0.9625\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0378 - accuracy: 0.9847 - val_loss: 0.0820 - val_accuracy: 0.9625\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0428 - accuracy: 0.9861 - val_loss: 0.0748 - val_accuracy: 0.9708\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0336 - accuracy: 0.9903 - val_loss: 0.0823 - val_accuracy: 0.9583\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 64s 119ms/step - loss: 0.0354 - accuracy: 0.9894 - val_loss: 0.0777 - val_accuracy: 0.9625\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0345 - accuracy: 0.9875 - val_loss: 0.0778 - val_accuracy: 0.9625\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0320 - accuracy: 0.9903 - val_loss: 0.0768 - val_accuracy: 0.9625\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0338 - accuracy: 0.9917 - val_loss: 0.0793 - val_accuracy: 0.9625\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0354 - accuracy: 0.9889 - val_loss: 0.0795 - val_accuracy: 0.9625\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0372 - accuracy: 0.9884 - val_loss: 0.0787 - val_accuracy: 0.9625\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0354 - accuracy: 0.9903 - val_loss: 0.0796 - val_accuracy: 0.9583\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0335 - accuracy: 0.9894 - val_loss: 0.0808 - val_accuracy: 0.9583\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0360 - accuracy: 0.9894 - val_loss: 0.0795 - val_accuracy: 0.9583\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0340 - accuracy: 0.9884 - val_loss: 0.0794 - val_accuracy: 0.9625\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0377 - accuracy: 0.9856 - val_loss: 0.0783 - val_accuracy: 0.9625\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0306 - accuracy: 0.9912 - val_loss: 0.0786 - val_accuracy: 0.9625\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0324 - accuracy: 0.9907 - val_loss: 0.0811 - val_accuracy: 0.9583\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0359 - accuracy: 0.9884 - val_loss: 0.0801 - val_accuracy: 0.9583\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0344 - accuracy: 0.9903 - val_loss: 0.0798 - val_accuracy: 0.9625\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0349 - accuracy: 0.9889 - val_loss: 0.0789 - val_accuracy: 0.9625\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0319 - accuracy: 0.9884 - val_loss: 0.0765 - val_accuracy: 0.9667\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0393 - accuracy: 0.9875 - val_loss: 0.0795 - val_accuracy: 0.9625\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0304 - accuracy: 0.9917 - val_loss: 0.0784 - val_accuracy: 0.9667\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0339 - accuracy: 0.9866 - val_loss: 0.0783 - val_accuracy: 0.9625\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0394 - accuracy: 0.9843 - val_loss: 0.0788 - val_accuracy: 0.9625\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 65s 120ms/step - loss: 0.0388 - accuracy: 0.9856 - val_loss: 0.0797 - val_accuracy: 0.9625\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0342 - accuracy: 0.9875 - val_loss: 0.0798 - val_accuracy: 0.9625\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0319 - accuracy: 0.9917 - val_loss: 0.0788 - val_accuracy: 0.9625\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0258 - accuracy: 0.9935 - val_loss: 0.0783 - val_accuracy: 0.9625\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0325 - accuracy: 0.9903 - val_loss: 0.0779 - val_accuracy: 0.9667\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0378 - accuracy: 0.9856 - val_loss: 0.0795 - val_accuracy: 0.9625\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0330 - accuracy: 0.9898 - val_loss: 0.0792 - val_accuracy: 0.9583\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0334 - accuracy: 0.9889 - val_loss: 0.0785 - val_accuracy: 0.9625\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0327 - accuracy: 0.9889 - val_loss: 0.0783 - val_accuracy: 0.9625\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0317 - accuracy: 0.9889 - val_loss: 0.0779 - val_accuracy: 0.9625\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0351 - accuracy: 0.9894 - val_loss: 0.0795 - val_accuracy: 0.9667\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0355 - accuracy: 0.9889 - val_loss: 0.0822 - val_accuracy: 0.9583\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0379 - accuracy: 0.9843 - val_loss: 0.0793 - val_accuracy: 0.9625\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0369 - accuracy: 0.9870 - val_loss: 0.0797 - val_accuracy: 0.9625\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0395 - accuracy: 0.9894 - val_loss: 0.0788 - val_accuracy: 0.9625\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0331 - accuracy: 0.9884 - val_loss: 0.0801 - val_accuracy: 0.9625\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0361 - accuracy: 0.9866 - val_loss: 0.0780 - val_accuracy: 0.9625\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0336 - accuracy: 0.9894 - val_loss: 0.0804 - val_accuracy: 0.9625\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0369 - accuracy: 0.9875 - val_loss: 0.0787 - val_accuracy: 0.9667\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0341 - accuracy: 0.9894 - val_loss: 0.0792 - val_accuracy: 0.9625\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0299 - accuracy: 0.9912 - val_loss: 0.0779 - val_accuracy: 0.9625\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0302 - accuracy: 0.9944 - val_loss: 0.0785 - val_accuracy: 0.9625\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0349 - accuracy: 0.9894 - val_loss: 0.0800 - val_accuracy: 0.9625\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0306 - accuracy: 0.9926 - val_loss: 0.0812 - val_accuracy: 0.9583\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0349 - accuracy: 0.9889 - val_loss: 0.0781 - val_accuracy: 0.9625\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0344 - accuracy: 0.9866 - val_loss: 0.0792 - val_accuracy: 0.9625\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0338 - accuracy: 0.9898 - val_loss: 0.0789 - val_accuracy: 0.9625\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0309 - accuracy: 0.9926 - val_loss: 0.0811 - val_accuracy: 0.9583\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0359 - accuracy: 0.9880 - val_loss: 0.0813 - val_accuracy: 0.9583\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0350 - accuracy: 0.9889 - val_loss: 0.0802 - val_accuracy: 0.9625\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0371 - accuracy: 0.9884 - val_loss: 0.0792 - val_accuracy: 0.9625\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0362 - accuracy: 0.9870 - val_loss: 0.0814 - val_accuracy: 0.9583\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0328 - accuracy: 0.9903 - val_loss: 0.0794 - val_accuracy: 0.9625\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0382 - accuracy: 0.9894 - val_loss: 0.0786 - val_accuracy: 0.9667\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0359 - accuracy: 0.9875 - val_loss: 0.0810 - val_accuracy: 0.9583\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0331 - accuracy: 0.9894 - val_loss: 0.0810 - val_accuracy: 0.9583\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 61s 114ms/step - loss: 0.0380 - accuracy: 0.9875 - val_loss: 0.0807 - val_accuracy: 0.9583\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0326 - accuracy: 0.9912 - val_loss: 0.0820 - val_accuracy: 0.9583\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0341 - accuracy: 0.9907 - val_loss: 0.0812 - val_accuracy: 0.9583\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0324 - accuracy: 0.9912 - val_loss: 0.0802 - val_accuracy: 0.9583\n",
      "Score for fold 8: loss of 0.08022759109735489; accuracy of 95.83333134651184%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 74s 120ms/step - loss: 0.3082 - accuracy: 0.7995 - val_loss: 0.7028 - val_accuracy: 0.6625\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.2291 - accuracy: 0.8634 - val_loss: 0.1549 - val_accuracy: 0.9125\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.1889 - accuracy: 0.9046 - val_loss: 0.1942 - val_accuracy: 0.8958\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.1681 - accuracy: 0.9106 - val_loss: 0.1296 - val_accuracy: 0.9375\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.1510 - accuracy: 0.9222 - val_loss: 0.1171 - val_accuracy: 0.9417\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.1472 - accuracy: 0.9231 - val_loss: 0.1937 - val_accuracy: 0.8958\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.1299 - accuracy: 0.9296 - val_loss: 0.1158 - val_accuracy: 0.9333\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.1302 - accuracy: 0.9301 - val_loss: 0.1089 - val_accuracy: 0.9458\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.1149 - accuracy: 0.9426 - val_loss: 0.1069 - val_accuracy: 0.9458\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.1121 - accuracy: 0.9458 - val_loss: 0.1177 - val_accuracy: 0.9417\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.1054 - accuracy: 0.9454 - val_loss: 0.1879 - val_accuracy: 0.9292\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0981 - accuracy: 0.9463 - val_loss: 0.1064 - val_accuracy: 0.9292\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0895 - accuracy: 0.9546 - val_loss: 0.1524 - val_accuracy: 0.8917\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0878 - accuracy: 0.9556 - val_loss: 0.1617 - val_accuracy: 0.9250\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0846 - accuracy: 0.9542 - val_loss: 0.1507 - val_accuracy: 0.9375\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0728 - accuracy: 0.9653 - val_loss: 0.1121 - val_accuracy: 0.9375\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0690 - accuracy: 0.9685 - val_loss: 0.1109 - val_accuracy: 0.9417\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 64s 119ms/step - loss: 0.0734 - accuracy: 0.9662 - val_loss: 0.1173 - val_accuracy: 0.9333\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0645 - accuracy: 0.9731 - val_loss: 0.1290 - val_accuracy: 0.9292\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0655 - accuracy: 0.9671 - val_loss: 0.2025 - val_accuracy: 0.9292\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0513 - accuracy: 0.9778 - val_loss: 0.1449 - val_accuracy: 0.9417\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0440 - accuracy: 0.9819 - val_loss: 0.1576 - val_accuracy: 0.9417\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0392 - accuracy: 0.9875 - val_loss: 0.1539 - val_accuracy: 0.9417\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.0398 - accuracy: 0.9838 - val_loss: 0.1379 - val_accuracy: 0.9333\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0457 - accuracy: 0.9829 - val_loss: 0.1623 - val_accuracy: 0.9417\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0403 - accuracy: 0.9852 - val_loss: 0.1522 - val_accuracy: 0.9417\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0343 - accuracy: 0.9852 - val_loss: 0.1505 - val_accuracy: 0.9417\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.0353 - accuracy: 0.9898 - val_loss: 0.1612 - val_accuracy: 0.9417\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0385 - accuracy: 0.9861 - val_loss: 0.1599 - val_accuracy: 0.9417\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0329 - accuracy: 0.9907 - val_loss: 0.1556 - val_accuracy: 0.9417\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.0368 - accuracy: 0.9880 - val_loss: 0.1478 - val_accuracy: 0.9375\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.0367 - accuracy: 0.9898 - val_loss: 0.1575 - val_accuracy: 0.9417\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0301 - accuracy: 0.9940 - val_loss: 0.1682 - val_accuracy: 0.9417\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0341 - accuracy: 0.9866 - val_loss: 0.1684 - val_accuracy: 0.9417\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0340 - accuracy: 0.9917 - val_loss: 0.1675 - val_accuracy: 0.9375\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0349 - accuracy: 0.9903 - val_loss: 0.1628 - val_accuracy: 0.9417\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0324 - accuracy: 0.9898 - val_loss: 0.1581 - val_accuracy: 0.9417\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0294 - accuracy: 0.9926 - val_loss: 0.1621 - val_accuracy: 0.9417\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0298 - accuracy: 0.9912 - val_loss: 0.1536 - val_accuracy: 0.9417\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0340 - accuracy: 0.9903 - val_loss: 0.1571 - val_accuracy: 0.9375\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0244 - accuracy: 0.9949 - val_loss: 0.1614 - val_accuracy: 0.9417\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0269 - accuracy: 0.9917 - val_loss: 0.1571 - val_accuracy: 0.9375\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0292 - accuracy: 0.9935 - val_loss: 0.1624 - val_accuracy: 0.9417\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0323 - accuracy: 0.9898 - val_loss: 0.1577 - val_accuracy: 0.9375\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0332 - accuracy: 0.9894 - val_loss: 0.1576 - val_accuracy: 0.9375\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0246 - accuracy: 0.9958 - val_loss: 0.1559 - val_accuracy: 0.9375\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0292 - accuracy: 0.9912 - val_loss: 0.1612 - val_accuracy: 0.9417\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0303 - accuracy: 0.9931 - val_loss: 0.1659 - val_accuracy: 0.9417\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0303 - accuracy: 0.9917 - val_loss: 0.1666 - val_accuracy: 0.9417\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0303 - accuracy: 0.9903 - val_loss: 0.1619 - val_accuracy: 0.9417\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0335 - accuracy: 0.9870 - val_loss: 0.1647 - val_accuracy: 0.9417\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0258 - accuracy: 0.9958 - val_loss: 0.1630 - val_accuracy: 0.9417\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0299 - accuracy: 0.9944 - val_loss: 0.1596 - val_accuracy: 0.9417\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0316 - accuracy: 0.9912 - val_loss: 0.1616 - val_accuracy: 0.9417\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0275 - accuracy: 0.9907 - val_loss: 0.1620 - val_accuracy: 0.9417\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 64s 118ms/step - loss: 0.0350 - accuracy: 0.9870 - val_loss: 0.1626 - val_accuracy: 0.9417\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0297 - accuracy: 0.9917 - val_loss: 0.1601 - val_accuracy: 0.9417\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0302 - accuracy: 0.9912 - val_loss: 0.1641 - val_accuracy: 0.9417\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0299 - accuracy: 0.9912 - val_loss: 0.1612 - val_accuracy: 0.9417\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0402 - accuracy: 0.9866 - val_loss: 0.1617 - val_accuracy: 0.9417\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0310 - accuracy: 0.9894 - val_loss: 0.1588 - val_accuracy: 0.9417\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0300 - accuracy: 0.9884 - val_loss: 0.1591 - val_accuracy: 0.9417\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0287 - accuracy: 0.9931 - val_loss: 0.1629 - val_accuracy: 0.9417\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0342 - accuracy: 0.9889 - val_loss: 0.1618 - val_accuracy: 0.9417\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0302 - accuracy: 0.9907 - val_loss: 0.1583 - val_accuracy: 0.9417\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0348 - accuracy: 0.9898 - val_loss: 0.1602 - val_accuracy: 0.9417\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0298 - accuracy: 0.9912 - val_loss: 0.1615 - val_accuracy: 0.9417\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0325 - accuracy: 0.9898 - val_loss: 0.1617 - val_accuracy: 0.9417\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0311 - accuracy: 0.9898 - val_loss: 0.1621 - val_accuracy: 0.9417\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.0395 - accuracy: 0.9852 - val_loss: 0.1632 - val_accuracy: 0.9417\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0336 - accuracy: 0.9903 - val_loss: 0.1615 - val_accuracy: 0.9417\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0311 - accuracy: 0.9912 - val_loss: 0.1595 - val_accuracy: 0.9417\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0313 - accuracy: 0.9898 - val_loss: 0.1612 - val_accuracy: 0.9417\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0316 - accuracy: 0.9907 - val_loss: 0.1591 - val_accuracy: 0.9417\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0302 - accuracy: 0.9894 - val_loss: 0.1588 - val_accuracy: 0.9417\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0294 - accuracy: 0.9907 - val_loss: 0.1600 - val_accuracy: 0.9417\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0323 - accuracy: 0.9894 - val_loss: 0.1659 - val_accuracy: 0.9417\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0292 - accuracy: 0.9917 - val_loss: 0.1611 - val_accuracy: 0.9417\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0297 - accuracy: 0.9926 - val_loss: 0.1636 - val_accuracy: 0.9417\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0303 - accuracy: 0.9898 - val_loss: 0.1602 - val_accuracy: 0.9417\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0264 - accuracy: 0.9935 - val_loss: 0.1628 - val_accuracy: 0.9417\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0308 - accuracy: 0.9898 - val_loss: 0.1637 - val_accuracy: 0.9417\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0322 - accuracy: 0.9907 - val_loss: 0.1610 - val_accuracy: 0.9417\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0340 - accuracy: 0.9880 - val_loss: 0.1624 - val_accuracy: 0.9417\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0298 - accuracy: 0.9898 - val_loss: 0.1627 - val_accuracy: 0.9417\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0296 - accuracy: 0.9912 - val_loss: 0.1611 - val_accuracy: 0.9417\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0284 - accuracy: 0.9912 - val_loss: 0.1619 - val_accuracy: 0.9417\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0322 - accuracy: 0.9866 - val_loss: 0.1655 - val_accuracy: 0.9417\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0319 - accuracy: 0.9921 - val_loss: 0.1595 - val_accuracy: 0.9417\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0273 - accuracy: 0.9921 - val_loss: 0.1623 - val_accuracy: 0.9417\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0334 - accuracy: 0.9884 - val_loss: 0.1629 - val_accuracy: 0.9417\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0288 - accuracy: 0.9907 - val_loss: 0.1624 - val_accuracy: 0.9417\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0286 - accuracy: 0.9949 - val_loss: 0.1608 - val_accuracy: 0.9417\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0334 - accuracy: 0.9870 - val_loss: 0.1649 - val_accuracy: 0.9417\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0301 - accuracy: 0.9894 - val_loss: 0.1652 - val_accuracy: 0.9417\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0284 - accuracy: 0.9926 - val_loss: 0.1643 - val_accuracy: 0.9417\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0277 - accuracy: 0.9935 - val_loss: 0.1635 - val_accuracy: 0.9417\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0272 - accuracy: 0.9907 - val_loss: 0.1651 - val_accuracy: 0.9417\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0275 - accuracy: 0.9944 - val_loss: 0.1672 - val_accuracy: 0.9417\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0318 - accuracy: 0.9935 - val_loss: 0.1677 - val_accuracy: 0.9417\n",
      "Score for fold 9: loss of 0.1676643043756485; accuracy of 94.16666626930237%\n",
      "(2160,) (240,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 73s 119ms/step - loss: 0.3165 - accuracy: 0.7991 - val_loss: 0.3517 - val_accuracy: 0.7333\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.2186 - accuracy: 0.8806 - val_loss: 0.1618 - val_accuracy: 0.9083\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.1999 - accuracy: 0.8884 - val_loss: 0.1198 - val_accuracy: 0.9167\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.1639 - accuracy: 0.9125 - val_loss: 0.1159 - val_accuracy: 0.9458\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.1616 - accuracy: 0.9111 - val_loss: 0.1111 - val_accuracy: 0.9417\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.1439 - accuracy: 0.9236 - val_loss: 0.1230 - val_accuracy: 0.9292\n",
      "Epoch 7/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.1307 - accuracy: 0.9287 - val_loss: 0.1053 - val_accuracy: 0.9458\n",
      "Epoch 8/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.1305 - accuracy: 0.9296 - val_loss: 0.0881 - val_accuracy: 0.9542\n",
      "Epoch 9/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.1219 - accuracy: 0.9329 - val_loss: 0.0967 - val_accuracy: 0.9458\n",
      "Epoch 10/100\n",
      "540/540 [==============================] - 64s 118ms/step - loss: 0.1141 - accuracy: 0.9417 - val_loss: 0.1066 - val_accuracy: 0.9417\n",
      "Epoch 11/100\n",
      "540/540 [==============================] - 64s 118ms/step - loss: 0.1127 - accuracy: 0.9412 - val_loss: 0.0804 - val_accuracy: 0.9583\n",
      "Epoch 12/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0897 - accuracy: 0.9509 - val_loss: 0.0825 - val_accuracy: 0.9625\n",
      "Epoch 13/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0924 - accuracy: 0.9565 - val_loss: 0.0903 - val_accuracy: 0.9500\n",
      "Epoch 14/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0900 - accuracy: 0.9574 - val_loss: 0.0736 - val_accuracy: 0.9667\n",
      "Epoch 15/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0898 - accuracy: 0.9523 - val_loss: 0.1067 - val_accuracy: 0.9458\n",
      "Epoch 16/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0816 - accuracy: 0.9593 - val_loss: 0.1027 - val_accuracy: 0.9458\n",
      "Epoch 17/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0799 - accuracy: 0.9597 - val_loss: 0.0783 - val_accuracy: 0.9542\n",
      "Epoch 18/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0662 - accuracy: 0.9694 - val_loss: 0.0921 - val_accuracy: 0.9500\n",
      "Epoch 19/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0724 - accuracy: 0.9676 - val_loss: 0.0820 - val_accuracy: 0.9542\n",
      "Epoch 20/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0612 - accuracy: 0.9745 - val_loss: 0.0887 - val_accuracy: 0.9542\n",
      "Epoch 21/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0468 - accuracy: 0.9829 - val_loss: 0.0864 - val_accuracy: 0.9667\n",
      "Epoch 22/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0395 - accuracy: 0.9838 - val_loss: 0.0829 - val_accuracy: 0.9583\n",
      "Epoch 23/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0470 - accuracy: 0.9819 - val_loss: 0.0805 - val_accuracy: 0.9542\n",
      "Epoch 24/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0440 - accuracy: 0.9838 - val_loss: 0.0815 - val_accuracy: 0.9625\n",
      "Epoch 25/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0376 - accuracy: 0.9894 - val_loss: 0.0875 - val_accuracy: 0.9667\n",
      "Epoch 26/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0398 - accuracy: 0.9843 - val_loss: 0.0898 - val_accuracy: 0.9667\n",
      "Epoch 27/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0418 - accuracy: 0.9824 - val_loss: 0.0878 - val_accuracy: 0.9667\n",
      "Epoch 28/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0405 - accuracy: 0.9870 - val_loss: 0.0879 - val_accuracy: 0.9667\n",
      "Epoch 29/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0368 - accuracy: 0.9880 - val_loss: 0.0842 - val_accuracy: 0.9667\n",
      "Epoch 30/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0408 - accuracy: 0.9852 - val_loss: 0.0848 - val_accuracy: 0.9625\n",
      "Epoch 31/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0307 - accuracy: 0.9926 - val_loss: 0.0842 - val_accuracy: 0.9625\n",
      "Epoch 32/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0336 - accuracy: 0.9880 - val_loss: 0.0867 - val_accuracy: 0.9667\n",
      "Epoch 33/100\n",
      "540/540 [==============================] - 65s 120ms/step - loss: 0.0382 - accuracy: 0.9852 - val_loss: 0.0859 - val_accuracy: 0.9625\n",
      "Epoch 34/100\n",
      "540/540 [==============================] - 63s 118ms/step - loss: 0.0358 - accuracy: 0.9861 - val_loss: 0.0888 - val_accuracy: 0.9667\n",
      "Epoch 35/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0356 - accuracy: 0.9870 - val_loss: 0.0827 - val_accuracy: 0.9583\n",
      "Epoch 36/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0346 - accuracy: 0.9875 - val_loss: 0.0879 - val_accuracy: 0.9708\n",
      "Epoch 37/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0346 - accuracy: 0.9875 - val_loss: 0.0861 - val_accuracy: 0.9667\n",
      "Epoch 38/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0377 - accuracy: 0.9866 - val_loss: 0.0834 - val_accuracy: 0.9667\n",
      "Epoch 39/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.0335 - accuracy: 0.9898 - val_loss: 0.0831 - val_accuracy: 0.9625\n",
      "Epoch 40/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0343 - accuracy: 0.9889 - val_loss: 0.0862 - val_accuracy: 0.9667\n",
      "Epoch 41/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0350 - accuracy: 0.9884 - val_loss: 0.0869 - val_accuracy: 0.9667\n",
      "Epoch 42/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0340 - accuracy: 0.9880 - val_loss: 0.0874 - val_accuracy: 0.9667\n",
      "Epoch 43/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0300 - accuracy: 0.9889 - val_loss: 0.0847 - val_accuracy: 0.9625\n",
      "Epoch 44/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0359 - accuracy: 0.9889 - val_loss: 0.0840 - val_accuracy: 0.9667\n",
      "Epoch 45/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0292 - accuracy: 0.9903 - val_loss: 0.0829 - val_accuracy: 0.9625\n",
      "Epoch 46/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0292 - accuracy: 0.9912 - val_loss: 0.0848 - val_accuracy: 0.9667\n",
      "Epoch 47/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0312 - accuracy: 0.9898 - val_loss: 0.0851 - val_accuracy: 0.9625\n",
      "Epoch 48/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.0289 - accuracy: 0.9931 - val_loss: 0.0832 - val_accuracy: 0.9625\n",
      "Epoch 49/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0367 - accuracy: 0.9884 - val_loss: 0.0866 - val_accuracy: 0.9667\n",
      "Epoch 50/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0302 - accuracy: 0.9935 - val_loss: 0.0852 - val_accuracy: 0.9667\n",
      "Epoch 51/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0270 - accuracy: 0.9912 - val_loss: 0.0848 - val_accuracy: 0.9667\n",
      "Epoch 52/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0326 - accuracy: 0.9889 - val_loss: 0.0853 - val_accuracy: 0.9625\n",
      "Epoch 53/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0275 - accuracy: 0.9935 - val_loss: 0.0850 - val_accuracy: 0.9625\n",
      "Epoch 54/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0306 - accuracy: 0.9907 - val_loss: 0.0856 - val_accuracy: 0.9667\n",
      "Epoch 55/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0353 - accuracy: 0.9889 - val_loss: 0.0853 - val_accuracy: 0.9667\n",
      "Epoch 56/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0281 - accuracy: 0.9903 - val_loss: 0.0853 - val_accuracy: 0.9667\n",
      "Epoch 57/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0323 - accuracy: 0.9898 - val_loss: 0.0844 - val_accuracy: 0.9667\n",
      "Epoch 58/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0273 - accuracy: 0.9949 - val_loss: 0.0853 - val_accuracy: 0.9667\n",
      "Epoch 59/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0300 - accuracy: 0.9926 - val_loss: 0.0855 - val_accuracy: 0.9667\n",
      "Epoch 60/100\n",
      "540/540 [==============================] - 62s 116ms/step - loss: 0.0363 - accuracy: 0.9898 - val_loss: 0.0844 - val_accuracy: 0.9667\n",
      "Epoch 61/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0342 - accuracy: 0.9912 - val_loss: 0.0854 - val_accuracy: 0.9667\n",
      "Epoch 62/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0349 - accuracy: 0.9898 - val_loss: 0.0839 - val_accuracy: 0.9667\n",
      "Epoch 63/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0377 - accuracy: 0.9884 - val_loss: 0.0848 - val_accuracy: 0.9625\n",
      "Epoch 64/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0358 - accuracy: 0.9875 - val_loss: 0.0852 - val_accuracy: 0.9625\n",
      "Epoch 65/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0363 - accuracy: 0.9907 - val_loss: 0.0860 - val_accuracy: 0.9667\n",
      "Epoch 66/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0354 - accuracy: 0.9866 - val_loss: 0.0844 - val_accuracy: 0.9625\n",
      "Epoch 67/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0349 - accuracy: 0.9884 - val_loss: 0.0862 - val_accuracy: 0.9625\n",
      "Epoch 68/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0288 - accuracy: 0.9921 - val_loss: 0.0847 - val_accuracy: 0.9625\n",
      "Epoch 69/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0315 - accuracy: 0.9894 - val_loss: 0.0855 - val_accuracy: 0.9667\n",
      "Epoch 70/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0273 - accuracy: 0.9935 - val_loss: 0.0853 - val_accuracy: 0.9667\n",
      "Epoch 71/100\n",
      "540/540 [==============================] - 63s 117ms/step - loss: 0.0278 - accuracy: 0.9944 - val_loss: 0.0869 - val_accuracy: 0.9667\n",
      "Epoch 72/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0377 - accuracy: 0.9852 - val_loss: 0.0852 - val_accuracy: 0.9625\n",
      "Epoch 73/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0280 - accuracy: 0.9940 - val_loss: 0.0854 - val_accuracy: 0.9667\n",
      "Epoch 74/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0239 - accuracy: 0.9944 - val_loss: 0.0860 - val_accuracy: 0.9667\n",
      "Epoch 75/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0332 - accuracy: 0.9884 - val_loss: 0.0872 - val_accuracy: 0.9667\n",
      "Epoch 76/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0313 - accuracy: 0.9907 - val_loss: 0.0876 - val_accuracy: 0.9667\n",
      "Epoch 77/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0292 - accuracy: 0.9903 - val_loss: 0.0869 - val_accuracy: 0.9667\n",
      "Epoch 78/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0345 - accuracy: 0.9889 - val_loss: 0.0862 - val_accuracy: 0.9667\n",
      "Epoch 79/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0258 - accuracy: 0.9944 - val_loss: 0.0861 - val_accuracy: 0.9667\n",
      "Epoch 80/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0316 - accuracy: 0.9898 - val_loss: 0.0851 - val_accuracy: 0.9667\n",
      "Epoch 81/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0322 - accuracy: 0.9912 - val_loss: 0.0860 - val_accuracy: 0.9667\n",
      "Epoch 82/100\n",
      "540/540 [==============================] - 62s 114ms/step - loss: 0.0300 - accuracy: 0.9921 - val_loss: 0.0856 - val_accuracy: 0.9625\n",
      "Epoch 83/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0261 - accuracy: 0.9926 - val_loss: 0.0858 - val_accuracy: 0.9625\n",
      "Epoch 84/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0295 - accuracy: 0.9917 - val_loss: 0.0867 - val_accuracy: 0.9667\n",
      "Epoch 85/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0295 - accuracy: 0.9912 - val_loss: 0.0851 - val_accuracy: 0.9667\n",
      "Epoch 86/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0333 - accuracy: 0.9894 - val_loss: 0.0853 - val_accuracy: 0.9667\n",
      "Epoch 87/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0303 - accuracy: 0.9917 - val_loss: 0.0835 - val_accuracy: 0.9625\n",
      "Epoch 88/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0292 - accuracy: 0.9917 - val_loss: 0.0847 - val_accuracy: 0.9667\n",
      "Epoch 89/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0287 - accuracy: 0.9931 - val_loss: 0.0862 - val_accuracy: 0.9667\n",
      "Epoch 90/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0352 - accuracy: 0.9884 - val_loss: 0.0844 - val_accuracy: 0.9625\n",
      "Epoch 91/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0310 - accuracy: 0.9907 - val_loss: 0.0838 - val_accuracy: 0.9625\n",
      "Epoch 92/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0309 - accuracy: 0.9894 - val_loss: 0.0861 - val_accuracy: 0.9667\n",
      "Epoch 93/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0289 - accuracy: 0.9912 - val_loss: 0.0863 - val_accuracy: 0.9667\n",
      "Epoch 94/100\n",
      "540/540 [==============================] - 63s 116ms/step - loss: 0.0340 - accuracy: 0.9903 - val_loss: 0.0849 - val_accuracy: 0.9667\n",
      "Epoch 95/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0284 - accuracy: 0.9898 - val_loss: 0.0856 - val_accuracy: 0.9625\n",
      "Epoch 96/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0302 - accuracy: 0.9903 - val_loss: 0.0858 - val_accuracy: 0.9667\n",
      "Epoch 97/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0291 - accuracy: 0.9921 - val_loss: 0.0850 - val_accuracy: 0.9667\n",
      "Epoch 98/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0329 - accuracy: 0.9917 - val_loss: 0.0876 - val_accuracy: 0.9667\n",
      "Epoch 99/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0377 - accuracy: 0.9889 - val_loss: 0.0866 - val_accuracy: 0.9667\n",
      "Epoch 100/100\n",
      "540/540 [==============================] - 62s 115ms/step - loss: 0.0280 - accuracy: 0.9917 - val_loss: 0.0854 - val_accuracy: 0.9625\n",
      "Score for fold 10: loss of 0.08535891026258469; accuracy of 96.24999761581421%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.14074088633060455 - Accuracy: 92.91666746139526%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.10217005759477615 - Accuracy: 94.16666626930237%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.09843650460243225 - Accuracy: 96.66666388511658%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.11224793642759323 - Accuracy: 93.75%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.11230198293924332 - Accuracy: 94.16666626930237%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.0969468206167221 - Accuracy: 94.16666626930237%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.08098980784416199 - Accuracy: 94.9999988079071%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.08022759109735489 - Accuracy: 95.83333134651184%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.1676643043756485 - Accuracy: 94.16666626930237%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.08535891026258469 - Accuracy: 96.24999761581421%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 94.70833241939545 (+- 1.1342203840568024)\n",
      "> Loss: 0.10770848020911217\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    for train, test in kfold.split(X_train, y_train):\n",
    "        print(train.shape, test.shape)\n",
    "        \n",
    "        np.savetxt('E:/3.18/MTF2/train/' + f'train_{fold_no}.csv', train, delimiter=\",\")\n",
    "        np.savetxt('E:/3.18/MTF2/test/' + f'test_{fold_no}.csv', test, delimiter=\",\")\n",
    "\n",
    "        input = Input(shape=(300, 300, 2))\n",
    "        model = DenseNet121(input_tensor=input, include_top=False, weights=None, pooling='avg')\n",
    "        \n",
    "        x = model.output\n",
    "\n",
    "        x = Dense(3, activation='softmax', name='softmax', kernel_initializer='he_normal')(x)\n",
    "        model = Model(model.input, x)\n",
    "\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.SGD(lr=0.01, decay=1e-3, momentum=0.9, nesterov=True)\n",
    "        #optimizer = optimizers.Adam(lr=0.001)\n",
    "        \n",
    "        callbacks_list = [LearningRateSchedule([20,40])]\n",
    "        \n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        \n",
    "        history = model.fit(inputs[train], targets[train], \n",
    "                            batch_size=4, \n",
    "                            epochs=100, \n",
    "                            verbose=1,\n",
    "                            validation_data=(inputs[test], targets[test]),\n",
    "                            callbacks = callbacks_list) #  Validation set  ?\n",
    "        \n",
    "        scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "        print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "        acc_per_fold.append(scores[1] * 100)\n",
    "        loss_per_fold.append(scores[0])\n",
    "        \n",
    "        model.save('E:/3.18/MTF/weight/' + f'MTF_{fold_no}.h5',fold_no)\n",
    "        \n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training','validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training','validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c7ec7915fb6663623914b2919607c696d31d8503b403c6d54bcb30c0bb224a7"
  },
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
