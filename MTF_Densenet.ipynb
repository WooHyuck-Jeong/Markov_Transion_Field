{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "WARNING:tensorflow:From C:\\Users\\jwhyu\\AppData\\Local\\Temp/ipykernel_5492/3517717524.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "print(tf.__version__) \n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7468868182042614339\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6300696576\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 4950795495204760934\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:0a:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from math import ceil\n",
    "from numba import njit, prange\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array\n",
    "from pyts.preprocessing import MinMaxScaler\n",
    "from pyts.approximation import PiecewiseAggregateApproximation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import np_utils\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Welding_data = np.load('E:/Result/ver.3.22/MTF/MTF.npz')\n",
    "\n",
    "X_data = Welding_data['X_data']\n",
    "y_data = Welding_data['y_data']\n",
    "i_data = Welding_data['i_data']\n",
    "\n",
    "Welding_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 2 ... 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, i_train, i_test = train_test_split(X_data,y_data,i_data, test_size = 0.2, shuffle = True, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 300, 300, 2)\n",
      "(943,)\n",
      "(236, 300, 300, 2)\n",
      "(236,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate((X_train,X_test))\n",
    "targets = np.concatenate((y_train,y_test))\n",
    "index = np.concatenate((i_train,i_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "targets = np_utils.to_categorical(targets)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import Dense, Input, Activation, Flatten, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization.batch_normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 300, 300, 2  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " zero_padding2d (ZeroPadding2D)  (None, 306, 306, 2)  0          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1/conv (Conv2D)            (None, 150, 150, 64  6272        ['zero_padding2d[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1/bn (BatchNormalization)  (None, 150, 150, 64  256         ['conv1/conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1/relu (Activation)        (None, 150, 150, 64  0           ['conv1/bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " zero_padding2d_1 (ZeroPadding2  (None, 152, 152, 64  0          ['conv1/relu[0][0]']             \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " pool1 (MaxPooling2D)           (None, 75, 75, 64)   0           ['zero_padding2d_1[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 75, 75, 64)  256         ['pool1[0][0]']                  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_0_relu (Activatio  (None, 75, 75, 64)  0           ['conv2_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 75, 75, 128)  8192        ['conv2_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_concat (Concatena  (None, 75, 75, 96)  0           ['pool1[0][0]',                  \n",
      " te)                                                              'conv2_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_0_bn (BatchNormal  (None, 75, 75, 96)  384         ['conv2_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_0_relu (Activatio  (None, 75, 75, 96)  0           ['conv2_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 75, 75, 128)  12288       ['conv2_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_concat (Concatena  (None, 75, 75, 128)  0          ['conv2_block1_concat[0][0]',    \n",
      " te)                                                              'conv2_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_0_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_0_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 75, 75, 128)  16384       ['conv2_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_concat (Concatena  (None, 75, 75, 160)  0          ['conv2_block2_concat[0][0]',    \n",
      " te)                                                              'conv2_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_0_bn (BatchNormal  (None, 75, 75, 160)  640        ['conv2_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_0_relu (Activatio  (None, 75, 75, 160)  0          ['conv2_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_1_conv (Conv2D)   (None, 75, 75, 128)  20480       ['conv2_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_concat (Concatena  (None, 75, 75, 192)  0          ['conv2_block3_concat[0][0]',    \n",
      " te)                                                              'conv2_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_0_bn (BatchNormal  (None, 75, 75, 192)  768        ['conv2_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_0_relu (Activatio  (None, 75, 75, 192)  0          ['conv2_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_1_conv (Conv2D)   (None, 75, 75, 128)  24576       ['conv2_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_concat (Concatena  (None, 75, 75, 224)  0          ['conv2_block4_concat[0][0]',    \n",
      " te)                                                              'conv2_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_0_bn (BatchNormal  (None, 75, 75, 224)  896        ['conv2_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_0_relu (Activatio  (None, 75, 75, 224)  0          ['conv2_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_1_conv (Conv2D)   (None, 75, 75, 128)  28672       ['conv2_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_1_bn (BatchNormal  (None, 75, 75, 128)  512        ['conv2_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_1_relu (Activatio  (None, 75, 75, 128)  0          ['conv2_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_2_conv (Conv2D)   (None, 75, 75, 32)   36864       ['conv2_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_concat (Concatena  (None, 75, 75, 256)  0          ['conv2_block5_concat[0][0]',    \n",
      " te)                                                              'conv2_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_bn (BatchNormalization)  (None, 75, 75, 256)  1024        ['conv2_block6_concat[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_relu (Activation)        (None, 75, 75, 256)  0           ['pool2_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool2_conv (Conv2D)            (None, 75, 75, 128)  32768       ['pool2_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool2_pool (AveragePooling2D)  (None, 37, 37, 128)  0           ['pool2_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 37, 37, 128)  512        ['pool2_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_0_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 37, 37, 128)  16384       ['conv3_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_concat (Concatena  (None, 37, 37, 160)  0          ['pool2_pool[0][0]',             \n",
      " te)                                                              'conv3_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_0_bn (BatchNormal  (None, 37, 37, 160)  640        ['conv3_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_0_relu (Activatio  (None, 37, 37, 160)  0          ['conv3_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 37, 37, 128)  20480       ['conv3_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_concat (Concatena  (None, 37, 37, 192)  0          ['conv3_block1_concat[0][0]',    \n",
      " te)                                                              'conv3_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_0_bn (BatchNormal  (None, 37, 37, 192)  768        ['conv3_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_0_relu (Activatio  (None, 37, 37, 192)  0          ['conv3_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 37, 37, 128)  24576       ['conv3_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_concat (Concatena  (None, 37, 37, 224)  0          ['conv3_block2_concat[0][0]',    \n",
      " te)                                                              'conv3_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_0_bn (BatchNormal  (None, 37, 37, 224)  896        ['conv3_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_0_relu (Activatio  (None, 37, 37, 224)  0          ['conv3_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 37, 37, 128)  28672       ['conv3_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_concat (Concatena  (None, 37, 37, 256)  0          ['conv3_block3_concat[0][0]',    \n",
      " te)                                                              'conv3_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_0_bn (BatchNormal  (None, 37, 37, 256)  1024       ['conv3_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_0_relu (Activatio  (None, 37, 37, 256)  0          ['conv3_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_1_conv (Conv2D)   (None, 37, 37, 128)  32768       ['conv3_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_concat (Concatena  (None, 37, 37, 288)  0          ['conv3_block4_concat[0][0]',    \n",
      " te)                                                              'conv3_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_0_bn (BatchNormal  (None, 37, 37, 288)  1152       ['conv3_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_0_relu (Activatio  (None, 37, 37, 288)  0          ['conv3_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_1_conv (Conv2D)   (None, 37, 37, 128)  36864       ['conv3_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_concat (Concatena  (None, 37, 37, 320)  0          ['conv3_block5_concat[0][0]',    \n",
      " te)                                                              'conv3_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_0_bn (BatchNormal  (None, 37, 37, 320)  1280       ['conv3_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_0_relu (Activatio  (None, 37, 37, 320)  0          ['conv3_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_1_conv (Conv2D)   (None, 37, 37, 128)  40960       ['conv3_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_concat (Concatena  (None, 37, 37, 352)  0          ['conv3_block6_concat[0][0]',    \n",
      " te)                                                              'conv3_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_0_bn (BatchNormal  (None, 37, 37, 352)  1408       ['conv3_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_0_relu (Activatio  (None, 37, 37, 352)  0          ['conv3_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_1_conv (Conv2D)   (None, 37, 37, 128)  45056       ['conv3_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_concat (Concatena  (None, 37, 37, 384)  0          ['conv3_block7_concat[0][0]',    \n",
      " te)                                                              'conv3_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_0_bn (BatchNormal  (None, 37, 37, 384)  1536       ['conv3_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_0_relu (Activatio  (None, 37, 37, 384)  0          ['conv3_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_1_conv (Conv2D)   (None, 37, 37, 128)  49152       ['conv3_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_1_bn (BatchNormal  (None, 37, 37, 128)  512        ['conv3_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_1_relu (Activatio  (None, 37, 37, 128)  0          ['conv3_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_2_conv (Conv2D)   (None, 37, 37, 32)   36864       ['conv3_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_concat (Concatena  (None, 37, 37, 416)  0          ['conv3_block8_concat[0][0]',    \n",
      " te)                                                              'conv3_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block10_0_bn (BatchNorma  (None, 37, 37, 416)  1664       ['conv3_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_0_relu (Activati  (None, 37, 37, 416)  0          ['conv3_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block10_1_conv (Conv2D)  (None, 37, 37, 128)  53248       ['conv3_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_1_bn (BatchNorma  (None, 37, 37, 128)  512        ['conv3_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_1_relu (Activati  (None, 37, 37, 128)  0          ['conv3_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block10_2_conv (Conv2D)  (None, 37, 37, 32)   36864       ['conv3_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_concat (Concaten  (None, 37, 37, 448)  0          ['conv3_block9_concat[0][0]',    \n",
      " ate)                                                             'conv3_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_0_bn (BatchNorma  (None, 37, 37, 448)  1792       ['conv3_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_0_relu (Activati  (None, 37, 37, 448)  0          ['conv3_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_1_conv (Conv2D)  (None, 37, 37, 128)  57344       ['conv3_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_1_bn (BatchNorma  (None, 37, 37, 128)  512        ['conv3_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_1_relu (Activati  (None, 37, 37, 128)  0          ['conv3_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_2_conv (Conv2D)  (None, 37, 37, 32)   36864       ['conv3_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_concat (Concaten  (None, 37, 37, 480)  0          ['conv3_block10_concat[0][0]',   \n",
      " ate)                                                             'conv3_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_0_bn (BatchNorma  (None, 37, 37, 480)  1920       ['conv3_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_0_relu (Activati  (None, 37, 37, 480)  0          ['conv3_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_1_conv (Conv2D)  (None, 37, 37, 128)  61440       ['conv3_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_1_bn (BatchNorma  (None, 37, 37, 128)  512        ['conv3_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_1_relu (Activati  (None, 37, 37, 128)  0          ['conv3_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_2_conv (Conv2D)  (None, 37, 37, 32)   36864       ['conv3_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_concat (Concaten  (None, 37, 37, 512)  0          ['conv3_block11_concat[0][0]',   \n",
      " ate)                                                             'conv3_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_bn (BatchNormalization)  (None, 37, 37, 512)  2048        ['conv3_block12_concat[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_relu (Activation)        (None, 37, 37, 512)  0           ['pool3_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool3_conv (Conv2D)            (None, 37, 37, 256)  131072      ['pool3_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool3_pool (AveragePooling2D)  (None, 18, 18, 256)  0           ['pool3_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 18, 18, 256)  1024       ['pool3_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_0_relu (Activatio  (None, 18, 18, 256)  0          ['conv4_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 18, 18, 128)  32768       ['conv4_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_concat (Concatena  (None, 18, 18, 288)  0          ['pool3_pool[0][0]',             \n",
      " te)                                                              'conv4_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_0_bn (BatchNormal  (None, 18, 18, 288)  1152       ['conv4_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_0_relu (Activatio  (None, 18, 18, 288)  0          ['conv4_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 18, 18, 128)  36864       ['conv4_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_concat (Concatena  (None, 18, 18, 320)  0          ['conv4_block1_concat[0][0]',    \n",
      " te)                                                              'conv4_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_0_bn (BatchNormal  (None, 18, 18, 320)  1280       ['conv4_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_0_relu (Activatio  (None, 18, 18, 320)  0          ['conv4_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 18, 18, 128)  40960       ['conv4_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_concat (Concatena  (None, 18, 18, 352)  0          ['conv4_block2_concat[0][0]',    \n",
      " te)                                                              'conv4_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_0_bn (BatchNormal  (None, 18, 18, 352)  1408       ['conv4_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_0_relu (Activatio  (None, 18, 18, 352)  0          ['conv4_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 18, 18, 128)  45056       ['conv4_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_concat (Concatena  (None, 18, 18, 384)  0          ['conv4_block3_concat[0][0]',    \n",
      " te)                                                              'conv4_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_0_bn (BatchNormal  (None, 18, 18, 384)  1536       ['conv4_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_0_relu (Activatio  (None, 18, 18, 384)  0          ['conv4_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 18, 18, 128)  49152       ['conv4_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_concat (Concatena  (None, 18, 18, 416)  0          ['conv4_block4_concat[0][0]',    \n",
      " te)                                                              'conv4_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_0_bn (BatchNormal  (None, 18, 18, 416)  1664       ['conv4_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_0_relu (Activatio  (None, 18, 18, 416)  0          ['conv4_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 18, 18, 128)  53248       ['conv4_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_concat (Concatena  (None, 18, 18, 448)  0          ['conv4_block5_concat[0][0]',    \n",
      " te)                                                              'conv4_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_0_bn (BatchNormal  (None, 18, 18, 448)  1792       ['conv4_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_0_relu (Activatio  (None, 18, 18, 448)  0          ['conv4_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_1_conv (Conv2D)   (None, 18, 18, 128)  57344       ['conv4_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_concat (Concatena  (None, 18, 18, 480)  0          ['conv4_block6_concat[0][0]',    \n",
      " te)                                                              'conv4_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_0_bn (BatchNormal  (None, 18, 18, 480)  1920       ['conv4_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_0_relu (Activatio  (None, 18, 18, 480)  0          ['conv4_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_1_conv (Conv2D)   (None, 18, 18, 128)  61440       ['conv4_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_concat (Concatena  (None, 18, 18, 512)  0          ['conv4_block7_concat[0][0]',    \n",
      " te)                                                              'conv4_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_0_bn (BatchNormal  (None, 18, 18, 512)  2048       ['conv4_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_0_relu (Activatio  (None, 18, 18, 512)  0          ['conv4_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_1_conv (Conv2D)   (None, 18, 18, 128)  65536       ['conv4_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_1_bn (BatchNormal  (None, 18, 18, 128)  512        ['conv4_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_1_relu (Activatio  (None, 18, 18, 128)  0          ['conv4_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_2_conv (Conv2D)   (None, 18, 18, 32)   36864       ['conv4_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_concat (Concatena  (None, 18, 18, 544)  0          ['conv4_block8_concat[0][0]',    \n",
      " te)                                                              'conv4_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block10_0_bn (BatchNorma  (None, 18, 18, 544)  2176       ['conv4_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_0_relu (Activati  (None, 18, 18, 544)  0          ['conv4_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_1_conv (Conv2D)  (None, 18, 18, 128)  69632       ['conv4_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_concat (Concaten  (None, 18, 18, 576)  0          ['conv4_block9_concat[0][0]',    \n",
      " ate)                                                             'conv4_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_0_bn (BatchNorma  (None, 18, 18, 576)  2304       ['conv4_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_0_relu (Activati  (None, 18, 18, 576)  0          ['conv4_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_1_conv (Conv2D)  (None, 18, 18, 128)  73728       ['conv4_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_concat (Concaten  (None, 18, 18, 608)  0          ['conv4_block10_concat[0][0]',   \n",
      " ate)                                                             'conv4_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_0_bn (BatchNorma  (None, 18, 18, 608)  2432       ['conv4_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_0_relu (Activati  (None, 18, 18, 608)  0          ['conv4_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_1_conv (Conv2D)  (None, 18, 18, 128)  77824       ['conv4_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_concat (Concaten  (None, 18, 18, 640)  0          ['conv4_block11_concat[0][0]',   \n",
      " ate)                                                             'conv4_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_0_bn (BatchNorma  (None, 18, 18, 640)  2560       ['conv4_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_0_relu (Activati  (None, 18, 18, 640)  0          ['conv4_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_1_conv (Conv2D)  (None, 18, 18, 128)  81920       ['conv4_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_concat (Concaten  (None, 18, 18, 672)  0          ['conv4_block12_concat[0][0]',   \n",
      " ate)                                                             'conv4_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_0_bn (BatchNorma  (None, 18, 18, 672)  2688       ['conv4_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_0_relu (Activati  (None, 18, 18, 672)  0          ['conv4_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_1_conv (Conv2D)  (None, 18, 18, 128)  86016       ['conv4_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_concat (Concaten  (None, 18, 18, 704)  0          ['conv4_block13_concat[0][0]',   \n",
      " ate)                                                             'conv4_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_0_bn (BatchNorma  (None, 18, 18, 704)  2816       ['conv4_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_0_relu (Activati  (None, 18, 18, 704)  0          ['conv4_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_1_conv (Conv2D)  (None, 18, 18, 128)  90112       ['conv4_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block15_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_concat (Concaten  (None, 18, 18, 736)  0          ['conv4_block14_concat[0][0]',   \n",
      " ate)                                                             'conv4_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_0_bn (BatchNorma  (None, 18, 18, 736)  2944       ['conv4_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_0_relu (Activati  (None, 18, 18, 736)  0          ['conv4_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_1_conv (Conv2D)  (None, 18, 18, 128)  94208       ['conv4_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_concat (Concaten  (None, 18, 18, 768)  0          ['conv4_block15_concat[0][0]',   \n",
      " ate)                                                             'conv4_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_0_bn (BatchNorma  (None, 18, 18, 768)  3072       ['conv4_block16_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_0_relu (Activati  (None, 18, 18, 768)  0          ['conv4_block17_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_1_conv (Conv2D)  (None, 18, 18, 128)  98304       ['conv4_block17_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block17_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block17_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block17_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_concat (Concaten  (None, 18, 18, 800)  0          ['conv4_block16_concat[0][0]',   \n",
      " ate)                                                             'conv4_block17_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_0_bn (BatchNorma  (None, 18, 18, 800)  3200       ['conv4_block17_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_0_relu (Activati  (None, 18, 18, 800)  0          ['conv4_block18_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_1_conv (Conv2D)  (None, 18, 18, 128)  102400      ['conv4_block18_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block18_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block18_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block18_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_concat (Concaten  (None, 18, 18, 832)  0          ['conv4_block17_concat[0][0]',   \n",
      " ate)                                                             'conv4_block18_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_0_bn (BatchNorma  (None, 18, 18, 832)  3328       ['conv4_block18_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_0_relu (Activati  (None, 18, 18, 832)  0          ['conv4_block19_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_1_conv (Conv2D)  (None, 18, 18, 128)  106496      ['conv4_block19_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block19_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block19_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block19_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_concat (Concaten  (None, 18, 18, 864)  0          ['conv4_block18_concat[0][0]',   \n",
      " ate)                                                             'conv4_block19_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_0_bn (BatchNorma  (None, 18, 18, 864)  3456       ['conv4_block19_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block20_0_relu (Activati  (None, 18, 18, 864)  0          ['conv4_block20_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_1_conv (Conv2D)  (None, 18, 18, 128)  110592      ['conv4_block20_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block20_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block20_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block20_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block20_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_concat (Concaten  (None, 18, 18, 896)  0          ['conv4_block19_concat[0][0]',   \n",
      " ate)                                                             'conv4_block20_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_0_bn (BatchNorma  (None, 18, 18, 896)  3584       ['conv4_block20_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_0_relu (Activati  (None, 18, 18, 896)  0          ['conv4_block21_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_1_conv (Conv2D)  (None, 18, 18, 128)  114688      ['conv4_block21_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block21_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block21_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block21_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_concat (Concaten  (None, 18, 18, 928)  0          ['conv4_block20_concat[0][0]',   \n",
      " ate)                                                             'conv4_block21_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_0_bn (BatchNorma  (None, 18, 18, 928)  3712       ['conv4_block21_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_0_relu (Activati  (None, 18, 18, 928)  0          ['conv4_block22_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_1_conv (Conv2D)  (None, 18, 18, 128)  118784      ['conv4_block22_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block22_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block22_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block22_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_concat (Concaten  (None, 18, 18, 960)  0          ['conv4_block21_concat[0][0]',   \n",
      " ate)                                                             'conv4_block22_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_0_bn (BatchNorma  (None, 18, 18, 960)  3840       ['conv4_block22_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_0_relu (Activati  (None, 18, 18, 960)  0          ['conv4_block23_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_1_conv (Conv2D)  (None, 18, 18, 128)  122880      ['conv4_block23_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block23_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block23_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block23_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_concat (Concaten  (None, 18, 18, 992)  0          ['conv4_block22_concat[0][0]',   \n",
      " ate)                                                             'conv4_block23_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_0_bn (BatchNorma  (None, 18, 18, 992)  3968       ['conv4_block23_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block24_0_relu (Activati  (None, 18, 18, 992)  0          ['conv4_block24_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_1_conv (Conv2D)  (None, 18, 18, 128)  126976      ['conv4_block24_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_1_bn (BatchNorma  (None, 18, 18, 128)  512        ['conv4_block24_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block24_1_relu (Activati  (None, 18, 18, 128)  0          ['conv4_block24_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_2_conv (Conv2D)  (None, 18, 18, 32)   36864       ['conv4_block24_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_concat (Concaten  (None, 18, 18, 1024  0          ['conv4_block23_concat[0][0]',   \n",
      " ate)                           )                                 'conv4_block24_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool4_bn (BatchNormalization)  (None, 18, 18, 1024  4096        ['conv4_block24_concat[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool4_relu (Activation)        (None, 18, 18, 1024  0           ['pool4_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool4_conv (Conv2D)            (None, 18, 18, 512)  524288      ['pool4_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool4_pool (AveragePooling2D)  (None, 9, 9, 512)    0           ['pool4_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 9, 9, 512)   2048        ['pool4_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_0_relu (Activatio  (None, 9, 9, 512)   0           ['conv5_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 9, 9, 128)    65536       ['conv5_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_concat (Concatena  (None, 9, 9, 544)   0           ['pool4_pool[0][0]',             \n",
      " te)                                                              'conv5_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_0_bn (BatchNormal  (None, 9, 9, 544)   2176        ['conv5_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_0_relu (Activatio  (None, 9, 9, 544)   0           ['conv5_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 9, 9, 128)    69632       ['conv5_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_concat (Concatena  (None, 9, 9, 576)   0           ['conv5_block1_concat[0][0]',    \n",
      " te)                                                              'conv5_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_0_bn (BatchNormal  (None, 9, 9, 576)   2304        ['conv5_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_0_relu (Activatio  (None, 9, 9, 576)   0           ['conv5_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 9, 9, 128)    73728       ['conv5_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_concat (Concatena  (None, 9, 9, 608)   0           ['conv5_block2_concat[0][0]',    \n",
      " te)                                                              'conv5_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_0_bn (BatchNormal  (None, 9, 9, 608)   2432        ['conv5_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_0_relu (Activatio  (None, 9, 9, 608)   0           ['conv5_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_1_conv (Conv2D)   (None, 9, 9, 128)    77824       ['conv5_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_concat (Concatena  (None, 9, 9, 640)   0           ['conv5_block3_concat[0][0]',    \n",
      " te)                                                              'conv5_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_0_bn (BatchNormal  (None, 9, 9, 640)   2560        ['conv5_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_0_relu (Activatio  (None, 9, 9, 640)   0           ['conv5_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_1_conv (Conv2D)   (None, 9, 9, 128)    81920       ['conv5_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_concat (Concatena  (None, 9, 9, 672)   0           ['conv5_block4_concat[0][0]',    \n",
      " te)                                                              'conv5_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_0_bn (BatchNormal  (None, 9, 9, 672)   2688        ['conv5_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_0_relu (Activatio  (None, 9, 9, 672)   0           ['conv5_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_1_conv (Conv2D)   (None, 9, 9, 128)    86016       ['conv5_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_concat (Concatena  (None, 9, 9, 704)   0           ['conv5_block5_concat[0][0]',    \n",
      " te)                                                              'conv5_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_0_bn (BatchNormal  (None, 9, 9, 704)   2816        ['conv5_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_0_relu (Activatio  (None, 9, 9, 704)   0           ['conv5_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_1_conv (Conv2D)   (None, 9, 9, 128)    90112       ['conv5_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_concat (Concatena  (None, 9, 9, 736)   0           ['conv5_block6_concat[0][0]',    \n",
      " te)                                                              'conv5_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_0_bn (BatchNormal  (None, 9, 9, 736)   2944        ['conv5_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_0_relu (Activatio  (None, 9, 9, 736)   0           ['conv5_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block8_1_conv (Conv2D)   (None, 9, 9, 128)    94208       ['conv5_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block8_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_concat (Concatena  (None, 9, 9, 768)   0           ['conv5_block7_concat[0][0]',    \n",
      " te)                                                              'conv5_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_0_bn (BatchNormal  (None, 9, 9, 768)   3072        ['conv5_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_0_relu (Activatio  (None, 9, 9, 768)   0           ['conv5_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_1_conv (Conv2D)   (None, 9, 9, 128)    98304       ['conv5_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_1_bn (BatchNormal  (None, 9, 9, 128)   512         ['conv5_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_1_relu (Activatio  (None, 9, 9, 128)   0           ['conv5_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_2_conv (Conv2D)   (None, 9, 9, 32)     36864       ['conv5_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_concat (Concatena  (None, 9, 9, 800)   0           ['conv5_block8_concat[0][0]',    \n",
      " te)                                                              'conv5_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block10_0_bn (BatchNorma  (None, 9, 9, 800)   3200        ['conv5_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_0_relu (Activati  (None, 9, 9, 800)   0           ['conv5_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_1_conv (Conv2D)  (None, 9, 9, 128)    102400      ['conv5_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_concat (Concaten  (None, 9, 9, 832)   0           ['conv5_block9_concat[0][0]',    \n",
      " ate)                                                             'conv5_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_0_bn (BatchNorma  (None, 9, 9, 832)   3328        ['conv5_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_0_relu (Activati  (None, 9, 9, 832)   0           ['conv5_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_1_conv (Conv2D)  (None, 9, 9, 128)    106496      ['conv5_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_concat (Concaten  (None, 9, 9, 864)   0           ['conv5_block10_concat[0][0]',   \n",
      " ate)                                                             'conv5_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_0_bn (BatchNorma  (None, 9, 9, 864)   3456        ['conv5_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_0_relu (Activati  (None, 9, 9, 864)   0           ['conv5_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_1_conv (Conv2D)  (None, 9, 9, 128)    110592      ['conv5_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_concat (Concaten  (None, 9, 9, 896)   0           ['conv5_block11_concat[0][0]',   \n",
      " ate)                                                             'conv5_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_0_bn (BatchNorma  (None, 9, 9, 896)   3584        ['conv5_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block13_0_relu (Activati  (None, 9, 9, 896)   0           ['conv5_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_1_conv (Conv2D)  (None, 9, 9, 128)    114688      ['conv5_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block13_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_concat (Concaten  (None, 9, 9, 928)   0           ['conv5_block12_concat[0][0]',   \n",
      " ate)                                                             'conv5_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_0_bn (BatchNorma  (None, 9, 9, 928)   3712        ['conv5_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_0_relu (Activati  (None, 9, 9, 928)   0           ['conv5_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_1_conv (Conv2D)  (None, 9, 9, 128)    118784      ['conv5_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_concat (Concaten  (None, 9, 9, 960)   0           ['conv5_block13_concat[0][0]',   \n",
      " ate)                                                             'conv5_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_0_bn (BatchNorma  (None, 9, 9, 960)   3840        ['conv5_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_0_relu (Activati  (None, 9, 9, 960)   0           ['conv5_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_1_conv (Conv2D)  (None, 9, 9, 128)    122880      ['conv5_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block15_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_concat (Concaten  (None, 9, 9, 992)   0           ['conv5_block14_concat[0][0]',   \n",
      " ate)                                                             'conv5_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_0_bn (BatchNorma  (None, 9, 9, 992)   3968        ['conv5_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_0_relu (Activati  (None, 9, 9, 992)   0           ['conv5_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_1_conv (Conv2D)  (None, 9, 9, 128)    126976      ['conv5_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_1_bn (BatchNorma  (None, 9, 9, 128)   512         ['conv5_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_1_relu (Activati  (None, 9, 9, 128)   0           ['conv5_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_2_conv (Conv2D)  (None, 9, 9, 32)     36864       ['conv5_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_concat (Concaten  (None, 9, 9, 1024)  0           ['conv5_block15_concat[0][0]',   \n",
      " ate)                                                             'conv5_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " bn (BatchNormalization)        (None, 9, 9, 1024)   4096        ['conv5_block16_concat[0][0]']   \n",
      "                                                                                                  \n",
      " relu (Activation)              (None, 9, 9, 1024)   0           ['bn[0][0]']                     \n",
      "                                                                                                  \n",
      " avg_pool (GlobalAveragePooling  (None, 1024)        0           ['relu[0][0]']                   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['avg_pool[0][0]']               \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 3)            3075        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,037,443\n",
      "Trainable params: 6,953,795\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(300, 300, 2))\n",
    "model = DenseNet121(input_tensor=input, include_top=False, weights=None, pooling='avg')\n",
    " \n",
    "x = model.output\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(3, activation='softmax', name='softmax')(x)\n",
    "\n",
    "model = Model(model.input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class LearningRateSchedule(Callback):\n",
    "    def __init__(self, selected_epochs=[]):\n",
    "        self.selected_epochs = selected_epochs\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch+1) in self.selected_epochs:\n",
    "            lr = K.get_value(self.model.optimizer.lr)\n",
    "            K.set_value(self.model.optimizer.lr, lr*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습데이터의 log를 저장할 폴더 생성(지정)\n",
    "import datetime\n",
    "\n",
    "log_dir = \"logs/my_board/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# 텐서보드 콜백 정의 하기\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= log_dir, histogram_freq= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "118\n",
      "1061\n",
      "117\n",
      "1062\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "test = []\n",
    "train= []\n",
    "test_ = []\n",
    "train_ = []\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    print(len(test))\n",
    "    print(len(train))\n",
    "    for i in zip(test):\n",
    "        test_.append(i)\n",
    "    for i in zip(train):\n",
    "        train_.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_[0:1061]\n",
    "train = np.reshape(train, 1061)\n",
    "test = test_[0:117]\n",
    "test = np.reshape(test, 117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   9   17   34   40   46   53   54   83   97  101  109  113  128  140\n",
      "  141  146  168  171  179  186  204  228  253  263  264  280  306  316\n",
      "  358  372  374  377  379  389  412  422  431  432  446  449  459  463\n",
      "  467  473  484  492  508  509  510  513  523  551  564  571  597  602\n",
      "  620  631  632  642  663  665  666  685  698  716  724  729  732  734\n",
      "  756  761  771  793  797  804  815  819  831  836  840  841  879  883\n",
      "  884  890  907  912  915  920  936  941  946  955  963  975  982  983\n",
      "  985 1003 1013 1025 1047 1050 1053 1067 1070 1077 1083 1102 1105 1111\n",
      " 1120 1154 1166 1172 1177]\n",
      "[   0    1    2 ... 1174 1175 1176]\n",
      "['P_282' 'N_252' 'P_327' 'H_260' 'N_303' 'H_232' 'P_44' 'N_110' 'P_0'\n",
      " 'H_369' 'P_191' 'N_244' 'N_178' 'H_137' 'H_156' 'P_220' 'H_277' 'P_382'\n",
      " 'H_108' 'N_79' 'H_45' 'H_12' 'N_85' 'P_153' 'N_185' 'P_82' 'N_46' 'H_375'\n",
      " 'P_80' 'N_120' 'H_192' 'P_328' 'H_377' 'N_247' 'H_251' 'H_217' 'N_52'\n",
      " 'P_193' 'P_173' 'P_62' 'N_336' 'N_275' 'P_242' 'N_352' 'N_194' 'P_371'\n",
      " 'H_297' 'P_6' 'N_91' 'P_362' 'H_336' 'N_216' 'P_84' 'P_387' 'H_314'\n",
      " 'P_11' 'P_158' 'H_326' 'P_295' 'N_73' 'N_358' 'H_24' 'N_47' 'P_115'\n",
      " 'N_219' 'P_223' 'N_323' 'P_9' 'H_330' 'N_217' 'P_225' 'H_339' 'P_22'\n",
      " 'N_325' 'H_386' 'H_100' 'H_252' 'P_68' 'P_139' 'P_163' 'N_239' 'P_329'\n",
      " 'N_224' 'H_173' 'P_336' 'H_79' 'H_38' 'P_231' 'H_107' 'N_302' 'H_328'\n",
      " 'H_48' 'H_281' 'N_49' 'H_381' 'N_281' 'N_14' 'N_199' 'N_215' 'H_273'\n",
      " 'H_106' 'N_367' 'N_322' 'P_350' 'H_123' 'H_244' 'H_93' 'H_103' 'H_254'\n",
      " 'N_364' 'H_0' 'P_278' 'P_151' 'P_159' 'P_283' 'P_290' 'N_381']\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(train)\n",
    "print(index[test])\n",
    "print(targets[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.mkdir('E:/Result/ver.3.22/MTF/'+ 'weight_')\n",
    "os.mkdir('E:/Result/ver.3.22/MTF/'+ 'train_')\n",
    "os.mkdir('E:/Result/ver.3.22/MTF/'+ 'test_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(848,) (95,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jwhyu\\anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/100\n",
      "212/212 [==============================] - 33s 108ms/step - loss: 0.4169 - accuracy: 0.7005 - val_loss: 1.2595 - val_accuracy: 0.3263\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.3525 - accuracy: 0.7700 - val_loss: 1.5077 - val_accuracy: 0.3263\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.2793 - accuracy: 0.8361 - val_loss: 0.3278 - val_accuracy: 0.8000\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.2467 - accuracy: 0.8573 - val_loss: 0.2401 - val_accuracy: 0.8526\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.2531 - accuracy: 0.8538 - val_loss: 0.6684 - val_accuracy: 0.6947\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.2207 - accuracy: 0.8750 - val_loss: 0.2420 - val_accuracy: 0.8316\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1970 - accuracy: 0.8915 - val_loss: 0.2659 - val_accuracy: 0.8421\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1719 - accuracy: 0.9210 - val_loss: 0.3201 - val_accuracy: 0.7789\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.1688 - accuracy: 0.9127 - val_loss: 0.3368 - val_accuracy: 0.8000\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.1347 - accuracy: 0.9304 - val_loss: 0.3232 - val_accuracy: 0.7158\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.1381 - accuracy: 0.9269 - val_loss: 0.3107 - val_accuracy: 0.8526\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.1301 - accuracy: 0.9328 - val_loss: 0.3423 - val_accuracy: 0.7684\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.1176 - accuracy: 0.9469 - val_loss: 0.5427 - val_accuracy: 0.8211\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.1125 - accuracy: 0.9505 - val_loss: 0.6052 - val_accuracy: 0.8316\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.1002 - accuracy: 0.9517 - val_loss: 0.2293 - val_accuracy: 0.8526\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0766 - accuracy: 0.9682 - val_loss: 0.3636 - val_accuracy: 0.8211\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0685 - accuracy: 0.9682 - val_loss: 0.8285 - val_accuracy: 0.6737\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0674 - accuracy: 0.9741 - val_loss: 0.1932 - val_accuracy: 0.8737\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0816 - accuracy: 0.9670 - val_loss: 0.3784 - val_accuracy: 0.8947\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0788 - accuracy: 0.9670 - val_loss: 0.2516 - val_accuracy: 0.8526\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0346 - accuracy: 0.9917 - val_loss: 0.2609 - val_accuracy: 0.8632\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0680 - accuracy: 0.9682 - val_loss: 0.2456 - val_accuracy: 0.8737\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0771 - accuracy: 0.9646 - val_loss: 0.2532 - val_accuracy: 0.8947\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0522 - accuracy: 0.9823 - val_loss: 0.2317 - val_accuracy: 0.8947\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0550 - accuracy: 0.9811 - val_loss: 0.2239 - val_accuracy: 0.8842\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0535 - accuracy: 0.9823 - val_loss: 0.2253 - val_accuracy: 0.8842\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0522 - accuracy: 0.9811 - val_loss: 0.2226 - val_accuracy: 0.8842\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0340 - accuracy: 0.9906 - val_loss: 0.2267 - val_accuracy: 0.8947\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0545 - accuracy: 0.9764 - val_loss: 0.2205 - val_accuracy: 0.8842\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0359 - accuracy: 0.9929 - val_loss: 0.2315 - val_accuracy: 0.8842\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0378 - accuracy: 0.9906 - val_loss: 0.2311 - val_accuracy: 0.9158\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0354 - accuracy: 0.9906 - val_loss: 0.2349 - val_accuracy: 0.9053\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0677 - accuracy: 0.9646 - val_loss: 0.2275 - val_accuracy: 0.9053\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 20s 97ms/step - loss: 0.0353 - accuracy: 0.9894 - val_loss: 0.2304 - val_accuracy: 0.9053\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0500 - accuracy: 0.9776 - val_loss: 0.2230 - val_accuracy: 0.8842\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0559 - accuracy: 0.9729 - val_loss: 0.2216 - val_accuracy: 0.8842\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0522 - accuracy: 0.9800 - val_loss: 0.2317 - val_accuracy: 0.9158\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0558 - accuracy: 0.9776 - val_loss: 0.2318 - val_accuracy: 0.9158\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0408 - accuracy: 0.9823 - val_loss: 0.2214 - val_accuracy: 0.9053\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 20s 97ms/step - loss: 0.0403 - accuracy: 0.9882 - val_loss: 0.2216 - val_accuracy: 0.8842\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0472 - accuracy: 0.9835 - val_loss: 0.2220 - val_accuracy: 0.8842\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0504 - accuracy: 0.9800 - val_loss: 0.2263 - val_accuracy: 0.9053\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0351 - accuracy: 0.9894 - val_loss: 0.2270 - val_accuracy: 0.9053\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0456 - accuracy: 0.9835 - val_loss: 0.2248 - val_accuracy: 0.8947\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0480 - accuracy: 0.9811 - val_loss: 0.2250 - val_accuracy: 0.8947\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 20s 97ms/step - loss: 0.0396 - accuracy: 0.9870 - val_loss: 0.2267 - val_accuracy: 0.9158\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0527 - accuracy: 0.9729 - val_loss: 0.2240 - val_accuracy: 0.8947\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0411 - accuracy: 0.9835 - val_loss: 0.2231 - val_accuracy: 0.8842\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0538 - accuracy: 0.9776 - val_loss: 0.2246 - val_accuracy: 0.8947\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0463 - accuracy: 0.9788 - val_loss: 0.2260 - val_accuracy: 0.8947\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0354 - accuracy: 0.9870 - val_loss: 0.2237 - val_accuracy: 0.8947\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0376 - accuracy: 0.9858 - val_loss: 0.2253 - val_accuracy: 0.9053\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0548 - accuracy: 0.9788 - val_loss: 0.2249 - val_accuracy: 0.8947\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0451 - accuracy: 0.9835 - val_loss: 0.2258 - val_accuracy: 0.8947\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0448 - accuracy: 0.9800 - val_loss: 0.2222 - val_accuracy: 0.8842\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0486 - accuracy: 0.9776 - val_loss: 0.2252 - val_accuracy: 0.8947\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0399 - accuracy: 0.9894 - val_loss: 0.2237 - val_accuracy: 0.8947\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0411 - accuracy: 0.9847 - val_loss: 0.2244 - val_accuracy: 0.8947\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0387 - accuracy: 0.9847 - val_loss: 0.2227 - val_accuracy: 0.8947\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0440 - accuracy: 0.9847 - val_loss: 0.2224 - val_accuracy: 0.8842\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0457 - accuracy: 0.9858 - val_loss: 0.2244 - val_accuracy: 0.8947\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0588 - accuracy: 0.9776 - val_loss: 0.2243 - val_accuracy: 0.8947\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0580 - accuracy: 0.9705 - val_loss: 0.2249 - val_accuracy: 0.8842\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0500 - accuracy: 0.9800 - val_loss: 0.2247 - val_accuracy: 0.8947\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0454 - accuracy: 0.9858 - val_loss: 0.2272 - val_accuracy: 0.8947\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0568 - accuracy: 0.9764 - val_loss: 0.2274 - val_accuracy: 0.9053\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0353 - accuracy: 0.9906 - val_loss: 0.2243 - val_accuracy: 0.8947\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0537 - accuracy: 0.9764 - val_loss: 0.2248 - val_accuracy: 0.8947\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0469 - accuracy: 0.9835 - val_loss: 0.2245 - val_accuracy: 0.8947\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0404 - accuracy: 0.9847 - val_loss: 0.2254 - val_accuracy: 0.8947\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0478 - accuracy: 0.9823 - val_loss: 0.2256 - val_accuracy: 0.8947\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0525 - accuracy: 0.9729 - val_loss: 0.2263 - val_accuracy: 0.8947\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0327 - accuracy: 0.9894 - val_loss: 0.2241 - val_accuracy: 0.8947\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0303 - accuracy: 0.9917 - val_loss: 0.2257 - val_accuracy: 0.8947\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0326 - accuracy: 0.9917 - val_loss: 0.2252 - val_accuracy: 0.8947\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0560 - accuracy: 0.9741 - val_loss: 0.2252 - val_accuracy: 0.8947\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0504 - accuracy: 0.9800 - val_loss: 0.2251 - val_accuracy: 0.8947\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0424 - accuracy: 0.9811 - val_loss: 0.2246 - val_accuracy: 0.8947\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0418 - accuracy: 0.9870 - val_loss: 0.2290 - val_accuracy: 0.9053\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0367 - accuracy: 0.9870 - val_loss: 0.2252 - val_accuracy: 0.8947\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0398 - accuracy: 0.9882 - val_loss: 0.2261 - val_accuracy: 0.8947\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0513 - accuracy: 0.9752 - val_loss: 0.2265 - val_accuracy: 0.8947\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0177 - accuracy: 0.9988 - val_loss: 0.2232 - val_accuracy: 0.8947\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0367 - accuracy: 0.9847 - val_loss: 0.2254 - val_accuracy: 0.8947\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0302 - accuracy: 0.9894 - val_loss: 0.2252 - val_accuracy: 0.8947\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0476 - accuracy: 0.9764 - val_loss: 0.2267 - val_accuracy: 0.9053\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0529 - accuracy: 0.9752 - val_loss: 0.2280 - val_accuracy: 0.8947\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0420 - accuracy: 0.9800 - val_loss: 0.2231 - val_accuracy: 0.8842\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0498 - accuracy: 0.9752 - val_loss: 0.2252 - val_accuracy: 0.8947\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0396 - accuracy: 0.9835 - val_loss: 0.2253 - val_accuracy: 0.9053\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0590 - accuracy: 0.9752 - val_loss: 0.2281 - val_accuracy: 0.8947\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0471 - accuracy: 0.9858 - val_loss: 0.2254 - val_accuracy: 0.8947\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0438 - accuracy: 0.9835 - val_loss: 0.2253 - val_accuracy: 0.8947\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0397 - accuracy: 0.9835 - val_loss: 0.2247 - val_accuracy: 0.8947\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0456 - accuracy: 0.9741 - val_loss: 0.2240 - val_accuracy: 0.8947\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0413 - accuracy: 0.9823 - val_loss: 0.2276 - val_accuracy: 0.9053\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0496 - accuracy: 0.9788 - val_loss: 0.2242 - val_accuracy: 0.8947\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0545 - accuracy: 0.9693 - val_loss: 0.2238 - val_accuracy: 0.8947\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0273 - accuracy: 0.9917 - val_loss: 0.2244 - val_accuracy: 0.8947\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0433 - accuracy: 0.9835 - val_loss: 0.2243 - val_accuracy: 0.8947\n",
      "Score for fold 1: loss of 0.22429415583610535; accuracy of 89.47368264198303%\n",
      "(848,) (95,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "212/212 [==============================] - 26s 100ms/step - loss: 0.4714 - accuracy: 0.6368 - val_loss: 0.9147 - val_accuracy: 0.3263\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.3515 - accuracy: 0.7771 - val_loss: 1.5444 - val_accuracy: 0.3263\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.2971 - accuracy: 0.8255 - val_loss: 0.5398 - val_accuracy: 0.7053\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.2534 - accuracy: 0.8538 - val_loss: 0.6147 - val_accuracy: 0.6842\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.2042 - accuracy: 0.8868 - val_loss: 0.6047 - val_accuracy: 0.6632\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.1839 - accuracy: 0.9057 - val_loss: 0.3500 - val_accuracy: 0.8526\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.2177 - accuracy: 0.8726 - val_loss: 0.1771 - val_accuracy: 0.8947\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1452 - accuracy: 0.9269 - val_loss: 1.6249 - val_accuracy: 0.6526\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.1596 - accuracy: 0.9222 - val_loss: 0.1429 - val_accuracy: 0.9263\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.1319 - accuracy: 0.9363 - val_loss: 0.1948 - val_accuracy: 0.8947\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.1264 - accuracy: 0.9387 - val_loss: 0.2193 - val_accuracy: 0.8421\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.1259 - accuracy: 0.9292 - val_loss: 1.1146 - val_accuracy: 0.6526\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.1170 - accuracy: 0.9493 - val_loss: 0.2209 - val_accuracy: 0.8316\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0976 - accuracy: 0.9587 - val_loss: 0.2389 - val_accuracy: 0.8421\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0889 - accuracy: 0.9599 - val_loss: 0.2411 - val_accuracy: 0.8632\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.1251 - accuracy: 0.9446 - val_loss: 0.3683 - val_accuracy: 0.8316\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0915 - accuracy: 0.9587 - val_loss: 0.1756 - val_accuracy: 0.9053\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0823 - accuracy: 0.9611 - val_loss: 1.0545 - val_accuracy: 0.6947\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0561 - accuracy: 0.9811 - val_loss: 0.2845 - val_accuracy: 0.8526\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0637 - accuracy: 0.9800 - val_loss: 0.1872 - val_accuracy: 0.8842\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0566 - accuracy: 0.9835 - val_loss: 0.1768 - val_accuracy: 0.8842\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0611 - accuracy: 0.9729 - val_loss: 0.2055 - val_accuracy: 0.8632\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0413 - accuracy: 0.9847 - val_loss: 0.1526 - val_accuracy: 0.8842\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0418 - accuracy: 0.9870 - val_loss: 0.1585 - val_accuracy: 0.8842\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0491 - accuracy: 0.9823 - val_loss: 0.1696 - val_accuracy: 0.8842\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0513 - accuracy: 0.9788 - val_loss: 0.1569 - val_accuracy: 0.8947\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0485 - accuracy: 0.9823 - val_loss: 0.1619 - val_accuracy: 0.8947\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0357 - accuracy: 0.9882 - val_loss: 0.1588 - val_accuracy: 0.9158\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0470 - accuracy: 0.9823 - val_loss: 0.1595 - val_accuracy: 0.8737\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0360 - accuracy: 0.9917 - val_loss: 0.1596 - val_accuracy: 0.8842\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0560 - accuracy: 0.9752 - val_loss: 0.1604 - val_accuracy: 0.8842\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0638 - accuracy: 0.9705 - val_loss: 0.1499 - val_accuracy: 0.9053\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0487 - accuracy: 0.9776 - val_loss: 0.1491 - val_accuracy: 0.8947\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0247 - accuracy: 0.9929 - val_loss: 0.1494 - val_accuracy: 0.9053\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0394 - accuracy: 0.9870 - val_loss: 0.1474 - val_accuracy: 0.9263\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0613 - accuracy: 0.9682 - val_loss: 0.1508 - val_accuracy: 0.9158\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0514 - accuracy: 0.9811 - val_loss: 0.1587 - val_accuracy: 0.8947\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0399 - accuracy: 0.9847 - val_loss: 0.1575 - val_accuracy: 0.8842\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0504 - accuracy: 0.9752 - val_loss: 0.1520 - val_accuracy: 0.9053\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0449 - accuracy: 0.9823 - val_loss: 0.1571 - val_accuracy: 0.9053\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0275 - accuracy: 0.9894 - val_loss: 0.1528 - val_accuracy: 0.9158\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0517 - accuracy: 0.9776 - val_loss: 0.1555 - val_accuracy: 0.8947\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0498 - accuracy: 0.9788 - val_loss: 0.1531 - val_accuracy: 0.9158\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0420 - accuracy: 0.9835 - val_loss: 0.1568 - val_accuracy: 0.8947\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0289 - accuracy: 0.9906 - val_loss: 0.1534 - val_accuracy: 0.9053\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0654 - accuracy: 0.9670 - val_loss: 0.1547 - val_accuracy: 0.9053\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0415 - accuracy: 0.9858 - val_loss: 0.1577 - val_accuracy: 0.8947\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0499 - accuracy: 0.9764 - val_loss: 0.1510 - val_accuracy: 0.9158\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0414 - accuracy: 0.9823 - val_loss: 0.1516 - val_accuracy: 0.9158\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0348 - accuracy: 0.9906 - val_loss: 0.1539 - val_accuracy: 0.9053\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0432 - accuracy: 0.9800 - val_loss: 0.1475 - val_accuracy: 0.9158\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0427 - accuracy: 0.9858 - val_loss: 0.1520 - val_accuracy: 0.9053\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0299 - accuracy: 0.9929 - val_loss: 0.1504 - val_accuracy: 0.9263\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0388 - accuracy: 0.9847 - val_loss: 0.1508 - val_accuracy: 0.9053\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0436 - accuracy: 0.9835 - val_loss: 0.1510 - val_accuracy: 0.9053\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0463 - accuracy: 0.9835 - val_loss: 0.1516 - val_accuracy: 0.9053\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0484 - accuracy: 0.9788 - val_loss: 0.1515 - val_accuracy: 0.9158\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0397 - accuracy: 0.9858 - val_loss: 0.1492 - val_accuracy: 0.9158\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0499 - accuracy: 0.9788 - val_loss: 0.1516 - val_accuracy: 0.9263\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0656 - accuracy: 0.9729 - val_loss: 0.1556 - val_accuracy: 0.9158\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0476 - accuracy: 0.9776 - val_loss: 0.1482 - val_accuracy: 0.9158\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0653 - accuracy: 0.9682 - val_loss: 0.1500 - val_accuracy: 0.9263\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0386 - accuracy: 0.9882 - val_loss: 0.1513 - val_accuracy: 0.9053\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0553 - accuracy: 0.9788 - val_loss: 0.1505 - val_accuracy: 0.9263\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0475 - accuracy: 0.9800 - val_loss: 0.1552 - val_accuracy: 0.9053\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0460 - accuracy: 0.9811 - val_loss: 0.1496 - val_accuracy: 0.9158\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0544 - accuracy: 0.9776 - val_loss: 0.1515 - val_accuracy: 0.9263\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0597 - accuracy: 0.9705 - val_loss: 0.1495 - val_accuracy: 0.9158\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0410 - accuracy: 0.9847 - val_loss: 0.1500 - val_accuracy: 0.9158\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0550 - accuracy: 0.9764 - val_loss: 0.1533 - val_accuracy: 0.9158\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0322 - accuracy: 0.9894 - val_loss: 0.1483 - val_accuracy: 0.9158\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0349 - accuracy: 0.9917 - val_loss: 0.1507 - val_accuracy: 0.9158\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0367 - accuracy: 0.9870 - val_loss: 0.1478 - val_accuracy: 0.9158\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0335 - accuracy: 0.9894 - val_loss: 0.1490 - val_accuracy: 0.9263\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0560 - accuracy: 0.9752 - val_loss: 0.1459 - val_accuracy: 0.9158\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0357 - accuracy: 0.9906 - val_loss: 0.1499 - val_accuracy: 0.9263\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0503 - accuracy: 0.9776 - val_loss: 0.1492 - val_accuracy: 0.9263\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0507 - accuracy: 0.9788 - val_loss: 0.1520 - val_accuracy: 0.9158\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0442 - accuracy: 0.9858 - val_loss: 0.1528 - val_accuracy: 0.9158\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0390 - accuracy: 0.9835 - val_loss: 0.1491 - val_accuracy: 0.9158\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0274 - accuracy: 0.9929 - val_loss: 0.1523 - val_accuracy: 0.9158\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0387 - accuracy: 0.9882 - val_loss: 0.1507 - val_accuracy: 0.9263\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0408 - accuracy: 0.9823 - val_loss: 0.1494 - val_accuracy: 0.9053\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0608 - accuracy: 0.9693 - val_loss: 0.1497 - val_accuracy: 0.9158\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0561 - accuracy: 0.9729 - val_loss: 0.1504 - val_accuracy: 0.9053\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0415 - accuracy: 0.9858 - val_loss: 0.1479 - val_accuracy: 0.9263\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0551 - accuracy: 0.9693 - val_loss: 0.1484 - val_accuracy: 0.9263\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0637 - accuracy: 0.9693 - val_loss: 0.1483 - val_accuracy: 0.9158\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0406 - accuracy: 0.9847 - val_loss: 0.1503 - val_accuracy: 0.9263\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0311 - accuracy: 0.9941 - val_loss: 0.1524 - val_accuracy: 0.9158\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0331 - accuracy: 0.9917 - val_loss: 0.1508 - val_accuracy: 0.9158\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0706 - accuracy: 0.9634 - val_loss: 0.1494 - val_accuracy: 0.9158\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0543 - accuracy: 0.9764 - val_loss: 0.1523 - val_accuracy: 0.9158\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0516 - accuracy: 0.9800 - val_loss: 0.1536 - val_accuracy: 0.9158\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0665 - accuracy: 0.9670 - val_loss: 0.1512 - val_accuracy: 0.9263\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0495 - accuracy: 0.9800 - val_loss: 0.1474 - val_accuracy: 0.9263\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0466 - accuracy: 0.9776 - val_loss: 0.1513 - val_accuracy: 0.9263\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0389 - accuracy: 0.9882 - val_loss: 0.1523 - val_accuracy: 0.9158\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0443 - accuracy: 0.9811 - val_loss: 0.1499 - val_accuracy: 0.9263\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0460 - accuracy: 0.9811 - val_loss: 0.1499 - val_accuracy: 0.9053\n",
      "Score for fold 2: loss of 0.14988403022289276; accuracy of 90.52631855010986%\n",
      "(848,) (95,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "212/212 [==============================] - 27s 100ms/step - loss: 0.4259 - accuracy: 0.6958 - val_loss: 1.4570 - val_accuracy: 0.3263\n",
      "Epoch 2/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.3387 - accuracy: 0.7795 - val_loss: 2.7703 - val_accuracy: 0.3263\n",
      "Epoch 3/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.3232 - accuracy: 0.8054 - val_loss: 0.4472 - val_accuracy: 0.7474\n",
      "Epoch 4/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.2563 - accuracy: 0.8467 - val_loss: 0.4420 - val_accuracy: 0.7895\n",
      "Epoch 5/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.2415 - accuracy: 0.8667 - val_loss: 0.2552 - val_accuracy: 0.8526\n",
      "Epoch 6/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.2023 - accuracy: 0.8892 - val_loss: 0.8184 - val_accuracy: 0.6211\n",
      "Epoch 7/100\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 0.1904 - accuracy: 0.8939 - val_loss: 0.5930 - val_accuracy: 0.6947\n",
      "Epoch 8/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1882 - accuracy: 0.8998 - val_loss: 0.5489 - val_accuracy: 0.7368\n",
      "Epoch 9/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1970 - accuracy: 0.8915 - val_loss: 0.1789 - val_accuracy: 0.9158\n",
      "Epoch 10/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1674 - accuracy: 0.9045 - val_loss: 0.2607 - val_accuracy: 0.8842\n",
      "Epoch 11/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.1416 - accuracy: 0.9222 - val_loss: 0.6226 - val_accuracy: 0.7368\n",
      "Epoch 12/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1217 - accuracy: 0.9363 - val_loss: 0.7971 - val_accuracy: 0.6632\n",
      "Epoch 13/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.1367 - accuracy: 0.9304 - val_loss: 0.4601 - val_accuracy: 0.7895\n",
      "Epoch 14/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.1112 - accuracy: 0.9434 - val_loss: 0.1714 - val_accuracy: 0.8737\n",
      "Epoch 15/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.1068 - accuracy: 0.9505 - val_loss: 0.2198 - val_accuracy: 0.8632\n",
      "Epoch 16/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0787 - accuracy: 0.9717 - val_loss: 0.2153 - val_accuracy: 0.8947\n",
      "Epoch 17/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0964 - accuracy: 0.9599 - val_loss: 0.1549 - val_accuracy: 0.8842\n",
      "Epoch 18/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0978 - accuracy: 0.9528 - val_loss: 0.9737 - val_accuracy: 0.6947\n",
      "Epoch 19/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0799 - accuracy: 0.9670 - val_loss: 0.4759 - val_accuracy: 0.8421\n",
      "Epoch 20/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0948 - accuracy: 0.9552 - val_loss: 0.6022 - val_accuracy: 0.8000\n",
      "Epoch 21/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0785 - accuracy: 0.9658 - val_loss: 0.2453 - val_accuracy: 0.8526\n",
      "Epoch 22/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0387 - accuracy: 0.9882 - val_loss: 0.1622 - val_accuracy: 0.8947\n",
      "Epoch 23/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0382 - accuracy: 0.9917 - val_loss: 0.1507 - val_accuracy: 0.9158\n",
      "Epoch 24/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0461 - accuracy: 0.9870 - val_loss: 0.1612 - val_accuracy: 0.8842\n",
      "Epoch 25/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0437 - accuracy: 0.9858 - val_loss: 0.1554 - val_accuracy: 0.8842\n",
      "Epoch 26/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0655 - accuracy: 0.9741 - val_loss: 0.1578 - val_accuracy: 0.9053\n",
      "Epoch 27/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0569 - accuracy: 0.9800 - val_loss: 0.1496 - val_accuracy: 0.9158\n",
      "Epoch 28/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0669 - accuracy: 0.9741 - val_loss: 0.1406 - val_accuracy: 0.9053\n",
      "Epoch 29/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0684 - accuracy: 0.9729 - val_loss: 0.1382 - val_accuracy: 0.8947\n",
      "Epoch 30/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0812 - accuracy: 0.9575 - val_loss: 0.1540 - val_accuracy: 0.9158\n",
      "Epoch 31/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0547 - accuracy: 0.9800 - val_loss: 0.1389 - val_accuracy: 0.9053\n",
      "Epoch 32/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0569 - accuracy: 0.9752 - val_loss: 0.1382 - val_accuracy: 0.9263\n",
      "Epoch 33/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0592 - accuracy: 0.9776 - val_loss: 0.1316 - val_accuracy: 0.9158\n",
      "Epoch 34/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0322 - accuracy: 0.9929 - val_loss: 0.1316 - val_accuracy: 0.9053\n",
      "Epoch 35/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0368 - accuracy: 0.9917 - val_loss: 0.1362 - val_accuracy: 0.9263\n",
      "Epoch 36/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0599 - accuracy: 0.9776 - val_loss: 0.1396 - val_accuracy: 0.9263\n",
      "Epoch 37/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0479 - accuracy: 0.9835 - val_loss: 0.1377 - val_accuracy: 0.9053\n",
      "Epoch 38/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0725 - accuracy: 0.9623 - val_loss: 0.1354 - val_accuracy: 0.9053\n",
      "Epoch 39/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0513 - accuracy: 0.9776 - val_loss: 0.1828 - val_accuracy: 0.9053\n",
      "Epoch 40/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0276 - accuracy: 0.9941 - val_loss: 0.1334 - val_accuracy: 0.9158\n",
      "Epoch 41/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0360 - accuracy: 0.9906 - val_loss: 0.1328 - val_accuracy: 0.9158\n",
      "Epoch 42/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0615 - accuracy: 0.9646 - val_loss: 0.1330 - val_accuracy: 0.9158\n",
      "Epoch 43/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0542 - accuracy: 0.9788 - val_loss: 0.1335 - val_accuracy: 0.9158\n",
      "Epoch 44/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0374 - accuracy: 0.9906 - val_loss: 0.1348 - val_accuracy: 0.9158\n",
      "Epoch 45/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0347 - accuracy: 0.9894 - val_loss: 0.1332 - val_accuracy: 0.9158\n",
      "Epoch 46/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0541 - accuracy: 0.9752 - val_loss: 0.1322 - val_accuracy: 0.9158\n",
      "Epoch 47/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0755 - accuracy: 0.9599 - val_loss: 0.1320 - val_accuracy: 0.9158\n",
      "Epoch 48/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0500 - accuracy: 0.9788 - val_loss: 0.1319 - val_accuracy: 0.9158\n",
      "Epoch 49/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0414 - accuracy: 0.9858 - val_loss: 0.1344 - val_accuracy: 0.9158\n",
      "Epoch 50/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0476 - accuracy: 0.9823 - val_loss: 0.1346 - val_accuracy: 0.9158\n",
      "Epoch 51/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0591 - accuracy: 0.9729 - val_loss: 0.1364 - val_accuracy: 0.9158\n",
      "Epoch 52/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0358 - accuracy: 0.9917 - val_loss: 0.1325 - val_accuracy: 0.9158\n",
      "Epoch 53/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0392 - accuracy: 0.9906 - val_loss: 0.1324 - val_accuracy: 0.9158\n",
      "Epoch 54/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0428 - accuracy: 0.9823 - val_loss: 0.1330 - val_accuracy: 0.9158\n",
      "Epoch 55/100\n",
      "212/212 [==============================] - 20s 97ms/step - loss: 0.0765 - accuracy: 0.9634 - val_loss: 0.1341 - val_accuracy: 0.9263\n",
      "Epoch 56/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0515 - accuracy: 0.9776 - val_loss: 0.1332 - val_accuracy: 0.9158\n",
      "Epoch 57/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0440 - accuracy: 0.9847 - val_loss: 0.1344 - val_accuracy: 0.9263\n",
      "Epoch 58/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0432 - accuracy: 0.9811 - val_loss: 0.1357 - val_accuracy: 0.9263\n",
      "Epoch 59/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0372 - accuracy: 0.9894 - val_loss: 0.1356 - val_accuracy: 0.9263\n",
      "Epoch 60/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0537 - accuracy: 0.9752 - val_loss: 0.1387 - val_accuracy: 0.9263\n",
      "Epoch 61/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0423 - accuracy: 0.9847 - val_loss: 0.1349 - val_accuracy: 0.9263\n",
      "Epoch 62/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0579 - accuracy: 0.9800 - val_loss: 0.1356 - val_accuracy: 0.9263\n",
      "Epoch 63/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0400 - accuracy: 0.9835 - val_loss: 0.1383 - val_accuracy: 0.9263\n",
      "Epoch 64/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0523 - accuracy: 0.9811 - val_loss: 0.1354 - val_accuracy: 0.9263\n",
      "Epoch 65/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0351 - accuracy: 0.9882 - val_loss: 0.1341 - val_accuracy: 0.9263\n",
      "Epoch 66/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0452 - accuracy: 0.9858 - val_loss: 0.1351 - val_accuracy: 0.9263\n",
      "Epoch 67/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0506 - accuracy: 0.9788 - val_loss: 0.1335 - val_accuracy: 0.9263\n",
      "Epoch 68/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0426 - accuracy: 0.9882 - val_loss: 0.1317 - val_accuracy: 0.9158\n",
      "Epoch 69/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0304 - accuracy: 0.9917 - val_loss: 0.1331 - val_accuracy: 0.9158\n",
      "Epoch 70/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0312 - accuracy: 0.9929 - val_loss: 0.1322 - val_accuracy: 0.9158\n",
      "Epoch 71/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0342 - accuracy: 0.9894 - val_loss: 0.1348 - val_accuracy: 0.9263\n",
      "Epoch 72/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0404 - accuracy: 0.9800 - val_loss: 0.1371 - val_accuracy: 0.9263\n",
      "Epoch 73/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0458 - accuracy: 0.9823 - val_loss: 0.1366 - val_accuracy: 0.9263\n",
      "Epoch 74/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0394 - accuracy: 0.9870 - val_loss: 0.1316 - val_accuracy: 0.9158\n",
      "Epoch 75/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0339 - accuracy: 0.9894 - val_loss: 0.1329 - val_accuracy: 0.9158\n",
      "Epoch 76/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0524 - accuracy: 0.9752 - val_loss: 0.1352 - val_accuracy: 0.9263\n",
      "Epoch 77/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0305 - accuracy: 0.9906 - val_loss: 0.1354 - val_accuracy: 0.9263\n",
      "Epoch 78/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0476 - accuracy: 0.9800 - val_loss: 0.1332 - val_accuracy: 0.9158\n",
      "Epoch 79/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0259 - accuracy: 0.9917 - val_loss: 0.1322 - val_accuracy: 0.9158\n",
      "Epoch 80/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0425 - accuracy: 0.9847 - val_loss: 0.1318 - val_accuracy: 0.9158\n",
      "Epoch 81/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0464 - accuracy: 0.9847 - val_loss: 0.1342 - val_accuracy: 0.9263\n",
      "Epoch 82/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0591 - accuracy: 0.9729 - val_loss: 0.1336 - val_accuracy: 0.9263\n",
      "Epoch 83/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0328 - accuracy: 0.9917 - val_loss: 0.1342 - val_accuracy: 0.9263\n",
      "Epoch 84/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0436 - accuracy: 0.9823 - val_loss: 0.1383 - val_accuracy: 0.9263\n",
      "Epoch 85/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0345 - accuracy: 0.9929 - val_loss: 0.1333 - val_accuracy: 0.9263\n",
      "Epoch 86/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0427 - accuracy: 0.9823 - val_loss: 0.1316 - val_accuracy: 0.9158\n",
      "Epoch 87/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0435 - accuracy: 0.9870 - val_loss: 0.1345 - val_accuracy: 0.9158\n",
      "Epoch 88/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0668 - accuracy: 0.9752 - val_loss: 0.1337 - val_accuracy: 0.9158\n",
      "Epoch 89/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0487 - accuracy: 0.9800 - val_loss: 0.1346 - val_accuracy: 0.9263\n",
      "Epoch 90/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0439 - accuracy: 0.9811 - val_loss: 0.1350 - val_accuracy: 0.9263\n",
      "Epoch 91/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0467 - accuracy: 0.9847 - val_loss: 0.1346 - val_accuracy: 0.9263\n",
      "Epoch 92/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0486 - accuracy: 0.9835 - val_loss: 0.1339 - val_accuracy: 0.9263\n",
      "Epoch 93/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0471 - accuracy: 0.9811 - val_loss: 0.1337 - val_accuracy: 0.9263\n",
      "Epoch 94/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0508 - accuracy: 0.9847 - val_loss: 0.1319 - val_accuracy: 0.9158\n",
      "Epoch 95/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0368 - accuracy: 0.9858 - val_loss: 0.1319 - val_accuracy: 0.9158\n",
      "Epoch 96/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0474 - accuracy: 0.9811 - val_loss: 0.1383 - val_accuracy: 0.9263\n",
      "Epoch 97/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0606 - accuracy: 0.9623 - val_loss: 0.1336 - val_accuracy: 0.9263\n",
      "Epoch 98/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0612 - accuracy: 0.9705 - val_loss: 0.1349 - val_accuracy: 0.9263\n",
      "Epoch 99/100\n",
      "212/212 [==============================] - 20s 96ms/step - loss: 0.0269 - accuracy: 0.9941 - val_loss: 0.1345 - val_accuracy: 0.9263\n",
      "Epoch 100/100\n",
      "212/212 [==============================] - 20s 95ms/step - loss: 0.0432 - accuracy: 0.9870 - val_loss: 0.1345 - val_accuracy: 0.9263\n",
      "Score for fold 3: loss of 0.13448110222816467; accuracy of 92.63157844543457%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 31s 124ms/step - loss: 0.4578 - accuracy: 0.6784 - val_loss: 1.2086 - val_accuracy: 0.3191\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.3274 - accuracy: 0.7715 - val_loss: 2.6217 - val_accuracy: 0.3617\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.2900 - accuracy: 0.8245 - val_loss: 0.8472 - val_accuracy: 0.7553\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.2401 - accuracy: 0.8634 - val_loss: 0.3398 - val_accuracy: 0.8404\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.2163 - accuracy: 0.8740 - val_loss: 0.6434 - val_accuracy: 0.6702\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.2325 - accuracy: 0.8657 - val_loss: 0.6523 - val_accuracy: 0.6277\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1998 - accuracy: 0.8987 - val_loss: 1.3764 - val_accuracy: 0.6489\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1545 - accuracy: 0.9164 - val_loss: 1.7528 - val_accuracy: 0.5745\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1659 - accuracy: 0.9128 - val_loss: 0.2934 - val_accuracy: 0.8723\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1740 - accuracy: 0.9105 - val_loss: 0.2805 - val_accuracy: 0.8617\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1371 - accuracy: 0.9352 - val_loss: 0.5160 - val_accuracy: 0.7872\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1130 - accuracy: 0.9411 - val_loss: 0.2591 - val_accuracy: 0.8936\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1379 - accuracy: 0.9270 - val_loss: 0.6938 - val_accuracy: 0.6170\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0989 - accuracy: 0.9564 - val_loss: 0.2850 - val_accuracy: 0.8936\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0949 - accuracy: 0.9505 - val_loss: 0.5675 - val_accuracy: 0.8191\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1032 - accuracy: 0.9564 - val_loss: 0.3129 - val_accuracy: 0.9043\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0789 - accuracy: 0.9729 - val_loss: 0.3179 - val_accuracy: 0.8191\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0581 - accuracy: 0.9729 - val_loss: 0.3862 - val_accuracy: 0.8511\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0566 - accuracy: 0.9812 - val_loss: 0.3396 - val_accuracy: 0.8617\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0961 - accuracy: 0.9588 - val_loss: 0.8958 - val_accuracy: 0.6596\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0563 - accuracy: 0.9717 - val_loss: 0.2390 - val_accuracy: 0.8723\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0566 - accuracy: 0.9741 - val_loss: 0.2088 - val_accuracy: 0.8617\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0647 - accuracy: 0.9694 - val_loss: 0.2063 - val_accuracy: 0.8617\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0656 - accuracy: 0.9776 - val_loss: 0.2191 - val_accuracy: 0.8723\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0600 - accuracy: 0.9753 - val_loss: 0.2165 - val_accuracy: 0.8723\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0460 - accuracy: 0.9835 - val_loss: 0.2306 - val_accuracy: 0.8511\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0519 - accuracy: 0.9788 - val_loss: 0.2257 - val_accuracy: 0.8617\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0496 - accuracy: 0.9835 - val_loss: 0.2654 - val_accuracy: 0.8617\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0451 - accuracy: 0.9847 - val_loss: 0.2415 - val_accuracy: 0.8617\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0381 - accuracy: 0.9906 - val_loss: 0.2553 - val_accuracy: 0.8617\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0520 - accuracy: 0.9776 - val_loss: 0.2517 - val_accuracy: 0.8617\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0460 - accuracy: 0.9859 - val_loss: 0.2704 - val_accuracy: 0.8617\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0493 - accuracy: 0.9800 - val_loss: 0.2763 - val_accuracy: 0.8617\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0516 - accuracy: 0.9776 - val_loss: 0.2626 - val_accuracy: 0.8617\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0539 - accuracy: 0.9800 - val_loss: 0.2728 - val_accuracy: 0.8617\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0492 - accuracy: 0.9764 - val_loss: 0.2509 - val_accuracy: 0.8511\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0517 - accuracy: 0.9847 - val_loss: 0.2466 - val_accuracy: 0.8617\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0532 - accuracy: 0.9800 - val_loss: 0.2419 - val_accuracy: 0.8617\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0484 - accuracy: 0.9800 - val_loss: 0.2446 - val_accuracy: 0.8617\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0388 - accuracy: 0.9859 - val_loss: 0.2451 - val_accuracy: 0.8723\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0532 - accuracy: 0.9788 - val_loss: 0.2510 - val_accuracy: 0.8511\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0410 - accuracy: 0.9859 - val_loss: 0.2438 - val_accuracy: 0.8617\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0439 - accuracy: 0.9823 - val_loss: 0.2489 - val_accuracy: 0.8511\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0429 - accuracy: 0.9835 - val_loss: 0.2496 - val_accuracy: 0.8511\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0516 - accuracy: 0.9788 - val_loss: 0.2465 - val_accuracy: 0.8511\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0467 - accuracy: 0.9800 - val_loss: 0.2420 - val_accuracy: 0.8511\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0556 - accuracy: 0.9741 - val_loss: 0.2491 - val_accuracy: 0.8723\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0571 - accuracy: 0.9717 - val_loss: 0.2436 - val_accuracy: 0.8511\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0362 - accuracy: 0.9870 - val_loss: 0.2508 - val_accuracy: 0.8511\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0492 - accuracy: 0.9823 - val_loss: 0.2480 - val_accuracy: 0.8511\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0499 - accuracy: 0.9788 - val_loss: 0.2500 - val_accuracy: 0.8617\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0451 - accuracy: 0.9859 - val_loss: 0.2494 - val_accuracy: 0.8511\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0485 - accuracy: 0.9812 - val_loss: 0.2436 - val_accuracy: 0.8617\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0424 - accuracy: 0.9847 - val_loss: 0.2421 - val_accuracy: 0.8511\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0549 - accuracy: 0.9776 - val_loss: 0.2480 - val_accuracy: 0.8617\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0574 - accuracy: 0.9717 - val_loss: 0.2446 - val_accuracy: 0.8511\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0532 - accuracy: 0.9764 - val_loss: 0.2390 - val_accuracy: 0.8617\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0607 - accuracy: 0.9694 - val_loss: 0.2359 - val_accuracy: 0.8511\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0496 - accuracy: 0.9741 - val_loss: 0.2375 - val_accuracy: 0.8511\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0676 - accuracy: 0.9682 - val_loss: 0.2386 - val_accuracy: 0.8511\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0368 - accuracy: 0.9835 - val_loss: 0.2433 - val_accuracy: 0.8617\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0406 - accuracy: 0.9847 - val_loss: 0.2665 - val_accuracy: 0.8617\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0570 - accuracy: 0.9741 - val_loss: 0.2404 - val_accuracy: 0.8511\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0506 - accuracy: 0.9788 - val_loss: 0.2417 - val_accuracy: 0.8511\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0349 - accuracy: 0.9882 - val_loss: 0.2413 - val_accuracy: 0.8511\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0294 - accuracy: 0.9941 - val_loss: 0.2425 - val_accuracy: 0.8617\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0520 - accuracy: 0.9812 - val_loss: 0.2484 - val_accuracy: 0.8617\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0389 - accuracy: 0.9882 - val_loss: 0.2482 - val_accuracy: 0.8617\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0620 - accuracy: 0.9706 - val_loss: 0.2435 - val_accuracy: 0.8617\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0382 - accuracy: 0.9929 - val_loss: 0.2470 - val_accuracy: 0.8617\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0502 - accuracy: 0.9800 - val_loss: 0.2413 - val_accuracy: 0.8511\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0385 - accuracy: 0.9882 - val_loss: 0.2424 - val_accuracy: 0.8617\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0458 - accuracy: 0.9823 - val_loss: 0.2409 - val_accuracy: 0.8511\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0477 - accuracy: 0.9788 - val_loss: 0.2539 - val_accuracy: 0.8617\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0556 - accuracy: 0.9741 - val_loss: 0.2529 - val_accuracy: 0.8723\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0440 - accuracy: 0.9788 - val_loss: 0.2454 - val_accuracy: 0.8511\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0645 - accuracy: 0.9682 - val_loss: 0.2515 - val_accuracy: 0.8617\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0531 - accuracy: 0.9764 - val_loss: 0.2436 - val_accuracy: 0.8617\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0483 - accuracy: 0.9823 - val_loss: 0.2377 - val_accuracy: 0.8617\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0403 - accuracy: 0.9918 - val_loss: 0.2524 - val_accuracy: 0.8617\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0479 - accuracy: 0.9847 - val_loss: 0.2459 - val_accuracy: 0.8617\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0308 - accuracy: 0.9918 - val_loss: 0.2417 - val_accuracy: 0.8617\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0395 - accuracy: 0.9823 - val_loss: 0.2391 - val_accuracy: 0.8511\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0372 - accuracy: 0.9906 - val_loss: 0.2386 - val_accuracy: 0.8617\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0417 - accuracy: 0.9859 - val_loss: 0.2397 - val_accuracy: 0.8617\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0392 - accuracy: 0.9859 - val_loss: 0.2479 - val_accuracy: 0.8723\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0406 - accuracy: 0.9882 - val_loss: 0.2413 - val_accuracy: 0.8511\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0443 - accuracy: 0.9835 - val_loss: 0.2535 - val_accuracy: 0.8617\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0725 - accuracy: 0.9576 - val_loss: 0.2484 - val_accuracy: 0.8617\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0424 - accuracy: 0.9835 - val_loss: 0.2507 - val_accuracy: 0.8617\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0516 - accuracy: 0.9741 - val_loss: 0.2407 - val_accuracy: 0.8511\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0484 - accuracy: 0.9823 - val_loss: 0.2496 - val_accuracy: 0.8617\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0500 - accuracy: 0.9823 - val_loss: 0.2440 - val_accuracy: 0.8723\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0511 - accuracy: 0.9800 - val_loss: 0.2448 - val_accuracy: 0.8617\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0570 - accuracy: 0.9741 - val_loss: 0.2496 - val_accuracy: 0.8617\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0354 - accuracy: 0.9894 - val_loss: 0.2509 - val_accuracy: 0.8617\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0391 - accuracy: 0.9847 - val_loss: 0.2587 - val_accuracy: 0.8617\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0526 - accuracy: 0.9764 - val_loss: 0.2517 - val_accuracy: 0.8617\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0437 - accuracy: 0.9847 - val_loss: 0.2510 - val_accuracy: 0.8617\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0518 - accuracy: 0.9823 - val_loss: 0.2456 - val_accuracy: 0.8617\n",
      "Score for fold 4: loss of 0.24563166499137878; accuracy of 86.17021441459656%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 28s 107ms/step - loss: 0.4495 - accuracy: 0.7032 - val_loss: 2.9305 - val_accuracy: 0.3191\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.3222 - accuracy: 0.8021 - val_loss: 2.0320 - val_accuracy: 0.3723\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.2640 - accuracy: 0.8551 - val_loss: 0.8830 - val_accuracy: 0.6702\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.2382 - accuracy: 0.8693 - val_loss: 0.3245 - val_accuracy: 0.8191\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.2483 - accuracy: 0.8563 - val_loss: 0.4312 - val_accuracy: 0.7234\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2227 - accuracy: 0.8775 - val_loss: 0.3234 - val_accuracy: 0.8298\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.2119 - accuracy: 0.8963 - val_loss: 0.8945 - val_accuracy: 0.7021\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1815 - accuracy: 0.9105 - val_loss: 0.8201 - val_accuracy: 0.6064\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1784 - accuracy: 0.9022 - val_loss: 0.2142 - val_accuracy: 0.8404\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1289 - accuracy: 0.9364 - val_loss: 0.2605 - val_accuracy: 0.8191\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.1355 - accuracy: 0.9270 - val_loss: 0.1957 - val_accuracy: 0.8511\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1246 - accuracy: 0.9388 - val_loss: 0.3809 - val_accuracy: 0.8085\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1412 - accuracy: 0.9329 - val_loss: 0.2458 - val_accuracy: 0.8511\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.1062 - accuracy: 0.9552 - val_loss: 0.2497 - val_accuracy: 0.8511\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1184 - accuracy: 0.9399 - val_loss: 0.2936 - val_accuracy: 0.8298\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.1094 - accuracy: 0.9494 - val_loss: 0.2910 - val_accuracy: 0.8191\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.1063 - accuracy: 0.9564 - val_loss: 0.3939 - val_accuracy: 0.7766\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0764 - accuracy: 0.9670 - val_loss: 0.3075 - val_accuracy: 0.8617\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0937 - accuracy: 0.9611 - val_loss: 0.6321 - val_accuracy: 0.7872\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0747 - accuracy: 0.9717 - val_loss: 0.3469 - val_accuracy: 0.8511\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0895 - accuracy: 0.9729 - val_loss: 0.2362 - val_accuracy: 0.8404\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0570 - accuracy: 0.9823 - val_loss: 0.2422 - val_accuracy: 0.8404\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0502 - accuracy: 0.9882 - val_loss: 0.2485 - val_accuracy: 0.8404\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0526 - accuracy: 0.9812 - val_loss: 0.2526 - val_accuracy: 0.8298\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0535 - accuracy: 0.9812 - val_loss: 0.2431 - val_accuracy: 0.8404\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0727 - accuracy: 0.9741 - val_loss: 0.2538 - val_accuracy: 0.8404\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0519 - accuracy: 0.9764 - val_loss: 0.2519 - val_accuracy: 0.8404\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0568 - accuracy: 0.9847 - val_loss: 0.2637 - val_accuracy: 0.8404\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0460 - accuracy: 0.9847 - val_loss: 0.2516 - val_accuracy: 0.8404\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0469 - accuracy: 0.9847 - val_loss: 0.2616 - val_accuracy: 0.8511\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0407 - accuracy: 0.9859 - val_loss: 0.2577 - val_accuracy: 0.8511\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0539 - accuracy: 0.9812 - val_loss: 0.2717 - val_accuracy: 0.8404\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 20s 93ms/step - loss: 0.0531 - accuracy: 0.9776 - val_loss: 0.2605 - val_accuracy: 0.8511\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0515 - accuracy: 0.9812 - val_loss: 0.2608 - val_accuracy: 0.8404\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0573 - accuracy: 0.9823 - val_loss: 0.2586 - val_accuracy: 0.8511\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0521 - accuracy: 0.9788 - val_loss: 0.2589 - val_accuracy: 0.8511\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0483 - accuracy: 0.9835 - val_loss: 0.2613 - val_accuracy: 0.8511\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0433 - accuracy: 0.9847 - val_loss: 0.2578 - val_accuracy: 0.8511\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0608 - accuracy: 0.9706 - val_loss: 0.2641 - val_accuracy: 0.8298\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0336 - accuracy: 0.9906 - val_loss: 0.2631 - val_accuracy: 0.8511\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0607 - accuracy: 0.9764 - val_loss: 0.2642 - val_accuracy: 0.8511\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0511 - accuracy: 0.9847 - val_loss: 0.2652 - val_accuracy: 0.8511\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0464 - accuracy: 0.9835 - val_loss: 0.2648 - val_accuracy: 0.8511\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0536 - accuracy: 0.9823 - val_loss: 0.2632 - val_accuracy: 0.8511\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0460 - accuracy: 0.9753 - val_loss: 0.2633 - val_accuracy: 0.8511\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0515 - accuracy: 0.9823 - val_loss: 0.2690 - val_accuracy: 0.8511\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0499 - accuracy: 0.9835 - val_loss: 0.2657 - val_accuracy: 0.8511\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0552 - accuracy: 0.9741 - val_loss: 0.2651 - val_accuracy: 0.8511\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0441 - accuracy: 0.9859 - val_loss: 0.2681 - val_accuracy: 0.8298\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0442 - accuracy: 0.9847 - val_loss: 0.2688 - val_accuracy: 0.8404\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0476 - accuracy: 0.9788 - val_loss: 0.2692 - val_accuracy: 0.8404\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0452 - accuracy: 0.9870 - val_loss: 0.2638 - val_accuracy: 0.8511\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0486 - accuracy: 0.9776 - val_loss: 0.2663 - val_accuracy: 0.8404\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0466 - accuracy: 0.9812 - val_loss: 0.2640 - val_accuracy: 0.8404\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0481 - accuracy: 0.9823 - val_loss: 0.2655 - val_accuracy: 0.8511\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0446 - accuracy: 0.9882 - val_loss: 0.2641 - val_accuracy: 0.8511\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0461 - accuracy: 0.9823 - val_loss: 0.2629 - val_accuracy: 0.8511\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0354 - accuracy: 0.9906 - val_loss: 0.2645 - val_accuracy: 0.8511\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0363 - accuracy: 0.9906 - val_loss: 0.2647 - val_accuracy: 0.8511\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0613 - accuracy: 0.9682 - val_loss: 0.2663 - val_accuracy: 0.8511\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0328 - accuracy: 0.9929 - val_loss: 0.2655 - val_accuracy: 0.8404\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0377 - accuracy: 0.9906 - val_loss: 0.2630 - val_accuracy: 0.8511\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0463 - accuracy: 0.9800 - val_loss: 0.2649 - val_accuracy: 0.8511\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0416 - accuracy: 0.9859 - val_loss: 0.2654 - val_accuracy: 0.8511\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0371 - accuracy: 0.9870 - val_loss: 0.2672 - val_accuracy: 0.8511\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0551 - accuracy: 0.9800 - val_loss: 0.2635 - val_accuracy: 0.8511\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0394 - accuracy: 0.9882 - val_loss: 0.2625 - val_accuracy: 0.8511\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0505 - accuracy: 0.9800 - val_loss: 0.2651 - val_accuracy: 0.8404\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0529 - accuracy: 0.9788 - val_loss: 0.2650 - val_accuracy: 0.8511\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0526 - accuracy: 0.9847 - val_loss: 0.2680 - val_accuracy: 0.8511\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0431 - accuracy: 0.9847 - val_loss: 0.2677 - val_accuracy: 0.8298\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0610 - accuracy: 0.9729 - val_loss: 0.2667 - val_accuracy: 0.8511\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0652 - accuracy: 0.9682 - val_loss: 0.2658 - val_accuracy: 0.8511\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0373 - accuracy: 0.9918 - val_loss: 0.2668 - val_accuracy: 0.8511\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0547 - accuracy: 0.9788 - val_loss: 0.2712 - val_accuracy: 0.8404\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0461 - accuracy: 0.9870 - val_loss: 0.2674 - val_accuracy: 0.8511\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0480 - accuracy: 0.9859 - val_loss: 0.2678 - val_accuracy: 0.8511\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0472 - accuracy: 0.9823 - val_loss: 0.2655 - val_accuracy: 0.8511\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0497 - accuracy: 0.9776 - val_loss: 0.2658 - val_accuracy: 0.8511\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0479 - accuracy: 0.9800 - val_loss: 0.2652 - val_accuracy: 0.8404\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0340 - accuracy: 0.9882 - val_loss: 0.2642 - val_accuracy: 0.8511\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0607 - accuracy: 0.9753 - val_loss: 0.2641 - val_accuracy: 0.8511\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0369 - accuracy: 0.9882 - val_loss: 0.2648 - val_accuracy: 0.8511\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0411 - accuracy: 0.9859 - val_loss: 0.2648 - val_accuracy: 0.8511\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0517 - accuracy: 0.9764 - val_loss: 0.2662 - val_accuracy: 0.8404\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 20s 93ms/step - loss: 0.0521 - accuracy: 0.9800 - val_loss: 0.2669 - val_accuracy: 0.8511\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0498 - accuracy: 0.9800 - val_loss: 0.2608 - val_accuracy: 0.8511\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0377 - accuracy: 0.9918 - val_loss: 0.2665 - val_accuracy: 0.8511\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0290 - accuracy: 0.9929 - val_loss: 0.2685 - val_accuracy: 0.8511\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0463 - accuracy: 0.9859 - val_loss: 0.2662 - val_accuracy: 0.8404\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 20s 93ms/step - loss: 0.0491 - accuracy: 0.9788 - val_loss: 0.2691 - val_accuracy: 0.8404\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0335 - accuracy: 0.9929 - val_loss: 0.2657 - val_accuracy: 0.8511\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0421 - accuracy: 0.9870 - val_loss: 0.2670 - val_accuracy: 0.8511\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0540 - accuracy: 0.9729 - val_loss: 0.2694 - val_accuracy: 0.8511\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0361 - accuracy: 0.9929 - val_loss: 0.2650 - val_accuracy: 0.8511\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0506 - accuracy: 0.9800 - val_loss: 0.2653 - val_accuracy: 0.8511\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0568 - accuracy: 0.9764 - val_loss: 0.2655 - val_accuracy: 0.8511\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0470 - accuracy: 0.9812 - val_loss: 0.2641 - val_accuracy: 0.8511\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0324 - accuracy: 0.9882 - val_loss: 0.2720 - val_accuracy: 0.8404\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0498 - accuracy: 0.9847 - val_loss: 0.2661 - val_accuracy: 0.8511\n",
      "Score for fold 5: loss of 0.26605919003486633; accuracy of 85.10638475418091%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 27s 103ms/step - loss: 0.4478 - accuracy: 0.6867 - val_loss: 1.2263 - val_accuracy: 0.3191\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.3354 - accuracy: 0.7892 - val_loss: 1.0518 - val_accuracy: 0.6277\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.3316 - accuracy: 0.8068 - val_loss: 0.3142 - val_accuracy: 0.7872\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.2554 - accuracy: 0.8398 - val_loss: 0.2636 - val_accuracy: 0.8936\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.2340 - accuracy: 0.8634 - val_loss: 0.3684 - val_accuracy: 0.7766\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2140 - accuracy: 0.8763 - val_loss: 0.3375 - val_accuracy: 0.7234\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.1948 - accuracy: 0.8987 - val_loss: 0.4922 - val_accuracy: 0.6915\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1838 - accuracy: 0.8928 - val_loss: 0.1898 - val_accuracy: 0.9043\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1827 - accuracy: 0.8940 - val_loss: 0.3526 - val_accuracy: 0.7872\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.1725 - accuracy: 0.9011 - val_loss: 0.2254 - val_accuracy: 0.8830\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.1448 - accuracy: 0.9246 - val_loss: 0.3105 - val_accuracy: 0.8511\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.1285 - accuracy: 0.9411 - val_loss: 1.5528 - val_accuracy: 0.6064\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1581 - accuracy: 0.9211 - val_loss: 1.2823 - val_accuracy: 0.5957\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1175 - accuracy: 0.9482 - val_loss: 0.2373 - val_accuracy: 0.8936\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.1041 - accuracy: 0.9564 - val_loss: 0.3544 - val_accuracy: 0.7766\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.1146 - accuracy: 0.9435 - val_loss: 0.2930 - val_accuracy: 0.7872\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0751 - accuracy: 0.9706 - val_loss: 0.4450 - val_accuracy: 0.7660\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.1015 - accuracy: 0.9482 - val_loss: 0.3770 - val_accuracy: 0.8723\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.1035 - accuracy: 0.9517 - val_loss: 0.3113 - val_accuracy: 0.7979\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.0650 - accuracy: 0.9764 - val_loss: 0.2325 - val_accuracy: 0.9043\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0634 - accuracy: 0.9741 - val_loss: 0.1555 - val_accuracy: 0.9149\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0512 - accuracy: 0.9776 - val_loss: 0.1655 - val_accuracy: 0.9149\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0537 - accuracy: 0.9800 - val_loss: 0.1702 - val_accuracy: 0.9043\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0507 - accuracy: 0.9847 - val_loss: 0.1660 - val_accuracy: 0.9043\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0715 - accuracy: 0.9658 - val_loss: 0.1747 - val_accuracy: 0.9043\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0558 - accuracy: 0.9812 - val_loss: 0.1675 - val_accuracy: 0.9255\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0417 - accuracy: 0.9870 - val_loss: 0.1656 - val_accuracy: 0.9043\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0613 - accuracy: 0.9694 - val_loss: 0.1693 - val_accuracy: 0.8936\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0413 - accuracy: 0.9859 - val_loss: 0.1704 - val_accuracy: 0.9043\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0427 - accuracy: 0.9835 - val_loss: 0.1701 - val_accuracy: 0.9043\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0447 - accuracy: 0.9847 - val_loss: 0.1687 - val_accuracy: 0.9149\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.0265 - accuracy: 0.9953 - val_loss: 0.1697 - val_accuracy: 0.9149\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0507 - accuracy: 0.9823 - val_loss: 0.1719 - val_accuracy: 0.9149\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0617 - accuracy: 0.9753 - val_loss: 0.1731 - val_accuracy: 0.9043\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0508 - accuracy: 0.9823 - val_loss: 0.1742 - val_accuracy: 0.9043\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0310 - accuracy: 0.9906 - val_loss: 0.1757 - val_accuracy: 0.9043\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0436 - accuracy: 0.9870 - val_loss: 0.1781 - val_accuracy: 0.9043\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0583 - accuracy: 0.9776 - val_loss: 0.1754 - val_accuracy: 0.9043\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0418 - accuracy: 0.9859 - val_loss: 0.1728 - val_accuracy: 0.9043\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0486 - accuracy: 0.9847 - val_loss: 0.1721 - val_accuracy: 0.9043\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0384 - accuracy: 0.9859 - val_loss: 0.1706 - val_accuracy: 0.9149\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0438 - accuracy: 0.9823 - val_loss: 0.1714 - val_accuracy: 0.9043\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0423 - accuracy: 0.9847 - val_loss: 0.1702 - val_accuracy: 0.9149\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0391 - accuracy: 0.9823 - val_loss: 0.1697 - val_accuracy: 0.9149\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0401 - accuracy: 0.9859 - val_loss: 0.1701 - val_accuracy: 0.9149\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0544 - accuracy: 0.9776 - val_loss: 0.1700 - val_accuracy: 0.9149\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0391 - accuracy: 0.9882 - val_loss: 0.1700 - val_accuracy: 0.9149\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.0436 - accuracy: 0.9812 - val_loss: 0.1698 - val_accuracy: 0.9149\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0492 - accuracy: 0.9764 - val_loss: 0.1694 - val_accuracy: 0.9149\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0382 - accuracy: 0.9906 - val_loss: 0.1720 - val_accuracy: 0.9043\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0315 - accuracy: 0.9953 - val_loss: 0.1690 - val_accuracy: 0.9149\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0546 - accuracy: 0.9682 - val_loss: 0.1705 - val_accuracy: 0.9149\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0417 - accuracy: 0.9823 - val_loss: 0.1712 - val_accuracy: 0.9149\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0447 - accuracy: 0.9823 - val_loss: 0.1716 - val_accuracy: 0.9149\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0426 - accuracy: 0.9835 - val_loss: 0.1705 - val_accuracy: 0.9149\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0324 - accuracy: 0.9918 - val_loss: 0.1706 - val_accuracy: 0.9149\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0356 - accuracy: 0.9894 - val_loss: 0.1707 - val_accuracy: 0.9149\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0433 - accuracy: 0.9847 - val_loss: 0.1715 - val_accuracy: 0.9149\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0493 - accuracy: 0.9753 - val_loss: 0.1716 - val_accuracy: 0.9149\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0307 - accuracy: 0.9965 - val_loss: 0.1715 - val_accuracy: 0.9149\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0362 - accuracy: 0.9894 - val_loss: 0.1713 - val_accuracy: 0.9149\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0443 - accuracy: 0.9835 - val_loss: 0.1729 - val_accuracy: 0.9149\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0467 - accuracy: 0.9835 - val_loss: 0.1715 - val_accuracy: 0.9149\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0558 - accuracy: 0.9706 - val_loss: 0.1714 - val_accuracy: 0.9149\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0422 - accuracy: 0.9835 - val_loss: 0.1726 - val_accuracy: 0.9043\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0441 - accuracy: 0.9764 - val_loss: 0.1706 - val_accuracy: 0.9149\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0388 - accuracy: 0.9835 - val_loss: 0.1720 - val_accuracy: 0.9043\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0464 - accuracy: 0.9776 - val_loss: 0.1719 - val_accuracy: 0.9043\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0371 - accuracy: 0.9870 - val_loss: 0.1716 - val_accuracy: 0.9043\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0412 - accuracy: 0.9870 - val_loss: 0.1706 - val_accuracy: 0.9149\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0409 - accuracy: 0.9882 - val_loss: 0.1705 - val_accuracy: 0.9043\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0503 - accuracy: 0.9764 - val_loss: 0.1720 - val_accuracy: 0.9149\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0384 - accuracy: 0.9835 - val_loss: 0.1714 - val_accuracy: 0.9149\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0386 - accuracy: 0.9870 - val_loss: 0.1698 - val_accuracy: 0.9149\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0326 - accuracy: 0.9929 - val_loss: 0.1712 - val_accuracy: 0.9149\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0342 - accuracy: 0.9906 - val_loss: 0.1708 - val_accuracy: 0.9149\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0291 - accuracy: 0.9894 - val_loss: 0.1711 - val_accuracy: 0.9149\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0438 - accuracy: 0.9847 - val_loss: 0.1719 - val_accuracy: 0.9149\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0387 - accuracy: 0.9847 - val_loss: 0.1723 - val_accuracy: 0.9043\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0417 - accuracy: 0.9859 - val_loss: 0.1734 - val_accuracy: 0.9043\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0312 - accuracy: 0.9906 - val_loss: 0.1716 - val_accuracy: 0.9149\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0520 - accuracy: 0.9800 - val_loss: 0.1732 - val_accuracy: 0.9043\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0394 - accuracy: 0.9859 - val_loss: 0.1721 - val_accuracy: 0.9149\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0511 - accuracy: 0.9753 - val_loss: 0.1712 - val_accuracy: 0.9149\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0415 - accuracy: 0.9847 - val_loss: 0.1708 - val_accuracy: 0.9149\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0396 - accuracy: 0.9835 - val_loss: 0.1722 - val_accuracy: 0.9149\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0391 - accuracy: 0.9894 - val_loss: 0.1704 - val_accuracy: 0.9149\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0455 - accuracy: 0.9847 - val_loss: 0.1716 - val_accuracy: 0.9149\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0411 - accuracy: 0.9870 - val_loss: 0.1713 - val_accuracy: 0.9149\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0290 - accuracy: 0.9918 - val_loss: 0.1726 - val_accuracy: 0.9043\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0547 - accuracy: 0.9753 - val_loss: 0.1713 - val_accuracy: 0.9149\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0403 - accuracy: 0.9870 - val_loss: 0.1718 - val_accuracy: 0.9149\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0558 - accuracy: 0.9788 - val_loss: 0.1705 - val_accuracy: 0.9149\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0337 - accuracy: 0.9870 - val_loss: 0.1711 - val_accuracy: 0.9149\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0597 - accuracy: 0.9741 - val_loss: 0.1710 - val_accuracy: 0.9149\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0473 - accuracy: 0.9776 - val_loss: 0.1708 - val_accuracy: 0.9149\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0522 - accuracy: 0.9835 - val_loss: 0.1703 - val_accuracy: 0.9149\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0459 - accuracy: 0.9788 - val_loss: 0.1713 - val_accuracy: 0.9149\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0323 - accuracy: 0.9918 - val_loss: 0.1722 - val_accuracy: 0.9149\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0469 - accuracy: 0.9776 - val_loss: 0.1730 - val_accuracy: 0.9043\n",
      "Score for fold 6: loss of 0.1730063408613205; accuracy of 90.42553305625916%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 27s 101ms/step - loss: 0.4586 - accuracy: 0.6643 - val_loss: 1.6662 - val_accuracy: 0.3404\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3381 - accuracy: 0.7951 - val_loss: 1.6214 - val_accuracy: 0.1702\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3067 - accuracy: 0.8104 - val_loss: 0.8842 - val_accuracy: 0.4681\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2565 - accuracy: 0.8433 - val_loss: 0.6070 - val_accuracy: 0.7660\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2348 - accuracy: 0.8634 - val_loss: 0.6397 - val_accuracy: 0.7128\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2006 - accuracy: 0.8928 - val_loss: 2.4076 - val_accuracy: 0.3617\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1993 - accuracy: 0.8869 - val_loss: 0.2330 - val_accuracy: 0.8830\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1606 - accuracy: 0.9152 - val_loss: 0.2716 - val_accuracy: 0.8830\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1881 - accuracy: 0.8881 - val_loss: 0.2776 - val_accuracy: 0.7979\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1233 - accuracy: 0.9435 - val_loss: 0.4448 - val_accuracy: 0.8617\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1308 - accuracy: 0.9364 - val_loss: 1.2297 - val_accuracy: 0.5106\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1189 - accuracy: 0.9423 - val_loss: 0.2241 - val_accuracy: 0.8511\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0944 - accuracy: 0.9611 - val_loss: 0.5446 - val_accuracy: 0.7872\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1332 - accuracy: 0.9352 - val_loss: 0.2946 - val_accuracy: 0.7447\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1190 - accuracy: 0.9529 - val_loss: 0.5150 - val_accuracy: 0.7340\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0966 - accuracy: 0.9541 - val_loss: 0.6340 - val_accuracy: 0.7447\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0614 - accuracy: 0.9717 - val_loss: 0.2673 - val_accuracy: 0.8723\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1213 - accuracy: 0.9435 - val_loss: 0.3832 - val_accuracy: 0.8191\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0676 - accuracy: 0.9741 - val_loss: 0.1868 - val_accuracy: 0.8936\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0583 - accuracy: 0.9823 - val_loss: 0.2203 - val_accuracy: 0.8830\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0549 - accuracy: 0.9800 - val_loss: 0.2229 - val_accuracy: 0.8936\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0389 - accuracy: 0.9929 - val_loss: 0.1941 - val_accuracy: 0.9149\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0389 - accuracy: 0.9870 - val_loss: 0.1995 - val_accuracy: 0.8936\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0500 - accuracy: 0.9847 - val_loss: 0.2043 - val_accuracy: 0.9043\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0909 - accuracy: 0.9564 - val_loss: 0.2097 - val_accuracy: 0.9149\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0565 - accuracy: 0.9764 - val_loss: 0.2083 - val_accuracy: 0.9149\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0466 - accuracy: 0.9812 - val_loss: 0.2031 - val_accuracy: 0.9149\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0505 - accuracy: 0.9812 - val_loss: 0.1979 - val_accuracy: 0.9149\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0606 - accuracy: 0.9741 - val_loss: 0.1947 - val_accuracy: 0.9149\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0610 - accuracy: 0.9729 - val_loss: 0.2069 - val_accuracy: 0.9255\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0405 - accuracy: 0.9859 - val_loss: 0.2026 - val_accuracy: 0.9149\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0571 - accuracy: 0.9682 - val_loss: 0.2004 - val_accuracy: 0.8830\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0327 - accuracy: 0.9894 - val_loss: 0.2001 - val_accuracy: 0.9149\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0390 - accuracy: 0.9847 - val_loss: 0.1943 - val_accuracy: 0.9149\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0575 - accuracy: 0.9717 - val_loss: 0.1970 - val_accuracy: 0.8830\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0447 - accuracy: 0.9776 - val_loss: 0.1995 - val_accuracy: 0.9043\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0578 - accuracy: 0.9753 - val_loss: 0.2024 - val_accuracy: 0.8830\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0495 - accuracy: 0.9776 - val_loss: 0.2050 - val_accuracy: 0.8830\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0544 - accuracy: 0.9753 - val_loss: 0.1988 - val_accuracy: 0.8936\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0532 - accuracy: 0.9788 - val_loss: 0.1958 - val_accuracy: 0.8830\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0365 - accuracy: 0.9859 - val_loss: 0.2006 - val_accuracy: 0.8936\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0323 - accuracy: 0.9882 - val_loss: 0.2048 - val_accuracy: 0.9043\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0360 - accuracy: 0.9882 - val_loss: 0.2027 - val_accuracy: 0.8936\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0501 - accuracy: 0.9788 - val_loss: 0.2005 - val_accuracy: 0.8936\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0556 - accuracy: 0.9753 - val_loss: 0.1995 - val_accuracy: 0.8830\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0583 - accuracy: 0.9753 - val_loss: 0.2026 - val_accuracy: 0.8936\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0447 - accuracy: 0.9847 - val_loss: 0.1988 - val_accuracy: 0.8936\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0570 - accuracy: 0.9753 - val_loss: 0.2049 - val_accuracy: 0.8936\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0375 - accuracy: 0.9847 - val_loss: 0.1978 - val_accuracy: 0.8936\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0277 - accuracy: 0.9929 - val_loss: 0.1997 - val_accuracy: 0.8936\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0463 - accuracy: 0.9847 - val_loss: 0.2028 - val_accuracy: 0.8936\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0477 - accuracy: 0.9776 - val_loss: 0.2028 - val_accuracy: 0.9043\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0388 - accuracy: 0.9894 - val_loss: 0.2044 - val_accuracy: 0.8936\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0495 - accuracy: 0.9800 - val_loss: 0.2059 - val_accuracy: 0.8936\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0384 - accuracy: 0.9870 - val_loss: 0.2029 - val_accuracy: 0.8936\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0490 - accuracy: 0.9812 - val_loss: 0.1997 - val_accuracy: 0.8830\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0453 - accuracy: 0.9812 - val_loss: 0.2031 - val_accuracy: 0.8936\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0483 - accuracy: 0.9812 - val_loss: 0.2020 - val_accuracy: 0.8830\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0368 - accuracy: 0.9882 - val_loss: 0.2044 - val_accuracy: 0.8936\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0483 - accuracy: 0.9776 - val_loss: 0.2029 - val_accuracy: 0.8936\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0436 - accuracy: 0.9812 - val_loss: 0.2017 - val_accuracy: 0.8936\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0414 - accuracy: 0.9835 - val_loss: 0.2016 - val_accuracy: 0.8936\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0646 - accuracy: 0.9694 - val_loss: 0.2009 - val_accuracy: 0.8936\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0469 - accuracy: 0.9812 - val_loss: 0.2023 - val_accuracy: 0.8936\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0388 - accuracy: 0.9870 - val_loss: 0.2009 - val_accuracy: 0.8936\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0495 - accuracy: 0.9753 - val_loss: 0.2035 - val_accuracy: 0.8936\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0299 - accuracy: 0.9929 - val_loss: 0.2012 - val_accuracy: 0.8936\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0504 - accuracy: 0.9776 - val_loss: 0.2063 - val_accuracy: 0.8936\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0389 - accuracy: 0.9847 - val_loss: 0.2053 - val_accuracy: 0.8936\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0451 - accuracy: 0.9823 - val_loss: 0.2031 - val_accuracy: 0.9043\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0410 - accuracy: 0.9847 - val_loss: 0.2042 - val_accuracy: 0.8936\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0571 - accuracy: 0.9741 - val_loss: 0.2042 - val_accuracy: 0.8936\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0399 - accuracy: 0.9823 - val_loss: 0.2069 - val_accuracy: 0.9043\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0491 - accuracy: 0.9812 - val_loss: 0.2007 - val_accuracy: 0.8936\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0376 - accuracy: 0.9859 - val_loss: 0.2035 - val_accuracy: 0.8936\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0343 - accuracy: 0.9882 - val_loss: 0.2048 - val_accuracy: 0.8936\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0413 - accuracy: 0.9870 - val_loss: 0.2039 - val_accuracy: 0.8936\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0667 - accuracy: 0.9635 - val_loss: 0.2043 - val_accuracy: 0.8936\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0552 - accuracy: 0.9741 - val_loss: 0.2032 - val_accuracy: 0.8936\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0469 - accuracy: 0.9800 - val_loss: 0.2050 - val_accuracy: 0.8936\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0536 - accuracy: 0.9800 - val_loss: 0.2074 - val_accuracy: 0.8936\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0489 - accuracy: 0.9800 - val_loss: 0.2083 - val_accuracy: 0.8936\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0499 - accuracy: 0.9729 - val_loss: 0.2059 - val_accuracy: 0.9043\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0517 - accuracy: 0.9741 - val_loss: 0.2047 - val_accuracy: 0.8936\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0325 - accuracy: 0.9882 - val_loss: 0.2056 - val_accuracy: 0.8936\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0395 - accuracy: 0.9859 - val_loss: 0.2021 - val_accuracy: 0.8936\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0460 - accuracy: 0.9835 - val_loss: 0.2042 - val_accuracy: 0.8936\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0477 - accuracy: 0.9776 - val_loss: 0.2036 - val_accuracy: 0.8936\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0290 - accuracy: 0.9929 - val_loss: 0.2032 - val_accuracy: 0.8936\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0329 - accuracy: 0.9906 - val_loss: 0.2017 - val_accuracy: 0.8830\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0511 - accuracy: 0.9753 - val_loss: 0.2069 - val_accuracy: 0.8936\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0397 - accuracy: 0.9823 - val_loss: 0.2063 - val_accuracy: 0.8936\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0519 - accuracy: 0.9788 - val_loss: 0.2038 - val_accuracy: 0.8936\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0622 - accuracy: 0.9694 - val_loss: 0.2044 - val_accuracy: 0.8936\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0463 - accuracy: 0.9859 - val_loss: 0.2055 - val_accuracy: 0.8936\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0534 - accuracy: 0.9812 - val_loss: 0.2040 - val_accuracy: 0.8936\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0612 - accuracy: 0.9741 - val_loss: 0.2074 - val_accuracy: 0.8936\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0318 - accuracy: 0.9918 - val_loss: 0.2024 - val_accuracy: 0.8936\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0477 - accuracy: 0.9800 - val_loss: 0.2024 - val_accuracy: 0.8830\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0458 - accuracy: 0.9788 - val_loss: 0.2053 - val_accuracy: 0.8936\n",
      "Score for fold 7: loss of 0.2053007036447525; accuracy of 89.3617033958435%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 27s 101ms/step - loss: 0.4524 - accuracy: 0.6737 - val_loss: 1.5688 - val_accuracy: 0.3298\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.3404 - accuracy: 0.8033 - val_loss: 1.0607 - val_accuracy: 0.5957\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.3040 - accuracy: 0.8174 - val_loss: 0.1934 - val_accuracy: 0.9255\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.2740 - accuracy: 0.8174 - val_loss: 0.1543 - val_accuracy: 0.9468\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.2540 - accuracy: 0.8539 - val_loss: 0.2283 - val_accuracy: 0.9043\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2217 - accuracy: 0.8704 - val_loss: 0.9306 - val_accuracy: 0.5957\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1909 - accuracy: 0.8916 - val_loss: 0.6893 - val_accuracy: 0.6596\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1904 - accuracy: 0.8975 - val_loss: 0.7707 - val_accuracy: 0.6383\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1744 - accuracy: 0.9069 - val_loss: 1.3352 - val_accuracy: 0.5638\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1495 - accuracy: 0.9199 - val_loss: 0.5728 - val_accuracy: 0.7872\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.1416 - accuracy: 0.9246 - val_loss: 0.1154 - val_accuracy: 0.9681\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1302 - accuracy: 0.9258 - val_loss: 0.1303 - val_accuracy: 0.9255\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1077 - accuracy: 0.9482 - val_loss: 0.2507 - val_accuracy: 0.8085\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1240 - accuracy: 0.9446 - val_loss: 0.2742 - val_accuracy: 0.7979\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1119 - accuracy: 0.9423 - val_loss: 0.4958 - val_accuracy: 0.7340\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1095 - accuracy: 0.9411 - val_loss: 0.2072 - val_accuracy: 0.8936\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.1074 - accuracy: 0.9517 - val_loss: 0.4063 - val_accuracy: 0.8191\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0805 - accuracy: 0.9670 - val_loss: 0.2133 - val_accuracy: 0.8830\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0916 - accuracy: 0.9564 - val_loss: 0.1389 - val_accuracy: 0.9149\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0929 - accuracy: 0.9552 - val_loss: 0.1285 - val_accuracy: 0.9362\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0666 - accuracy: 0.9753 - val_loss: 0.1342 - val_accuracy: 0.9574\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0726 - accuracy: 0.9682 - val_loss: 0.1511 - val_accuracy: 0.9362\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0513 - accuracy: 0.9882 - val_loss: 0.1384 - val_accuracy: 0.9574\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0450 - accuracy: 0.9870 - val_loss: 0.1403 - val_accuracy: 0.9468\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0703 - accuracy: 0.9658 - val_loss: 0.1362 - val_accuracy: 0.9362\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0542 - accuracy: 0.9741 - val_loss: 0.1408 - val_accuracy: 0.9149\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0594 - accuracy: 0.9670 - val_loss: 0.1242 - val_accuracy: 0.9468\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0642 - accuracy: 0.9658 - val_loss: 0.1521 - val_accuracy: 0.9362\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0461 - accuracy: 0.9847 - val_loss: 0.1433 - val_accuracy: 0.9468\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0538 - accuracy: 0.9764 - val_loss: 0.1278 - val_accuracy: 0.9468\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0444 - accuracy: 0.9882 - val_loss: 0.1377 - val_accuracy: 0.9468\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0519 - accuracy: 0.9788 - val_loss: 0.1480 - val_accuracy: 0.9681\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0442 - accuracy: 0.9847 - val_loss: 0.1387 - val_accuracy: 0.9681\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0515 - accuracy: 0.9823 - val_loss: 0.1266 - val_accuracy: 0.9468\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0493 - accuracy: 0.9823 - val_loss: 0.1251 - val_accuracy: 0.9681\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0564 - accuracy: 0.9706 - val_loss: 0.1476 - val_accuracy: 0.9681\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0488 - accuracy: 0.9835 - val_loss: 0.1413 - val_accuracy: 0.9574\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0450 - accuracy: 0.9859 - val_loss: 0.1371 - val_accuracy: 0.9362\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0518 - accuracy: 0.9764 - val_loss: 0.1227 - val_accuracy: 0.9362\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0621 - accuracy: 0.9717 - val_loss: 0.1286 - val_accuracy: 0.9681\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0359 - accuracy: 0.9859 - val_loss: 0.1250 - val_accuracy: 0.9681\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0467 - accuracy: 0.9835 - val_loss: 0.1251 - val_accuracy: 0.9574\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0464 - accuracy: 0.9859 - val_loss: 0.1238 - val_accuracy: 0.9574\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0518 - accuracy: 0.9823 - val_loss: 0.1215 - val_accuracy: 0.9574\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0332 - accuracy: 0.9941 - val_loss: 0.1253 - val_accuracy: 0.9574\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0579 - accuracy: 0.9706 - val_loss: 0.1284 - val_accuracy: 0.9681\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0486 - accuracy: 0.9800 - val_loss: 0.1272 - val_accuracy: 0.9574\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0392 - accuracy: 0.9894 - val_loss: 0.1247 - val_accuracy: 0.9574\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0479 - accuracy: 0.9764 - val_loss: 0.1289 - val_accuracy: 0.9574\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0351 - accuracy: 0.9894 - val_loss: 0.1302 - val_accuracy: 0.9574\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0349 - accuracy: 0.9918 - val_loss: 0.1288 - val_accuracy: 0.9574\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0449 - accuracy: 0.9812 - val_loss: 0.1270 - val_accuracy: 0.9574\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0607 - accuracy: 0.9776 - val_loss: 0.1311 - val_accuracy: 0.9681\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0392 - accuracy: 0.9823 - val_loss: 0.1301 - val_accuracy: 0.9574\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0335 - accuracy: 0.9906 - val_loss: 0.1294 - val_accuracy: 0.9574\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0334 - accuracy: 0.9906 - val_loss: 0.1299 - val_accuracy: 0.9574\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0430 - accuracy: 0.9800 - val_loss: 0.1307 - val_accuracy: 0.9574\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0511 - accuracy: 0.9788 - val_loss: 0.1302 - val_accuracy: 0.9574\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0464 - accuracy: 0.9800 - val_loss: 0.1301 - val_accuracy: 0.9574\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0466 - accuracy: 0.9788 - val_loss: 0.1274 - val_accuracy: 0.9574\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0463 - accuracy: 0.9788 - val_loss: 0.1302 - val_accuracy: 0.9574\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0472 - accuracy: 0.9800 - val_loss: 0.1312 - val_accuracy: 0.9574\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0502 - accuracy: 0.9812 - val_loss: 0.1285 - val_accuracy: 0.9574\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0460 - accuracy: 0.9835 - val_loss: 0.1314 - val_accuracy: 0.9574\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0560 - accuracy: 0.9753 - val_loss: 0.1307 - val_accuracy: 0.9574\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0436 - accuracy: 0.9835 - val_loss: 0.1304 - val_accuracy: 0.9574\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0525 - accuracy: 0.9764 - val_loss: 0.1291 - val_accuracy: 0.9574\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0502 - accuracy: 0.9800 - val_loss: 0.1296 - val_accuracy: 0.9574\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0520 - accuracy: 0.9741 - val_loss: 0.1363 - val_accuracy: 0.9574\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0388 - accuracy: 0.9894 - val_loss: 0.1376 - val_accuracy: 0.9574\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0336 - accuracy: 0.9894 - val_loss: 0.1351 - val_accuracy: 0.9574\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0389 - accuracy: 0.9835 - val_loss: 0.1298 - val_accuracy: 0.9574\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0514 - accuracy: 0.9800 - val_loss: 0.1261 - val_accuracy: 0.9574\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0393 - accuracy: 0.9859 - val_loss: 0.1311 - val_accuracy: 0.9574\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0503 - accuracy: 0.9776 - val_loss: 0.1307 - val_accuracy: 0.9574\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0358 - accuracy: 0.9894 - val_loss: 0.1308 - val_accuracy: 0.9574\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0407 - accuracy: 0.9847 - val_loss: 0.1266 - val_accuracy: 0.9574\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0450 - accuracy: 0.9812 - val_loss: 0.1292 - val_accuracy: 0.9574\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0577 - accuracy: 0.9823 - val_loss: 0.1260 - val_accuracy: 0.9574\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0407 - accuracy: 0.9823 - val_loss: 0.1290 - val_accuracy: 0.9574\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0479 - accuracy: 0.9800 - val_loss: 0.1349 - val_accuracy: 0.9574\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0607 - accuracy: 0.9706 - val_loss: 0.1364 - val_accuracy: 0.9574\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0417 - accuracy: 0.9870 - val_loss: 0.1305 - val_accuracy: 0.9681\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0399 - accuracy: 0.9859 - val_loss: 0.1311 - val_accuracy: 0.9574\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.0423 - accuracy: 0.9800 - val_loss: 0.1273 - val_accuracy: 0.9574\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0417 - accuracy: 0.9859 - val_loss: 0.1298 - val_accuracy: 0.9574\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0547 - accuracy: 0.9753 - val_loss: 0.1324 - val_accuracy: 0.9574\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0535 - accuracy: 0.9835 - val_loss: 0.1353 - val_accuracy: 0.9574\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0406 - accuracy: 0.9882 - val_loss: 0.1299 - val_accuracy: 0.9574\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0588 - accuracy: 0.9717 - val_loss: 0.1293 - val_accuracy: 0.9574\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0672 - accuracy: 0.9682 - val_loss: 0.1357 - val_accuracy: 0.9574\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0245 - accuracy: 0.9953 - val_loss: 0.1297 - val_accuracy: 0.9574\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0630 - accuracy: 0.9694 - val_loss: 0.1309 - val_accuracy: 0.9574\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0472 - accuracy: 0.9788 - val_loss: 0.1326 - val_accuracy: 0.9574\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0485 - accuracy: 0.9776 - val_loss: 0.1320 - val_accuracy: 0.9681\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0431 - accuracy: 0.9847 - val_loss: 0.1278 - val_accuracy: 0.9574\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0343 - accuracy: 0.9906 - val_loss: 0.1280 - val_accuracy: 0.9574\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0673 - accuracy: 0.9682 - val_loss: 0.1318 - val_accuracy: 0.9574\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0449 - accuracy: 0.9812 - val_loss: 0.1265 - val_accuracy: 0.9574\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.0684 - accuracy: 0.9658 - val_loss: 0.1256 - val_accuracy: 0.9574\n",
      "Score for fold 8: loss of 0.1255864053964615; accuracy of 95.7446813583374%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 27s 102ms/step - loss: 0.4314 - accuracy: 0.6832 - val_loss: 1.7473 - val_accuracy: 0.3298\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.3123 - accuracy: 0.8127 - val_loss: 3.9333 - val_accuracy: 0.3298\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2877 - accuracy: 0.8351 - val_loss: 0.3524 - val_accuracy: 0.7234\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2583 - accuracy: 0.8575 - val_loss: 0.5700 - val_accuracy: 0.6809\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2505 - accuracy: 0.8481 - val_loss: 0.2298 - val_accuracy: 0.8723\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2018 - accuracy: 0.8940 - val_loss: 0.7561 - val_accuracy: 0.5745\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1934 - accuracy: 0.8857 - val_loss: 1.1836 - val_accuracy: 0.6277\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1974 - accuracy: 0.9034 - val_loss: 0.5823 - val_accuracy: 0.7234\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.1445 - accuracy: 0.9211 - val_loss: 0.4622 - val_accuracy: 0.7553\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.1363 - accuracy: 0.9282 - val_loss: 1.0726 - val_accuracy: 0.6915\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1224 - accuracy: 0.9399 - val_loss: 0.4218 - val_accuracy: 0.8085\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1339 - accuracy: 0.9246 - val_loss: 0.2873 - val_accuracy: 0.8191\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1217 - accuracy: 0.9446 - val_loss: 0.2568 - val_accuracy: 0.8298\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0887 - accuracy: 0.9529 - val_loss: 0.7483 - val_accuracy: 0.7128\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1175 - accuracy: 0.9435 - val_loss: 2.6355 - val_accuracy: 0.4468\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0998 - accuracy: 0.9600 - val_loss: 0.6365 - val_accuracy: 0.7660\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 22s 103ms/step - loss: 0.1052 - accuracy: 0.9399 - val_loss: 0.6615 - val_accuracy: 0.7340\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1115 - accuracy: 0.9529 - val_loss: 0.2007 - val_accuracy: 0.8936\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0812 - accuracy: 0.9635 - val_loss: 0.3009 - val_accuracy: 0.8830\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0604 - accuracy: 0.9717 - val_loss: 0.2528 - val_accuracy: 0.8617\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0824 - accuracy: 0.9682 - val_loss: 0.2201 - val_accuracy: 0.8830\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0415 - accuracy: 0.9870 - val_loss: 0.2167 - val_accuracy: 0.8830\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0580 - accuracy: 0.9776 - val_loss: 0.2237 - val_accuracy: 0.8936\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0670 - accuracy: 0.9694 - val_loss: 0.2189 - val_accuracy: 0.8830\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0417 - accuracy: 0.9882 - val_loss: 0.2187 - val_accuracy: 0.8830\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0451 - accuracy: 0.9823 - val_loss: 0.2266 - val_accuracy: 0.8830\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0636 - accuracy: 0.9741 - val_loss: 0.2289 - val_accuracy: 0.8830\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0414 - accuracy: 0.9882 - val_loss: 0.2265 - val_accuracy: 0.8830\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0782 - accuracy: 0.9647 - val_loss: 0.2331 - val_accuracy: 0.9043\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0741 - accuracy: 0.9682 - val_loss: 0.2222 - val_accuracy: 0.8830\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0511 - accuracy: 0.9823 - val_loss: 0.2256 - val_accuracy: 0.8830\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0455 - accuracy: 0.9847 - val_loss: 0.2288 - val_accuracy: 0.8936\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0526 - accuracy: 0.9776 - val_loss: 0.2247 - val_accuracy: 0.8830\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0492 - accuracy: 0.9812 - val_loss: 0.2248 - val_accuracy: 0.8830\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0600 - accuracy: 0.9753 - val_loss: 0.2318 - val_accuracy: 0.8830\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0550 - accuracy: 0.9741 - val_loss: 0.2404 - val_accuracy: 0.9043\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0534 - accuracy: 0.9764 - val_loss: 0.2238 - val_accuracy: 0.8830\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0479 - accuracy: 0.9800 - val_loss: 0.2313 - val_accuracy: 0.8830\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0394 - accuracy: 0.9859 - val_loss: 0.2299 - val_accuracy: 0.8830\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0477 - accuracy: 0.9812 - val_loss: 0.2335 - val_accuracy: 0.8830\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0611 - accuracy: 0.9776 - val_loss: 0.2308 - val_accuracy: 0.8830\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0572 - accuracy: 0.9788 - val_loss: 0.2308 - val_accuracy: 0.8830\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0519 - accuracy: 0.9788 - val_loss: 0.2335 - val_accuracy: 0.8830\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0627 - accuracy: 0.9717 - val_loss: 0.2298 - val_accuracy: 0.8830\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0529 - accuracy: 0.9776 - val_loss: 0.2353 - val_accuracy: 0.8830\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0594 - accuracy: 0.9764 - val_loss: 0.2282 - val_accuracy: 0.8830\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0559 - accuracy: 0.9741 - val_loss: 0.2304 - val_accuracy: 0.8830\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0513 - accuracy: 0.9776 - val_loss: 0.2278 - val_accuracy: 0.8830\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0376 - accuracy: 0.9847 - val_loss: 0.2300 - val_accuracy: 0.8830\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0527 - accuracy: 0.9729 - val_loss: 0.2318 - val_accuracy: 0.8830\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0626 - accuracy: 0.9729 - val_loss: 0.2314 - val_accuracy: 0.8830\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0353 - accuracy: 0.9906 - val_loss: 0.2268 - val_accuracy: 0.8830\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0447 - accuracy: 0.9835 - val_loss: 0.2301 - val_accuracy: 0.8830\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0470 - accuracy: 0.9800 - val_loss: 0.2306 - val_accuracy: 0.8830\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0590 - accuracy: 0.9741 - val_loss: 0.2295 - val_accuracy: 0.8830\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0661 - accuracy: 0.9717 - val_loss: 0.2370 - val_accuracy: 0.8830\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0652 - accuracy: 0.9694 - val_loss: 0.2365 - val_accuracy: 0.8830\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0532 - accuracy: 0.9717 - val_loss: 0.2302 - val_accuracy: 0.8830\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0549 - accuracy: 0.9776 - val_loss: 0.2313 - val_accuracy: 0.8830\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0490 - accuracy: 0.9823 - val_loss: 0.2315 - val_accuracy: 0.8830\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0409 - accuracy: 0.9847 - val_loss: 0.2339 - val_accuracy: 0.8830\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0475 - accuracy: 0.9788 - val_loss: 0.2322 - val_accuracy: 0.8830\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0584 - accuracy: 0.9729 - val_loss: 0.2338 - val_accuracy: 0.8830\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0601 - accuracy: 0.9717 - val_loss: 0.2345 - val_accuracy: 0.8830\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0369 - accuracy: 0.9823 - val_loss: 0.2305 - val_accuracy: 0.8830\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0529 - accuracy: 0.9776 - val_loss: 0.2321 - val_accuracy: 0.8830\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0489 - accuracy: 0.9812 - val_loss: 0.2311 - val_accuracy: 0.8830\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0405 - accuracy: 0.9823 - val_loss: 0.2317 - val_accuracy: 0.8830\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0619 - accuracy: 0.9717 - val_loss: 0.2322 - val_accuracy: 0.8830\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0572 - accuracy: 0.9764 - val_loss: 0.2319 - val_accuracy: 0.8830\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0442 - accuracy: 0.9823 - val_loss: 0.2310 - val_accuracy: 0.8830\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0374 - accuracy: 0.9870 - val_loss: 0.2332 - val_accuracy: 0.8830\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0335 - accuracy: 0.9882 - val_loss: 0.2325 - val_accuracy: 0.8830\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0602 - accuracy: 0.9706 - val_loss: 0.2319 - val_accuracy: 0.8830\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0477 - accuracy: 0.9800 - val_loss: 0.2295 - val_accuracy: 0.8830\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0422 - accuracy: 0.9800 - val_loss: 0.2376 - val_accuracy: 0.8830\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0430 - accuracy: 0.9870 - val_loss: 0.2321 - val_accuracy: 0.8830\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0632 - accuracy: 0.9717 - val_loss: 0.2334 - val_accuracy: 0.8830\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0368 - accuracy: 0.9847 - val_loss: 0.2283 - val_accuracy: 0.8830\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0641 - accuracy: 0.9694 - val_loss: 0.2311 - val_accuracy: 0.8830\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0565 - accuracy: 0.9729 - val_loss: 0.2321 - val_accuracy: 0.8830\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0493 - accuracy: 0.9800 - val_loss: 0.2351 - val_accuracy: 0.8830\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0678 - accuracy: 0.9658 - val_loss: 0.2352 - val_accuracy: 0.8830\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0466 - accuracy: 0.9823 - val_loss: 0.2322 - val_accuracy: 0.8830\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0541 - accuracy: 0.9741 - val_loss: 0.2313 - val_accuracy: 0.8830\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0538 - accuracy: 0.9741 - val_loss: 0.2311 - val_accuracy: 0.8830\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0490 - accuracy: 0.9823 - val_loss: 0.2312 - val_accuracy: 0.8830\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0502 - accuracy: 0.9823 - val_loss: 0.2299 - val_accuracy: 0.8830\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0651 - accuracy: 0.9658 - val_loss: 0.2322 - val_accuracy: 0.8830\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0341 - accuracy: 0.9882 - val_loss: 0.2303 - val_accuracy: 0.8830\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0382 - accuracy: 0.9823 - val_loss: 0.2303 - val_accuracy: 0.8830\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0531 - accuracy: 0.9753 - val_loss: 0.2327 - val_accuracy: 0.8830\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0505 - accuracy: 0.9776 - val_loss: 0.2336 - val_accuracy: 0.8830\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0542 - accuracy: 0.9753 - val_loss: 0.2343 - val_accuracy: 0.8830\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0614 - accuracy: 0.9741 - val_loss: 0.2330 - val_accuracy: 0.8830\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0710 - accuracy: 0.9658 - val_loss: 0.2315 - val_accuracy: 0.8830\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0491 - accuracy: 0.9812 - val_loss: 0.2313 - val_accuracy: 0.8830\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0426 - accuracy: 0.9835 - val_loss: 0.2320 - val_accuracy: 0.8830\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0437 - accuracy: 0.9812 - val_loss: 0.2347 - val_accuracy: 0.8830\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0391 - accuracy: 0.9870 - val_loss: 0.2327 - val_accuracy: 0.8830\n",
      "Score for fold 9: loss of 0.23268577456474304; accuracy of 88.29787373542786%\n",
      "(849,) (94,)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/100\n",
      "213/213 [==============================] - 27s 102ms/step - loss: 0.4537 - accuracy: 0.6796 - val_loss: 1.0141 - val_accuracy: 0.3298\n",
      "Epoch 2/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.3366 - accuracy: 0.7880 - val_loss: 2.3048 - val_accuracy: 0.3298\n",
      "Epoch 3/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.3130 - accuracy: 0.8115 - val_loss: 0.2161 - val_accuracy: 0.9043\n",
      "Epoch 4/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2822 - accuracy: 0.8410 - val_loss: 0.1752 - val_accuracy: 0.9043\n",
      "Epoch 5/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2367 - accuracy: 0.8693 - val_loss: 0.8897 - val_accuracy: 0.5213\n",
      "Epoch 6/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2350 - accuracy: 0.8728 - val_loss: 0.5993 - val_accuracy: 0.7340\n",
      "Epoch 7/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2123 - accuracy: 0.8916 - val_loss: 0.1332 - val_accuracy: 0.9043\n",
      "Epoch 8/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1836 - accuracy: 0.8963 - val_loss: 0.2432 - val_accuracy: 0.8511\n",
      "Epoch 9/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1915 - accuracy: 0.8987 - val_loss: 0.1982 - val_accuracy: 0.8723\n",
      "Epoch 10/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1497 - accuracy: 0.9317 - val_loss: 0.1827 - val_accuracy: 0.8936\n",
      "Epoch 11/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1467 - accuracy: 0.9364 - val_loss: 0.2222 - val_accuracy: 0.8936\n",
      "Epoch 12/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1318 - accuracy: 0.9317 - val_loss: 0.6982 - val_accuracy: 0.7128\n",
      "Epoch 13/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1255 - accuracy: 0.9388 - val_loss: 0.1897 - val_accuracy: 0.8936\n",
      "Epoch 14/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1112 - accuracy: 0.9446 - val_loss: 0.1517 - val_accuracy: 0.8936\n",
      "Epoch 15/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0904 - accuracy: 0.9635 - val_loss: 0.7226 - val_accuracy: 0.7872\n",
      "Epoch 16/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0861 - accuracy: 0.9588 - val_loss: 0.3554 - val_accuracy: 0.8298\n",
      "Epoch 17/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0781 - accuracy: 0.9706 - val_loss: 0.3860 - val_accuracy: 0.8617\n",
      "Epoch 18/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0840 - accuracy: 0.9670 - val_loss: 1.5057 - val_accuracy: 0.7021\n",
      "Epoch 19/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0871 - accuracy: 0.9694 - val_loss: 0.3778 - val_accuracy: 0.8191\n",
      "Epoch 20/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0952 - accuracy: 0.9588 - val_loss: 0.6088 - val_accuracy: 0.7447\n",
      "Epoch 21/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0934 - accuracy: 0.9600 - val_loss: 0.1803 - val_accuracy: 0.9149\n",
      "Epoch 22/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0751 - accuracy: 0.9753 - val_loss: 0.1840 - val_accuracy: 0.9149\n",
      "Epoch 23/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0644 - accuracy: 0.9800 - val_loss: 0.1665 - val_accuracy: 0.9043\n",
      "Epoch 24/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0617 - accuracy: 0.9812 - val_loss: 0.1785 - val_accuracy: 0.9043\n",
      "Epoch 25/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0659 - accuracy: 0.9788 - val_loss: 0.1889 - val_accuracy: 0.9043\n",
      "Epoch 26/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0666 - accuracy: 0.9764 - val_loss: 0.1722 - val_accuracy: 0.9043\n",
      "Epoch 27/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0469 - accuracy: 0.9870 - val_loss: 0.1662 - val_accuracy: 0.9043\n",
      "Epoch 28/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0702 - accuracy: 0.9753 - val_loss: 0.1709 - val_accuracy: 0.8830\n",
      "Epoch 29/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0647 - accuracy: 0.9741 - val_loss: 0.1865 - val_accuracy: 0.9043\n",
      "Epoch 30/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0542 - accuracy: 0.9800 - val_loss: 0.1899 - val_accuracy: 0.9149\n",
      "Epoch 31/100\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.0296 - accuracy: 0.9976 - val_loss: 0.1724 - val_accuracy: 0.9043\n",
      "Epoch 32/100\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.0639 - accuracy: 0.9788 - val_loss: 0.1748 - val_accuracy: 0.8830\n",
      "Epoch 33/100\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.0680 - accuracy: 0.9670 - val_loss: 0.1710 - val_accuracy: 0.9043\n",
      "Epoch 34/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0508 - accuracy: 0.9847 - val_loss: 0.1673 - val_accuracy: 0.9043\n",
      "Epoch 35/100\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.0542 - accuracy: 0.9823 - val_loss: 0.1848 - val_accuracy: 0.9043\n",
      "Epoch 36/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0521 - accuracy: 0.9788 - val_loss: 0.1680 - val_accuracy: 0.8936\n",
      "Epoch 37/100\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.0430 - accuracy: 0.9882 - val_loss: 0.1733 - val_accuracy: 0.8936\n",
      "Epoch 38/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0600 - accuracy: 0.9812 - val_loss: 0.1651 - val_accuracy: 0.9043\n",
      "Epoch 39/100\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.0506 - accuracy: 0.9812 - val_loss: 0.1697 - val_accuracy: 0.8830\n",
      "Epoch 40/100\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.0500 - accuracy: 0.9847 - val_loss: 0.1642 - val_accuracy: 0.8936\n",
      "Epoch 41/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0685 - accuracy: 0.9635 - val_loss: 0.1701 - val_accuracy: 0.8936\n",
      "Epoch 42/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0571 - accuracy: 0.9776 - val_loss: 0.1761 - val_accuracy: 0.9043\n",
      "Epoch 43/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0376 - accuracy: 0.9882 - val_loss: 0.1750 - val_accuracy: 0.9043\n",
      "Epoch 44/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0497 - accuracy: 0.9847 - val_loss: 0.1741 - val_accuracy: 0.9043\n",
      "Epoch 45/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0528 - accuracy: 0.9776 - val_loss: 0.1762 - val_accuracy: 0.9043\n",
      "Epoch 46/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0455 - accuracy: 0.9859 - val_loss: 0.1768 - val_accuracy: 0.9043\n",
      "Epoch 47/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0497 - accuracy: 0.9847 - val_loss: 0.1749 - val_accuracy: 0.9043\n",
      "Epoch 48/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0641 - accuracy: 0.9776 - val_loss: 0.1739 - val_accuracy: 0.9043\n",
      "Epoch 49/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0425 - accuracy: 0.9894 - val_loss: 0.1802 - val_accuracy: 0.9043\n",
      "Epoch 50/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0403 - accuracy: 0.9847 - val_loss: 0.1716 - val_accuracy: 0.8936\n",
      "Epoch 51/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0394 - accuracy: 0.9859 - val_loss: 0.1681 - val_accuracy: 0.8936\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0659 - accuracy: 0.9717 - val_loss: 0.1719 - val_accuracy: 0.8936\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0514 - accuracy: 0.9776 - val_loss: 0.1726 - val_accuracy: 0.8936\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0405 - accuracy: 0.9870 - val_loss: 0.1737 - val_accuracy: 0.9043\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0539 - accuracy: 0.9788 - val_loss: 0.1688 - val_accuracy: 0.8936\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0607 - accuracy: 0.9753 - val_loss: 0.1750 - val_accuracy: 0.9043\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0740 - accuracy: 0.9670 - val_loss: 0.1711 - val_accuracy: 0.8936\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0447 - accuracy: 0.9800 - val_loss: 0.1670 - val_accuracy: 0.8936\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0458 - accuracy: 0.9847 - val_loss: 0.1744 - val_accuracy: 0.9043\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0434 - accuracy: 0.9859 - val_loss: 0.1714 - val_accuracy: 0.8936\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0437 - accuracy: 0.9847 - val_loss: 0.1687 - val_accuracy: 0.8936\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0412 - accuracy: 0.9882 - val_loss: 0.1681 - val_accuracy: 0.8936\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0441 - accuracy: 0.9835 - val_loss: 0.1717 - val_accuracy: 0.8936\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0583 - accuracy: 0.9764 - val_loss: 0.1709 - val_accuracy: 0.8936\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0441 - accuracy: 0.9870 - val_loss: 0.1731 - val_accuracy: 0.8936\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0531 - accuracy: 0.9764 - val_loss: 0.1663 - val_accuracy: 0.8936\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0258 - accuracy: 0.9953 - val_loss: 0.1761 - val_accuracy: 0.9043\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0616 - accuracy: 0.9729 - val_loss: 0.1781 - val_accuracy: 0.9043\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0483 - accuracy: 0.9847 - val_loss: 0.1661 - val_accuracy: 0.8936\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - 22s 101ms/step - loss: 0.0463 - accuracy: 0.9812 - val_loss: 0.1735 - val_accuracy: 0.8936\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.0312 - accuracy: 0.9918 - val_loss: 0.1680 - val_accuracy: 0.8936\n",
      "Epoch 72/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0543 - accuracy: 0.9812 - val_loss: 0.1659 - val_accuracy: 0.8936\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0485 - accuracy: 0.9812 - val_loss: 0.1693 - val_accuracy: 0.8936\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0445 - accuracy: 0.9800 - val_loss: 0.1730 - val_accuracy: 0.8936\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0503 - accuracy: 0.9812 - val_loss: 0.1714 - val_accuracy: 0.8936\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0669 - accuracy: 0.9706 - val_loss: 0.1750 - val_accuracy: 0.9043\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0575 - accuracy: 0.9717 - val_loss: 0.1746 - val_accuracy: 0.8936\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0534 - accuracy: 0.9835 - val_loss: 0.1737 - val_accuracy: 0.8936\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0737 - accuracy: 0.9658 - val_loss: 0.1686 - val_accuracy: 0.8936\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0629 - accuracy: 0.9717 - val_loss: 0.1709 - val_accuracy: 0.8936\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0747 - accuracy: 0.9635 - val_loss: 0.1694 - val_accuracy: 0.8936\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0548 - accuracy: 0.9835 - val_loss: 0.1685 - val_accuracy: 0.8936\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0525 - accuracy: 0.9776 - val_loss: 0.1691 - val_accuracy: 0.8936\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0699 - accuracy: 0.9670 - val_loss: 0.1680 - val_accuracy: 0.8936\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0462 - accuracy: 0.9800 - val_loss: 0.1692 - val_accuracy: 0.8936\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0449 - accuracy: 0.9882 - val_loss: 0.1734 - val_accuracy: 0.8936\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0503 - accuracy: 0.9764 - val_loss: 0.1735 - val_accuracy: 0.8936\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0561 - accuracy: 0.9776 - val_loss: 0.1722 - val_accuracy: 0.8936\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0588 - accuracy: 0.9729 - val_loss: 0.1708 - val_accuracy: 0.8936\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0519 - accuracy: 0.9776 - val_loss: 0.1729 - val_accuracy: 0.8936\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0484 - accuracy: 0.9859 - val_loss: 0.1732 - val_accuracy: 0.8936\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0460 - accuracy: 0.9847 - val_loss: 0.1756 - val_accuracy: 0.8936\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0516 - accuracy: 0.9823 - val_loss: 0.1695 - val_accuracy: 0.8936\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0438 - accuracy: 0.9847 - val_loss: 0.1692 - val_accuracy: 0.8936\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0601 - accuracy: 0.9753 - val_loss: 0.1693 - val_accuracy: 0.8936\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.0497 - accuracy: 0.9800 - val_loss: 0.1727 - val_accuracy: 0.8936\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.0508 - accuracy: 0.9800 - val_loss: 0.1724 - val_accuracy: 0.8936\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0364 - accuracy: 0.9918 - val_loss: 0.1703 - val_accuracy: 0.8936\n",
      "Epoch 99/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0572 - accuracy: 0.9788 - val_loss: 0.1714 - val_accuracy: 0.8936\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.0530 - accuracy: 0.9753 - val_loss: 0.1666 - val_accuracy: 0.8936\n",
      "Score for fold 10: loss of 0.16660840809345245; accuracy of 89.3617033958435%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.22429415583610535 - Accuracy: 89.47368264198303%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.14988403022289276 - Accuracy: 90.52631855010986%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.13448110222816467 - Accuracy: 92.63157844543457%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.24563166499137878 - Accuracy: 86.17021441459656%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.26605919003486633 - Accuracy: 85.10638475418091%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.1730063408613205 - Accuracy: 90.42553305625916%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.2053007036447525 - Accuracy: 89.3617033958435%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.1255864053964615 - Accuracy: 95.7446813583374%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.23268577456474304 - Accuracy: 88.29787373542786%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.16660840809345245 - Accuracy: 89.3617033958435%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 89.70996737480164 (+- 2.866134785396498)\n",
      "> Loss: 0.1923537775874138\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    for train, test in kfold.split(X_train, y_train):\n",
    "        print(train.shape, test.shape)\n",
    "        \n",
    "        np.savetxt('E:/Result/ver.3.22/MTF/train_/' + f'train_{fold_no}.csv', train, delimiter=\",\")\n",
    "        np.savetxt('E:/Result/ver.3.22/MTF/test_/' + f'test_{fold_no}.csv', test, delimiter=\",\")\n",
    "\n",
    "        input = Input(shape=(300, 300, 2))\n",
    "        model = DenseNet121(input_tensor=input, include_top=False, weights=None, pooling='avg')\n",
    "        \n",
    "        x = model.output\n",
    "\n",
    "        x = Dense(3, activation='softmax', name='softmax', kernel_initializer='he_normal')(x)\n",
    "        model = Model(model.input, x)\n",
    "\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.SGD(lr=0.01, decay=1e-3, momentum=0.9, nesterov=True)\n",
    "        #optimizer = optimizers.Adam(lr=0.001)\n",
    "        \n",
    "        callbacks_list = [LearningRateSchedule([20,40])]\n",
    "        \n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        \n",
    "        history = model.fit(inputs[train], targets[train], \n",
    "                            batch_size=4, \n",
    "                            epochs=100, \n",
    "                            verbose=1,\n",
    "                            validation_data=(inputs[test], targets[test]),\n",
    "                            callbacks = tensorboard_callback) # 여기에 Validation set을 넣어야되는거 아닌가?\n",
    "        \n",
    "        scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "        print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "        acc_per_fold.append(scores[1] * 100)\n",
    "        loss_per_fold.append(scores[0])\n",
    "        \n",
    "        model.save('E:/Result/ver.3.22/MTF/weight_/' + f'MTF_{fold_no}.h5',fold_no)\n",
    "        \n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training','validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA0HElEQVR4nO3deXxc1ZXg8d+tUmkr7ZttSbblfd+NMTE7hDEkQCAQoOkkME3oEDIkmUz3kPQMJJkknaQZQjNZaCAkQBMS2qQJIaxOTIAABu94xQteZNmSLGuXar/zx32vVJKqpLJUsvxK5/v51Ke2V6/uqyedOnXevfcprTVCCCGczzXaDRBCCJEaEtCFECJNSEAXQog0IQFdCCHShAR0IYRIExmj9cZlZWW6pqZmtN5eCCEcaePGjSe01uXxnhu1gF5TU8OGDRtG6+2FEMKRlFKHEj0nJRchhEgTEtCFECJNSEAXQog0MWo1dCFEegkGg9TW1uLz+Ua7KWkhOzub6upqPB5P0q+RgC6ESIna2lry8/OpqalBKTXazXE0rTVNTU3U1tYyZcqUpF8nJRchREr4fD5KS0slmKeAUorS0tJT/rUjAV0IkTISzFNnKJ+lswN6OASbnoRIeLRbIoQQo87ZAf3w2/D8l+HIe6PdEiHEKGtpaeFnP/vZKb/uiiuuoKWlZcBl7rnnHtauXTvElp0+zg7ogS5zHfaPbjuEEKMuUUAPhUIDvu7FF1+kqKhowGW+853vcOmllw6neaeFswO6HcjDA+8wIUT6u/vuu9m/fz+LFy/mrLPO4rzzzuOqq65i7ty5AHzqU59i2bJlzJs3j4cffjj6upqaGk6cOMHBgweZM2cOX/jCF5g3bx6XXXYZ3d3dANxyyy2sWbMmuvy9997L0qVLWbBgAbt37wagsbGRj3/848ybN4/bbruNyZMnc+LEidP6GTi722IoYK4jEtCFOJN8+w872FnXltJ1zq0s4N4r5yV8/gc/+AHbt29ny5YtvP7663ziE59g+/bt0W5/jz32GCUlJXR3d3PWWWfx6U9/mtLS0l7r2Lt3L08//TSPPPIIn/nMZ3j22Wf527/9237vVVZWxqZNm/jZz37Gfffdx6OPPsq3v/1tLr74Yr7xjW/w8ssv84tf/CKl25+M9MjQJaALIfpYsWJFrz7cDz74IIsWLWLlypUcOXKEvXv39nvNlClTWLx4MQDLli3j4MGDcdd97bXX9lvmrbfe4sYbbwRg9erVFBcXp25jkuTwDN0O6MHRbYcQopeBMunTxev1Rm+//vrrrF27lnfeeYfc3FwuvPDCuH28s7Kyorfdbne05JJoObfbPWiN/nRyeIZul1yk26IQY11+fj7t7e1xn2ttbaW4uJjc3Fx2797Nu+++m/L3X7VqFc888wwAr776Ks3NzSl/j8GkSYZ+5nxDCiFGR2lpKatWrWL+/Pnk5OQwbty46HOrV6/moYceYs6cOcyaNYuVK1em/P3vvfdebrrpJp588knOOeccxo8fT35+fsrfZyBKa31a39C2fPlyPewTXPzlR7Due3DVT2DpZ1PTMCHEkOzatYs5c+aMdjNGjd/vx+12k5GRwTvvvMMdd9zBli1bhrXOeJ+pUmqj1np5vOUlQxdCiBQ4fPgwn/nMZ4hEImRmZvLII4+c9jY4O6BLLxchxBlixowZbN68eVTb4OyDotIPXQghopwd0CVDF0KIKGcHdMnQhRAiytkBXeZyEUKIKGcHdOnlIoQYory8PADq6uq47rrr4i5z4YUXMlj36gceeICurq7o/WSm4x0pzg7oYSm5CCGGp7KyMjqT4lD0DejJTMc7Upwd0GUuFyGE5e677+anP/1p9P63vvUtvvvd73LJJZdEp7r9/e9/3+91Bw8eZP78+QB0d3dz4403MmfOHK655ppec7nccccdLF++nHnz5nHvvfcCZsKvuro6LrroIi666CKgZzpegPvvv5/58+czf/58Hnjggej7JZqmd7gc3g9d5nIR4oz00t1w/IPUrnP8Arj8BwmfvuGGG/jqV7/KnXfeCcAzzzzDK6+8wl133UVBQQEnTpxg5cqVXHXVVQnP1/nzn/+c3Nxcdu3axbZt21i6dGn0ue9973uUlJQQDoe55JJL2LZtG3fddRf3338/69ato6ysrNe6Nm7cyC9/+UvWr1+P1pqzzz6bCy64gOLi4qSn6T1VaZKhS8lFiLFuyZIlNDQ0UFdXx9atWykuLmb8+PF885vfZOHChVx66aUcPXqU+vr6hOt44403ooF14cKFLFy4MPrcM888w9KlS1myZAk7duxg586dA7bnrbfe4pprrsHr9ZKXl8e1117Lm2++CSQ/Te+pSo8MPSwlFyHOKANk0iPp+uuvZ82aNRw/fpwbbriBp556isbGRjZu3IjH46GmpibutLmD+eijj7jvvvt4//33KS4u5pZbbhnSemzJTtN7qhyeoVsfqGToQghM2eU3v/kNa9as4frrr6e1tZWKigo8Hg/r1q3j0KFDA77+/PPP59e//jUA27dvZ9u2bQC0tbXh9XopLCykvr6el156KfqaRNP2nnfeeTz33HN0dXXR2dnJf/7nf3LeeeelcGv7c3aGHpIauhCix7x582hvb6eqqooJEyZw8803c+WVV7JgwQKWL1/O7NmzB3z9HXfcwa233sqcOXOYM2cOy5YtA2DRokUsWbKE2bNnM3HiRFatWhV9ze23387q1auprKxk3bp10ceXLl3KLbfcwooVKwC47bbbWLJkScrKK/E4e/rc+2ZCRz0svAGufXjw5YUQI2asT587Ek51+txBSy5KqYlKqXVKqZ1KqR1Kqa/EWUYppR5USu1TSm1TSi2Nt66Uk4OiQggRlUzJJQR8XWu9SSmVD2xUSr2mtY49xHs5MMO6nA383LoeWTKwSAghogbN0LXWx7TWm6zb7cAuoKrPYlcDT2jjXaBIKTUh5a3tKyRzuQhxJhmtEm46GspneUq9XJRSNcASYH2fp6qAIzH3a+kf9FFK3a6U2qCU2tDY2HiKTe0jEgZtHQyVDF2IUZednU1TU5ME9RTQWtPU1ER2dvYpvS7pXi5KqTzgWeCrWuu2U2wfAFrrh4GHwRwUHco6ouzsHGTovxBngOrqampraxl2siYA8wVZXV19Sq9JKqArpTyYYP6U1vp3cRY5CkyMuV9tPTZywrEBXTJ0IUabx+NhypQpo92MMS2ZXi4K+AWwS2t9f4LFngc+Z/V2WQm0aq2PpbCd/dl90EH6oQshBMll6KuAzwIfKKW2WI99E5gEoLV+CHgRuALYB3QBt6a8pX3FZugy9F8IIQYP6Frrt4D4U5P1LKOBO1PVqKT0ytCl5CKEEM6dy0Vq6EII0YtzA7rdy8WVIQFdCCFwckC3R4l6vBLQhRACJwd0O0PPlIAuhBDg5IBuZ+iZXhn6L4QQODmgRzP0XMnQhRACJwd0u5eL1NCFEAJwckAPxZRcZC4XIYRwcEAPx5ZcZOi/EEI4N6BHM/Q8KbkIIQRODujRGnquzOUihBA4OaBLLxchhOjFuQE9OlI0F9AQiYxqc4QQYrQ5N6CH/ODOBLfH3JeeLkKIMc65AT0cAHeWmZwLpOwihBjznBvQQ37IyJSALoQQFucG9LDfytCtkovM5yKEGOOcG9BDAStDd5v7kqELIcY45wb0aIYuJRchhAAnB3Q7Q5deLkIIATg5oPfL0GU+FyHE2ObcgB4KQEaW1NCFEMLi3IAetgYWRXu5SMlFCDG2OTegh/xWhi4HRYUQApwc0MMBK0OXgC6EEODkgB7ymQzdLQFdCCHA0QFd5nIRQohYzg3o4T5zuchBUSHEGOfcgB7N0O2BRdIPXQgxtjk3oEczdOmHLoQQ4NSArnWc+dCl5CKEGNucGdDt08/1mstFMnQhxNjmzIBunyBa5nIRQogoZwb0aIYeM5eL9HIRQoxxgwZ0pdRjSqkGpdT2BM9fqJRqVUptsS73pL6ZfUQz9Ji5XKTkIoQY45LJ0H8FrB5kmTe11outy3eG36xBhK2Ani5zuXSdlJKREGLYBg3oWus3gJOnoS3JC1kll3SYyyXQBQ8sgO2/G+2WCCEcLlU19HOUUluVUi8ppeYlWkgpdbtSaoNSakNjY+PQ3y02Q3f6XC7+Ngh0QNvR0W6JEMLhUhHQNwGTtdaLgP8HPJdoQa31w1rr5Vrr5eXl5UN/RztDz8h2foYe7DbX9nEBIYQYomEHdK11m9a6w7r9IuBRSpUNu2UDCcceFHX4XC52IA/5RrcdQgjHG3ZAV0qNV0op6/YKa51Nw13vgEKxB0UdPpeLHcglQxdCDFPGYAsopZ4GLgTKlFK1wL2AB0Br/RBwHXCHUioEdAM3aq31iLUYevqhu9NgLhfJ0IUQKTJoQNda3zTI8z8BfpKyFiUjNkNXCpTbuXO5RDN0CehCiOFx9khRd5Z17ZEMXQgx5jkzoEcz9Exz7cqQGroQYsxzZkAPx0zOBaaOLr1chBBjnDMDeihm+lwwPV0cW3KRDF0IkRrODOj9MvSMNAjokqELIYbHmQE9FDN9Ljj8oKgEdCFEajgzoIf9pqui3Qfd5U6DgC4lFyHE8DgzoIf8Pdk5OLzkIgdFhRCp4cyAHg6YUaI2l8fBvVwkQxdCpIYzA3rcDN2p/dAlQxdCpIYzA3o40NPDBawaumToQoixzZkBPeTv6YMODu/lEpOhj/CcZkKI9ObMgN4vQ3fyQdGYUos9R40QQgyBMwN63wzdlQFhhwb0YExAlzq6EGIYnBnQw/70zNClji6EGAZnBvRQoH+G7tiAHhPEJUMXQgyDMwN63Azd4b1cQDJ0IcSwODOghwK9+6G7Hd4PXVm7QTJ0IcQwODSg+/qMFHVyycUH2YXWbcnQhRBD58yAHg70Hynq2KH//piALhm6EGLonBnQQ/7+c7k4tuTik4AuhEgJZwb0cN+5XJw8fa5fSi5CiJRwZkAP9Z1t0aG9XLSGULdk6EKIlHBmQO+boTt1LpdICHREMnQhREo4L6CHrSDYrx+6A2vodkaeXdT7vhBCDIEDA7qVxfYaKep2Zi8XOyO3M/SgBHQhxNA5L6DbQbBXhu7Qkotk6EKIFHJeQLenmE2HuVyiGXpB7/tCCDEEzgvocTP0DEA7r45uZ+SeHNNrRzJ0IcQwOC+gRzP07J7H3Bnm2mlZuh3AM7LNRTJ0IcQwOC+gh+IdFHVqQLe3JctcJEMXQgyD8wJ6OFHJBef1dJEMXQiRQs4L6KF4B0U95tppNXS7m6Jk6EKIFHBeQI+bobvNteNKLpKhCyFSZ9CArpR6TCnVoJTanuB5pZR6UCm1Tym1TSm1NPXNjBHN0OOUXJw2n0uvGnq2ZOhCiGFJJkP/FbB6gOcvB2ZYl9uBnw+/WQOIZugxJRe3XXKRDF0IMXYNGtC11m8AJwdY5GrgCW28CxQppSakqoFxZeb17rYYPSjqtIBuZ+jZUkMXQgxbRgrWUQUciblfaz12rO+CSqnbMVk8kyZNGtq7zbnSXGKlTQ29YXTbI4RwtNN6UFRr/bDWernWenl5eXnqVuxyaslF+qELIVInFQH9KDAx5n619djp49iDoj7zZeRySw1dCDFsqQjozwOfs3q7rARatdb9yi0jKhrQHdYPPeTvORYgGboQYpgGraErpZ4GLgTKlFK1wL2AB0Br/RDwInAFsA/oAm4dqcYm5Ni5XLp7ul9Khi6EGKZBA7rW+qZBntfAnSlr0VA4dui/ZOhCiNRx3kjReBw7OZevd4Ye9psTRwshxBCkSUB36FwufTN0+zEhhBiCNAnodj90p5Vc+mTo9mNCCDEE6RHQBxr6rzX88etQt/n0tikZIb85WxFIhi6EGLZUjBQdfQPV0H0t8P6jkOmFyiWntVmDCvkgK9/clgxdCDFM6ZGhDzSXi7/DXLcc6f/caAv5pIYuhEiZ9Aro8TL0QKe5bj0TA7pfauhCiJQZAwH9DM7Qg7EZuh3QJUMXQgxNmgX0OL1c/O3muuN4zynfzhS9ernYJZfu0WuPEMLR0iOguwfoh26XXADaTu+cYYPq1Q9dMnQhxPCkR0AfaD50u+QC0HL49LQnWXEz9DPsV4QQwjHSJKAPMJeLXXKBM+vAqNZmqL9k6EKIFEmTgD7AwKLYksuZlKHHntwCwCO9XIQQw5NmA4vi1dA7AAUFlWdWT5fo6efskaIS0IUQw5MmGfoAc7n4O8xJpYsmnVkll74ZugwsEkIMU3oEdKVMlh635NIOWVZAPyMz9L41dMnQhRBDkx4BHQYI6J0mQy+caLotxpseYDREA7qVmbszrcclQxdCDE16BfREc7lkeqFoIugwtNedvjZpDSf2xn+ub4aulHUaOsnQhRBDk14BPVE/9Kx8k6HD6S27HPor/GQ51O/s/1y0hp7d81hGlmToQoghGxsBPTMPiiab+6k6MBoJQ6Br4GVarZGpJw/0f65vyQUkQxdCDEuaBfREvVy8UFht7qcqQ3/vEXhwycDnAPW3meuO4/2fkwxdCJFi6RPQ3Z7E/dCz8szAHW8FtBxKzfud+NAE6tipBfrytZjrjob+z0mGLoRIsfQJ6C53/KH/di8XMAdGU1Vy6T5pXbckXsbXaq7bJUMXQoy8NArocWrokTAEu3pO85bKvuhddkBvTryMHdA76vs/Jxm6ECLF0iige/oHdLsckuk114UTobUWIpHhv193igK6fZJosAK6ZOhCiKFJo4AeJ0O3J+aKllwmmRkOO+PUtE/VqWTo7XECejBehp4lGboQYsjSKKC7+wd0+wTRdskllX3RTyWgdzb0/1XQd2CRfVsydCHEEKVPQHfHK7lYc6HbJZeiSea6dZjT6Aa7e04Vl0xAj4Sgq6n3c3bgtof8Q/8MvW5z/OxeCCHiSJ+A7sro38vFztBje7nA8DN0OzuHwQN6bqm53beOHrJOEK1Uz2OxGbrW8MTV8OZ9w2urEGLMSK+A3rcful1Dz7ICelY+ZBVA+7HhvVd3TEC3+5r3pTX42qBslrnfd3BRyN+7fg69M/S2OvOF0HYa554RQjhamgX0RL1c8noeyy3pnWEPRTIZerDbjFwtn2nu9y2d2Bl6rNgMvcma1KtvqUYIIRJIs4Det+Ri19BjAnpOyfCDpJ2hZxcmHlhk18/LrIDer+QySIZuz9LYeWJ4bRVCjBlpFtATdFvMis3QS3uXTIbCztBLpiXO0O2Anj8BMvMT19BjZWRDOGB6xDTts95LAroQIjnpE9Dd8WroVsnF4+15LDeFGXppEgE9uwDyx/Uf/h83Q7cCfNhv5ooBs/4z5aQcQogzWvoE9ES9XDxecMVsZm4pdA3QMyUZXc1mvXnjkgjoRZA3vv8EXaHunhNE2+yAHuyGE/ti3k/q6EKIwSUV0JVSq5VSe5RS+5RSd8d5/halVKNSaot1uS31TR1EooOiseUWMDX0QDuEAkN/r+6TJtPPKTJzxcQbDBQN6IUmQ0+2lwuYL4nWI1Ax19yXsosQIgmDBnSllBv4KXA5MBe4SSk1N86iv9VaL7Yuj6a4nYNzxZk+1z65RazcEnM9nDp610nIKTYXiH9g1O7OmF1oMvlke7kANOwENEw6x9yXA6NCiCQkk6GvAPZprQ9orQPAb4CrR7ZZQ+Byx+nl0tEzStRmB/ThlDGiGbod0OOUXeyTW2QVmIAe7OwZ6AQDZ+jHt5trO6BLhi6ESEIyAb0KiB1aWWs91tenlVLblFJrlFIT461IKXW7UmqDUmpDY2PjEJo7gES9XOx5XGw5dkAfboY+SED3tZqM25MN+ePNY7E9XQbK0OvtgH62ue6UGroQYnCpOij6B6BGa70QeA14PN5CWuuHtdbLtdbLy8vLU/TWlkRzufQruVhD8Uc6Q/e1mnILmAwdevd0CfnjBHQ7Q//ATCRWUAUoydCFEElJJqAfBWIz7mrrsSitdZPW2j4y+CiwLDXNi88XjHOqOVdG/+59A5VchlpDj4RNzTy39NQDeuyB0ZAvcbfFlkNQOt2UkXJLoDPFv2aEEGkpmYD+PjBDKTVFKZUJ3Ag8H7uAUmpCzN2rgF2pa2Jvf9x2jEXffpW6lu7eT5xKLxcYesnF1wpos57sIuuxlvjL2QE9WnKJ6boYN0OPuV82w1znlslBUSFEUgYN6FrrEPBl4BVMoH5Ga71DKfUdpdRV1mJ3KaV2KKW2AncBt4xUg2eNz8cfivCnXX16jSSqoWf2qaF7sk0f8qEGdLtUk1tiDngqV+IMPavA3M4pNr1w2gfL0GPu21MGeMukH7oQIikZySyktX4ReLHPY/fE3P4G8I3UNi2+aeVeppR5Wburgc+eU9PzhD2Xi9ZmStpIJH6GDsMb/m9/EeSUmAFL2UWJA3rRZHNbKVN2sQ+KhkPmy2egDL10ek9bG/cMra1CiDHFcSNFlVJcMruCd/Y30eGPycjdHnOtrTMDBe3Tz/WpoQPkFg8967W/CHKt+nlO8eA1dLAGF1kBPXo+0QQHRaGn5OItl4OiQoikOC6gA1w6dxyBcIS39sYcLHS5zbVddul7PtFYuaXDKLnEZOiQfEDPG98zuMgeWZooQ/fkQn6lue0tM+/Zd9CUEEL04ciAvnxyMYU5Hl7bGXOQ0WVVj+z5XPqeTzTWcKbQjWboAwT0oM/MmtgvQ7dq6NHziSaooZdO75l/JrcM0MOfw10IkfYcGdAz3C4unFXOuj0NhCPaPOiySi7RDL3P+URjDbeG7sqIOeBZ1H/of+w8Lra8ceZLJByEva9Ybevz68HO0O1yC4DX7jcvZRchxMAcGdABLp0zjpOdATYftrJjO0O3SxMDllxKTNAdyrS03dY8Lva5QONl6IkCOsBL/wgvfA2mnA+zLu/9OrfHzJ8+8eyYtpaZa+m6KIQYhGMD+gWzyslwKdbussou0Rp635JLgho6DHyC50TsYf+2nGITwGNr3LFT59rsvugbHoPFN8PNz/b/9aAUfGUrnPWFnse81ohaydCFEINwbEAvyPZw9tQS1tr90d19Sy5xzidqs0d4DqWO3t3cUz+Prkv3BHGIn6GXzQR3Flz0v+Dqn0JGZvz1Z2T1nr/dKxm6ECI5jg3oAJfMHse+hg7W7qyPc1A0zvlEbdEMfQh19HgZOvTO9mOnzrWVToNvHoUL/qGnXJMM+70koAshBuHogH714kqmV+Rx2xMbeHpDnXmwbw09bsllGFPodp/s6YMO8edEjz39XCz7V8SpcGeY95CSixBiEI4O6KV5Wbzw387lixdM468fmSC666iVdQ9UconOuHiKGbrWSWbocUouwyHzuQghkpDU0P8zWbbHzd2Xz+aAdy78Gf7nf2ziS+6JrPa3m3N22gdLY+UMMUMPdpkTOMfW0ONN0OVrBXdm/4FDQyXzuQghkuDoDD3W1AqTDc8oz+aOpzax6/AxdLxyC0Bmrgn2p1pD7ztKFOJn6P42k52fSq18IF7J0IUQg0ubgG7Xp79/9RxWzxvPnsPHaItkJV4+t+TUSy59R4mCGVgE/UsuqSq3gCm5SA1dCDGI9AnoVmklS0X46d8sZWqB5minm7f3JwiEOUMI6PEydLfHTNE7kgHdLrlEIqlbpxAi7aRRQLdHioZwuRRzy9yEPV7uenozx1t9/ZfPHcJ8LtEMvbT3431Hi45Ehq4jQxsIJYQYM9IooPceWJQR7GRq1Ti6A2G+9NRGGtp8dAVCaG3N/ZJb0ruGvulJ2P67gd+jK07JBaz5XEY4QwcpuwghBuT4Xi5RMRk6AP4OvEWT+dF1i7jz15tY8f0/mcUUXL5gAv+SU0SunaEHuswcKyGfyb6nXhD/PaIll+Lej+cU9++HPhIBvfMElM9K3XqFEGkljQJ6nPnQs/L4xMIJlHhXsq+xg05/iOOtPn793mGeyGjj72mBcAi1/0+mS2JOMay5FW7/CxRN7P8eTfvMLIt9BwjlFEHD7p77saefS4VcydCFEINLn5KLXdfe/qwZABToiJ5P9JxppXx25WS+eME0vnXVPF76ynlkFZSj0HzlV68T2P57E8xvfQlCAXjms2ZO81jv/wI+eAYWfqb/e8fW0EN+k+mPSIbeOPByQogxLX0CevFkuPh/wwf/Aeu+n/h8osC08jw+f8lSAPYd2Edgx4u0Tb4MKubANQ9B3WaCv74JajeaF+z6A7z4P2Dmalj9w/4rtAO61uBrM4+l9KCo9WXVeZoGF2ltylBCCEdJn5ILwHlfh+aD8MaPzP14J7ewuKwTRzyy9Ah5H3Ry564aQk9uYOuRHC4Pfpb/8dEaPI9eDFXL4fgHULUMrvulmVulr9wyM23v0Y09I0djp84dLrfHfEEc3QDvPwrtx6F4Ciy4vvesjZEw1G6Ava+ak2j4Ws1UvMtvjX/mJlugE7b+BrY9A21HzblPwwGYcRlccDdUL0u+rd0tsOlxM0p28c0Jv1TPCOEQNH9kRvUWTUrdQLAzUSRinZg8wSyfIi2kV0BXCj75Y2g9Agdejz+Pi83qS1518FkimXm0lKzi8LE2VkwpoWz8Vzn/zxdxR+F6/mvnK7hKpsBNvzUjTONZcD2892/w1PVw2XfNY6nM0AEKJ1mB+lVAARpe/wGc9zUonw07/hN2/t4EY+WGSSvNl8pr/xvevA+WfBYql0DFXCioNIG7+SAcWQ8bHzdTF4xfADXnQl6Fec9NT8CjF8PUi6CwygRr+4Bv/gSznoJKM9e7txx2PAfrHzIjZQFe/2dY8fcw+Rxo2m8uABNXmPZ5K0x7Ww6ZX1Ql00xgdbnNL53GPdC4yxyfaNxl2qvc5ssiI8tc3B4TkAOdpm2+VvNlFAlZASzHfJll5YMnx5qSIQvaj5n126cDLKiCyR+DCYvMtuWNM8u210HbMdMjKhwwXwLhgHmdfW7YoklQMtV8Bk17oX6n+aLILTHrLag014XV5rNt2g9HN8HxbWZbveUmKcgtNa/JLQGP1yQPrgwzt3/7MXMJdvdscyTcs81uD4ybb9qflQf718H+P8HRzeb5QLv57KZeCPOugWkXm8+zbhM07jbnvC2baU5/6C0zE8tl5pt92VFvkoj241Y7jptutJlecwl29TwX6DTv43Kbttufd3YhlEwx+9hbZmZD9bWa13pyzb7JyDFfOG5rv9r7MBKO2d9ZPZ9/sMvsm9ZaaKs15VJizmDmyTbr9NiXXLM+f7u5hLrNurU2t7tbzP+BckHZLPOLvbjGtEW5THyxVk84AJ0N5jzB9rEt5TLbnJlnPr+sfLP+kN9MGeKtMJ9BcY35+xqBBEJFu/GdZsuXL9cbNmwYmZX7WmHtt+GcO820tfE0H4J/XWhuz/80XPdYr6df3n6ML/77Jj65YDwP3rAIV8Yg330nD8Bjq60BQCH4r6/CpLMHfs2paD1qzkmaX2kCwIF1JqAftT7DjGyTUc+9GqZf0tMTp3YDvPVj2PMS6DgnmlYumHMlrPySOVNS7B+Zvx3ee8QcP9ARs87sAvP5th3tPQe8bc5VcP4/mD/it34Me/7Y81xGDqB7gqg70/xjxHJnmfexz79qv658pgkGYB2n6Db/wGHrkplngkZ2gTWnfIYJLCGfCUq+tp4gHPab4DluPoybZ4LQob/CobdN8IpHuUyQcHvMuj055n0iEfNZxH623nLTVl8LtNX1fMH1XqEJnspljo0M9ZSIymUOwIcDJsDFKqiGmlVmW7PyzXbufsEE8ljectODK97fRzxZBSZgBzrN+7qsM23ljzcBXoetXwRBK/gGzP9F7D5NKWUCpCen56FIyHz52YFfR3ovn5Vv/mdcbitJyDQJUE6RmYK7cXfyx6yyC81+sLe5736IZ+WdsPr7p7CNMa1XaqPWennc59IyoCfD3w7/XG1uX/84zPtUv0V+/vp+fvjybj6xYALzqgoo82YxrSKPpZOKUPG+Xet3wC8vN4HuS+uhYvbIboPW8NEb5p9lxscHLqsEfSZ7bNhtss7CapMplEzt3w0zWYFOkyHZWdv4+SariXVirwl4pdPNl1EkZDLTw++a4Fk0yZSPMnNN5nriQ9M9s2w6lM8xn2HR5PiTrKWatk5U0lFvtikc7PklEnvawb7CQWg5DB0NJoGwf+HYfG0msLfWmvUW15hMOnZ65XDQZIhdTSa4B7tMdhcOms8mv7InYIaDJnAol/kiU8os27TffLbdzeYUh2Uz+7dZazi2xXx5lU6HyqWQV26CbvNHZh3dJ02b/e2mjXkVJmDGBu3Ydit375OyJBLohJMfmW3MLjQXT44JuoEuE4DDAfOFGw6ZfW5nx+FAT4cDe+I7T645+Xp+5cClJK1NO4PWr4fMvOTa23nC/NrXEROsdcT6PJV5fd44k3X3fe9I2HyJ+zvMl7/9S7K93nyZNn9kEonJHxu8DXFIQI9Ha/huhflj+Yf9cWu9Wmv+zwu7ePq9w3QHe7KXWePy+dzHJvOpxVV4s/pk7rUb4b2H4aoHzY4UQogUkoCeyAMLYcJCuOHfB120KxCiqSPA2/tP8Pjbh9h5rA2PWzGtPI/Z4/NZUF3EVYsqKc+XIC6EGDkS0BNp3GPqi3Y/7yRprdl0uJnXdjaw53gbe463U9fqw+NWXDZ3PDevnMQ5U0vjl2WEEGIYBgro6dXL5VQNcRi9Uoplk0tYNrlnTpd9DR08/d5hnt1Uyx8/OMaFs8q555NzmVp+BnfbE0KklbGdoY8AXzDMv797iH9duxdfKMznz6lheU0JVUU5TCzJoSg3/sEbrTWdgTANbT4+ONrKpkPNbD7Sgj8YwZvlxpuVwdlTSrhl1RTy+tbthRBjhpRcRkFDu48fvbyHNRtrez0+sSSHpZOKWVBVSGOHn331Hexr7KC+zYcv2NO1KjfTzaLqIvKzM+gKhGnuCrCjro3iXA9fvGAanz1nMrmZEthF+ghHNG6XlCkHIwF9FLV2BTnS3EVtczeHmjrZcqSFjYeaaWj3k+l2MbXcy7SKPKqKcij1ZlKWl8Ws8fnMHp9Phrt316qtR1q4/7UP+cuHjWR7XJw7vZxL51SwvKaYEm8WhTketNYcb/NR1+KjqcNPIBwhFNYUez1cOLMCV8w/zPsHT/L0e4epyM9mWrmXGePymV9Z0O99YzV1+NnX0MGxVh/1bT6WTi7mrJqShMsPx65jbfzlw0ZWTClhcXVRr7YPR4c/RKbbRWZGcjNftHYHOXKyi3mVBUkfFwmFI7yxt5E1G2t598BJbv1YDV++eHraHlfRWrPxUDPtvhBnTSmJ+yvyQGMH339xN1trWzh3ehmXzKmgptTLazvrefGDYxxs6uRz59Rw18UzKMz19Hv95sPNvLDtGJ9YOIGlk4bY1TYNSEA/w2itaeoMUJTjGTB4JrLxUDPPbznK2l0NHG3pjj6ulBlDGkmwS+dXFfBPV8xlYXUh//LKHh5/5yB5WRn4gmGCYfOiguwMLphVwSWzK7hiwYReQe/1PQ38/ZMb8Yd6nzlpxZQS/tvF082vjnY/je1+wlqTn+0hPzuDCYXZvX5N1LV088ibB9jX0MHZU0pYNb2MBVWF0c/iQGMHP167lxe21WH/eY4ryGL1vPHcdt5UJpb0H7EbjmhaugI0dwWpLMru9+tFa817H53kqfWHeXn7cUrzMrnnk3NZPX/8gEF2/YEm7vrNZurb/Ewt8/I3Z0/iumXV/Upn3YEwb+8/wQdHW9lR18bmw82c6AhQ4s1kRkUe6z86yZWLKvmX6xaS7Rlen/qGdh/PbjxKuy/ItUurmV7Rc5ym3RfEF4z062313Oaj3PfqHi6aVcF1y6pZWF046JfL1iMtZFg9uRK1ORLRvLqznof+sp8tR1oAyHAplkwqYsmkYiYUZjOhMIf3D57k8bcPku1xc+70Mt79qImWriBg/m5X1JQwvjCb57fWUZhjfoXOGpdPQY6H7kCYf3tjP2/uNSMyXQq+cN5UvvbxmXHbpbXmREeAfQ3m129Dm4+TnQGauwLkZWUwv6qQeZWFTK/IoyA7I+HnEIloAuFIr/do7Qry7+sP8ebeRpZMKuaS2RUsmVQc/WWhte63vtbuIL9efxiNprIwh8qiHKaUeYfcI04CeprSWrPrWDsf1rfT3BWguTOABiqLcqgqyqEsL4vMDBcet2Lz4Rb+5ZU9HG3ppiA7g3Z/iM+fU8M//JdZZGW4ONLczY66Vl7f08jrexo40RFgXmUBD9ywmBnj8nl1x3G+/OvNzBiXxz+unk1VUTbFuZk8v7WOf/vLAY63xTkrlMXjViyqLuJj00ppaPfz7KZatIYpZV72NnQA5p80w+0iw6XwBcNkZbi5dVUNN62YxMZDzby8/Th/3tMAGm5ZVcOXLpzG3oYOXthax9pdDRxr7Y5+kbkUTK/IY35VIeGIpq6lm8Mnu6hv85OfncGnFlex4VAzu461cf7Mcv7u3CmU5GZSmOMhN8uNx+0i0+3iF28d4P7XPmRyqZdbPlbD77ccZdPhFjLdLlZOK+Xjc8cxrczLH7Yd44WtdbT7QyhltmtBVSFXLJjARbMq8LgV//bGAX748m7mVxZy0ewK/KEw/mCEnEw3RTkeCq0v94jWRCKanEw3hdbjEa1p6ghwsjPAXz5s5LWd9YSs8kQ4olkxpYSFVYW8f6iZ7UdbUcDdl8/m786dglKK3285ytd+u4VJJbkca/XhD0WoKc2lqjiH/CwPxV4Pl8+fwLnTy3C5FA3tPu55bgcv7zAjO5WCSSW5TCzOtQJ0Ni3dQfbWd/BhfTtNnQEmleRy+/lTmVLm5a/7TvDWvhPsPt5OwPryVwpuWD6R/37ZTCryswmFI2w63MLhk12cP7OMivxsAHbWtfG9F3fy1329J6Ir9WbyhfOncs2SKh5Yu5en3zvM1HIvq6aVke0xv7aOt/o5cKKDA42dtHYHo691uxRFOR6Kcj00dwU52dkzOjnb42JcQTal3szo5x2KaA40dnLgRAf+UIRZ4/JZMqmITLeLNRtr6QyEmVGRx4ETnYQjmtxMNy6l8IfCaA0Xza7g5rMnce70Mp7dVMuPXt5DU2fvEdG3nz+Vb17RZxBekiSgC8AcsH3srx/x5ocn+PplM1meoFQSiWhe2XGcf3puOx3+ENcvq+a37x9hflUhj9+6ot/PYX8ozB+2HqOlK0BFQTbleVlkuBXtviDtvhC7j7fz9v4mPqhtIcPt4sazJnL7+VOpLs6lqcPPOwea2H2snWAkQjhsMvu/OXtSvwymrqWb//vqh/xuc230l0hWhosLZ5Uzc1y++afM9XDwRBcfHG1l+9FWsjwuKgvNF9zKaaVcubCSnEw3oXCEJ945xP2vfUiHP5TwM7tqUSXfv3ZBtISws66N57Yc5bWd9Xx0ohMwQeGK+RO4dmk1SyYV9R9sZlm7s57//swW2nwhsjJMEOoOhAkl+kkVR4k3k+uWVXPDWRMpyPawZmMtv3n/MHUt3SyeWMTKqaXsPt7OazvruWLBeC6YWc43fvcBK6aU8MtbVhCMRPjjtmOs3VlPS3eQdl+Q460+2nwhppZ7uWzu+OhAuq9cMoOaUi97G9rZ29BBbXM3x1u7aWj3k5eZwfRxecyoyOO8GeVcPn98v1+b9i/RYy0+8rMzqClLPFle39cdPtlFU2eA1u4ggVCE82aU9frV9ebeRr73x13RY0++UJiK/CymluUxtdzL9Iq86GVcfna0XGeXJD+obbW+5H3Ut/lp6vTT2h2MfhFMLTOvzc10s7W2lS2Hm+kMhLly4QS+cP5U5lUW0tod5M29jWw42IxLKbI8Zn/+YWsdTZ0BvJluOgNhlk0u5ttXzWNquZe6Fh91Ld2ML8xm5rgBRnYPYNgBXSm1GvhXwA08qrX+QZ/ns4AngGVAE3CD1vrgQOuUgH7ma2z3c/ez2/jT7gbOqinmsVvOIj+7f20zWW2+IFpDYc7Q1wGwo66VNRtrWVRdxKVzxw2r18/JzgB769tp84Vo6w7SFQjhD0UIhCNMKfUmLMlordnf2MG+hg5WTS9L+nMJRzQuRXSdWuvoQe9IxIwodylFdzBMS1eQ1u4ALqUo9WZRkpdJRX4WnjiBMxTR0ce11jz8xgF+9MqeaAb/q1vPSngQ3R8K8+IHx/jV24fYeqSF5ZOL+eF1C5mWoMttKBzB7VJn1PGAeKWOVIpENL5QOKmOCIFQhFd2HOe1nfVcPLuCqxdXprRtwwroSik38CHwcaAWeB+4SWu9M2aZLwELtdZfVErdCFyjtb5hoPVKQHcGrTUbDjWzoKpw2LVfcXqtP9DEazvr+drHZyb81dBXQ5uPsryslB2AFqk33IFFK4B9WusD1sp+A1wN7IxZ5mrgW9btNcBPlFJKj1Y9R6SMUmrEerGIkXX21FLOnlp6Sq+pKMgeodaI0yGZLhZVwJGY+7XWY3GX0VqHgFag31+SUup2pdQGpdSGxkY5nZoQQqTSaT0Fndb6Ya31cq318vLy8tP51kIIkfaSCehHgYkx96utx+Iuo5TKAAoxB0eFEEKcJskE9PeBGUqpKUqpTOBG4Pk+yzwPfN66fR3wZ6mfCyHE6TXoQVGtdUgp9WXgFUy3xce01juUUt8BNmitnwd+ATyplNoHnMQEfSGEEKdRUn2ZtNYvAi/2eeyemNs+4PrUNk0IIcSpOK0HRYUQQowcCehCCJEmRm0uF6VUI3BoiC8vA06ksDlOMRa3eyxuM4zN7R6L2wynvt2TtdZx+32PWkAfDqXUhkRDX9PZWNzusbjNMDa3eyxuM6R2u6XkIoQQaUICuhBCpAmnBvSHR7sBo2QsbvdY3GYYm9s9FrcZUrjdjqyhCyGE6M+pGboQQog+JKALIUSacFxAV0qtVkrtUUrtU0rdPdrtGQlKqYlKqXVKqZ1KqR1Kqa9Yj5copV5TSu21rotHu60jQSnlVkptVkq9YN2fopRab+3z31qTxKUNpVSRUmqNUmq3UmqXUuqcsbCvlVJfs/6+tyulnlZKZafjvlZKPaaUalBKbY95LO7+VcaD1vZvU0otPZX3clRAt06H91PgcmAucJNSau7otmpEhICva63nAiuBO63tvBv4k9Z6BvAn6346+gqwK+b+D4Efa62nA83A341Kq0bOvwIva61nA4sw257W+1opVQXcBSzXWs/HTPx3I+m5r38FrO7zWKL9ezkww7rcDvz8VN7IUQGdmNPhaa0DgH06vLSitT6mtd5k3W7H/INXYbb1cWuxx4FPjUoDR5BSqhr4BPCodV8BF2NObQhptt1KqULgfMyMpWitA1rrFsbAvsZMDphjnUMhFzhGGu5rrfUbmFloYyXav1cDT2jjXaBIKTUh2fdyWkBP5nR4aUUpVQMsAdYD47TWx6ynjgPjRqtdI+gB4B+BiHW/FGixTm0I6bfPpwCNwC+tMtOjSikvab6vtdZHgfuAw5hA3gpsJL33daxE+3dYMc5pAX1MUUrlAc8CX9Vat8U+Z51AJK36nCqlPgk0aK03jnZbTqMMYCnwc631EqCTPuWVNN3XxZhsdApQCXjpX5YYE1K5f50W0JM5HV5aUEp5MMH8Ka3176yH6+2fX9Z1w2i1b4SsAq5SSh3ElNMuxtSXi6yf5ZB++7wWqNVar7fur8EE+HTf15cCH2mtG7XWQeB3mP2fzvs6VqL9O6wY57SAnszp8BzPqhv/Atiltb4/5qnYU/19Hvj96W7bSNJaf0NrXa21rsHs2z9rrW8G1mFObQhptt1a6+PAEaXULOuhS4CdpPm+xpRaViqlcq2/d3u703Zf95Fo/z4PfM7q7bISaI0pzQxOa+2oC3AF8CGwH/in0W7PCG3juZifYNuALdblCkw9+U/AXmAtUDLabR3Bz+BC4AXr9lTgPWAf8B9A1mi3L8XbuhjYYO3v54DisbCvgW8Du4HtwJNAVjrua+BpzHGCIOYX2d8l2r+AwvTk2w98gOkFlPR7ydB/IYRIE04ruQghhEhAAroQQqQJCehCCJEmJKALIUSakIAuhBBpQgK6EEKkCQnoQgiRJv4/emJwl8ZAAOEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training','validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c7ec7915fb6663623914b2919607c696d31d8503b403c6d54bcb30c0bb224a7"
  },
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
